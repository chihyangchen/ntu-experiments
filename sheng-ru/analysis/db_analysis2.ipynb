{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import json\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "# This function input the path of experiment directory and output a list of device directories of the experiment directory.\n",
    "def find_device_under_exp(exp_dir_path):\n",
    "    dev_dir_list = sorted([os.path.join(exp_dir_path, d) for d in os.listdir(exp_dir_path) if d.startswith('qc') or d.startswith('sm')])\n",
    "    return dev_dir_list\n",
    "\n",
    "def find_trace_under_device(dev_dir_path):\n",
    "    trace_dir_list = sorted([os.path.join(dev_dir_path, d) for d in os.listdir(dev_dir_path)])\n",
    "    return trace_dir_list\n",
    "\n",
    "# Statistic functions\n",
    "# This function input the file path of the loss_latency csv and output the loss and excessive latency rate.\n",
    "def count_loss_excl_rate(file_path):\n",
    "\n",
    "    df = pd.read_csv (file_path)\n",
    "\n",
    "    # Total package in the experiment\n",
    "    total_pkg_num = len(df)\n",
    "\n",
    "    # Loss calculate\n",
    "    loss_cond = df['lost'] == True\n",
    "    try: loss_num = loss_cond.value_counts().loc[True]\n",
    "    except: loss_num = 0\n",
    "    loss_rate = loss_num/total_pkg_num\n",
    "\n",
    "    # Excexxive latency calculate\n",
    "    exc_lat = 0.1\n",
    "    excl_cond = df[loss_cond==False]['latency'] > exc_lat\n",
    "    try: excl_num = excl_cond.value_counts().loc[True]\n",
    "    except: excl_num = 0\n",
    "    excl_rate = excl_num/total_pkg_num\n",
    "\n",
    "    return loss_rate, excl_rate\n",
    "\n",
    "# This function input two file paths of the loss_latency csv and output the \n",
    "# loss and excessive latency rate of dual radio condition.\n",
    "def count_loss_excl_rate_dual(file_path1, file_path2):\n",
    "\n",
    "    df1 = pd.read_csv(file_path1)\n",
    "    df2 = pd.read_csv(file_path2)\n",
    "\n",
    "    start_seq = df1['seq'].iloc[0] if df1['seq'].iloc[0] >=  df2['seq'].iloc[0] else df2['seq'].iloc[0]\n",
    "    end_seq = df1['seq'].iloc[-1] if df1['seq'].iloc[-1] <=  df2['seq'].iloc[-1] else df2['seq'].iloc[-1]\n",
    "    total_pkg_num = end_seq - start_seq + 1\n",
    "\n",
    "    cond1 = (df1['seq'] >= start_seq) & (df1['seq'] <= end_seq)\n",
    "    df1 = df1[cond1]\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    cond2 = (df2['seq'] >= start_seq) & (df2['seq'] <= end_seq)\n",
    "    df2 = df2[cond2]\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    # Loss calculate for dual radios redundant packets.\n",
    "    loss_cond = (df1['lost'] == True) & (df2['lost'] == True)\n",
    "    try: loss_num = loss_cond.value_counts().loc[True]\n",
    "    except: loss_num = 0\n",
    "    loss_rate = loss_num/total_pkg_num\n",
    "\n",
    "    # Excexxive latency calculate for dual radios redundant packets.\n",
    "    exc_lat = 0.1   \n",
    "    excl_cond1 = df1[(loss_cond==False)]['latency'] > exc_lat\n",
    "    excl_cond2 = df2[(loss_cond==False)]['latency'] > exc_lat\n",
    "    excl_cond = (excl_cond1 == True) & (excl_cond2 == True)\n",
    "    try: excl_num = excl_cond.value_counts().loc[True]\n",
    "    except: excl_num = 0\n",
    "    excl_rate = excl_num/total_pkg_num\n",
    "\n",
    "    return loss_rate, excl_rate\n",
    "\n",
    "# Convenience instance\n",
    "class EXPERIMENT():\n",
    "    def __init__(self, exp_dir_path, settings):\n",
    "        self.path = exp_dir_path\n",
    "        self.settings = json.loads(settings)\n",
    "    def __repr__(self):\n",
    "        return f'EXP: {self.path} -> {self.settings}'\n",
    "    \n",
    "# Convenience funciton\n",
    "def add_dicts_with_lists(dict1, dict2):\n",
    "    result_dict = {}\n",
    "    \n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            combined_list = dict1[key] + dict2[key]\n",
    "            result_dict[key] = combined_list\n",
    "        else:\n",
    "            result_dict[key] = dict1[key]\n",
    "    \n",
    "    for key in dict2:\n",
    "        if key not in result_dict:\n",
    "            result_dict[key] = dict2[key]\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place give a XXXX-XX-XX.md file and find the experiment directory path\n",
    "# and the corresponding band settings. It will be presented by a list of special\n",
    "# instance EXPERIMENTs.\n",
    "\n",
    "md_files = ['/home/wmnlab/D/database/2023-05-15/2023-05-15.md',\n",
    "            '/home/wmnlab/D/database/2023-05-24/2023-05-24.md']\n",
    "EXPs = []\n",
    "\n",
    "for md_file_path in md_files:\n",
    "\n",
    "    date_dir_path = os.path.dirname(md_file_path)\n",
    "\n",
    "    with open(md_file_path) as f:\n",
    "\n",
    "        exp = f.readline()[:-1]\n",
    "        settings = f.readline()[:-1]\n",
    "\n",
    "        while exp != '#endif' and settings:\n",
    "            E = EXPERIMENT(os.path.join(date_dir_path, exp), settings)\n",
    "            EXPs.append(E)\n",
    "            exp = f.readline()[:-1]\n",
    "            settings = f.readline()[:-1]\n",
    "\n",
    "EXPs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the example of given a experiment directory and plot the \n",
    "# box plot of DL/UL loss/excessive latency. \n",
    "\n",
    "# Still need to revise here.\n",
    "\n",
    "dev_metric_dicts = []\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    dev_metric_dict = {}    \n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "\n",
    "    for dev_dir_path in dev_dir_list:\n",
    "\n",
    "        dev = dev_dir_path.split('/')[-1]\n",
    "        metrics_dict = {}\n",
    "        trace_dir_list = find_trace_under_device(dev_dir_path)\n",
    "        dl_loss_rates, dl_excl_rates = [], []    \n",
    "        ul_loss_rates, ul_excl_rates = [], []\n",
    "\n",
    "        for trace_dir_path in trace_dir_list:\n",
    "\n",
    "            # if '#01' not in trace_dir_path:\n",
    "            #     continue\n",
    "\n",
    "            dl_file_path = os.path.join(trace_dir_path, 'data/udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path = os.path.join(trace_dir_path, 'data/udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_rate, dl_excl_rate = count_loss_excl_rate(dl_file_path)\n",
    "            ul_loss_rate, ul_excl_rate = count_loss_excl_rate(ul_file_path)\n",
    "\n",
    "            dl_loss_rates.append(dl_loss_rate); dl_excl_rates.append(dl_excl_rate)\n",
    "            ul_loss_rates.append(ul_loss_rate); ul_excl_rates.append(ul_excl_rate)\n",
    "        \n",
    "        metrics_dict['dl_loss'] = dl_loss_rates\n",
    "        metrics_dict['dl_excl'] = dl_excl_rates\n",
    "        metrics_dict['ul_loss'] = ul_loss_rates\n",
    "        metrics_dict['ul_excl'] = ul_excl_rates\n",
    "\n",
    "        dev_metric_dict[dev] = metrics_dict\n",
    "\n",
    "    dev_metric_dicts.append(dev_metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "empty_dict = {'dl_loss': [],'dl_excl': [],'ul_loss': [], 'ul_excl': []}\n",
    "all_dev_metric_dict = {k: copy.deepcopy(empty_dict) for k in dev_metric_dicts[0].keys()}\n",
    "\n",
    "for d in dev_metric_dicts:\n",
    "    for device in d:\n",
    "        for metric in d[device]:\n",
    "            all_dev_metric_dict[device][metric] += d[device][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dev_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Here\n",
    "dev_metric_dict = dev_metric_dicts[1]\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12, 8))\n",
    "dl_loss_boxes, dl_excl_boxes = [], []\n",
    "ul_loss_boxes, ul_excl_boxes = [], []\n",
    "labels = [settings[k] for k in list(dev_metric_dict.keys())]\n",
    "\n",
    "for k, v in dev_metric_dict.items():\n",
    "\n",
    "    dl_loss_box = [value*100 for value in dev_metric_dict[k]['dl_loss'] ] \n",
    "    dl_excl_box = [value*100 for value in dev_metric_dict[k]['dl_excl'] ] \n",
    "    ul_loss_box = [value*100 for value in dev_metric_dict[k]['ul_loss'] ]\n",
    "    ul_excl_box = [value*100 for value in dev_metric_dict[k]['ul_excl'] ]\n",
    "\n",
    "    dl_loss_boxes.append(dl_loss_box)\n",
    "    dl_excl_boxes.append(dl_excl_box)\n",
    "    ul_loss_boxes.append(ul_loss_box)\n",
    "    ul_excl_boxes.append(ul_excl_box)\n",
    "\n",
    "axes[0][0].boxplot(dl_loss_boxes, labels=labels)\n",
    "axes[0][0].set_title('DL Loss')\n",
    "axes[0][1].boxplot(dl_excl_boxes, labels=labels)\n",
    "axes[0][1].set_title('DL Excessive Latency')\n",
    "axes[1][0].boxplot(ul_loss_boxes, labels=labels)\n",
    "axes[1][0].set_title('UL Loss')\n",
    "axes[1][1].boxplot(ul_excl_boxes, labels=labels)\n",
    "axes[1][1].set_title('UL Excessive Latency')\n",
    "\n",
    "# fig.text(0.5, 0.0, 'common xlabel', ha='center', va='center', fontsize=16)\n",
    "fig.text(0.0, 0.5, 'Percentage %', ha='center', va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in all_dev_metric_dict.keys():\n",
    "    \n",
    "    for metric in all_dev_metric_dict[device].keys():\n",
    "\n",
    "        if metric == 'dl_loss':\n",
    "            L = all_dev_metric_dict[device][metric]\n",
    "            for i in range(len(L)):\n",
    "                if L[i] > 0.01:\n",
    "                    print(metric, i, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place given an experiment directory, plot bar plot of UL/DL loss/excessive latency rate  \n",
    "# of all the traces. You must run the above box plot code to make this place work.\n",
    "\n",
    "dl_loss_bars, dl_excl_bars = [], []\n",
    "ul_loss_bars, ul_excl_bars = [], []\n",
    "trace_num = len(trace_dir_list)\n",
    "\n",
    "for i in range(trace_num):\n",
    "\n",
    "    dl_loss_bar = [dev_metric_dict[k]['dl_loss'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    dl_excl_bar = [dev_metric_dict[k]['dl_excl'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    ul_loss_bar = [dev_metric_dict[k]['ul_loss'][i]*100 for k in dev_metric_dict.keys()]\n",
    "    ul_excl_bar = [dev_metric_dict[k]['ul_excl'][i]*100 for k in dev_metric_dict.keys()]\n",
    "\n",
    "    dl_loss_bars.append(dl_loss_bar)\n",
    "    dl_excl_bars.append(dl_excl_bar)\n",
    "    ul_loss_bars.append(ul_loss_bar)\n",
    "    ul_excl_bars.append(ul_excl_bar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(trace_num):\n",
    "    fig, axes = plt.subplots(2,2, figsize=(8, 4))\n",
    "\n",
    "    axes[0][0].bar(labels, dl_loss_bars[i], width=0.4)\n",
    "    axes[0][0].set_title('DL Loss')\n",
    "    axes[0][1].bar(labels, dl_excl_bars[i], width=0.4)\n",
    "    axes[0][1].set_title('DL Excessive Latency')\n",
    "    axes[1][0].bar(labels, ul_loss_bars[i], width=0.4)\n",
    "    axes[1][0].set_title('UL Loss')\n",
    "    axes[1][1].bar(labels, ul_excl_bars[i], width=0.4)\n",
    "    axes[1][1].set_title('UL Excessive Latency')\n",
    "\n",
    "    fig.text(0.0, 0.5, 'Percentage %', ha='center', va='center', rotation='vertical', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place calculate the dual redio performance \n",
    "# given a experiment directory.\n",
    "\n",
    "# Still need to revise here.\n",
    "\n",
    "comb_metric_dicts = []\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "    comb = itertools.combinations(dev_dir_list, 2)\n",
    "    comb_metric_dict = {}\n",
    "\n",
    "    for dev_dir_path1, dev_dir_path2 in comb:\n",
    "        \n",
    "        dev1 = dev_dir_path1.split('/')[-1]\n",
    "        dev2 = dev_dir_path2.split('/')[-1]\n",
    "\n",
    "        trace_dir_list1 = find_trace_under_device(dev_dir_path1)\n",
    "        trace_dir_list2 = find_trace_under_device(dev_dir_path2)\n",
    "        metrics_dict = {}\n",
    "\n",
    "        dl_loss_rates, dl_excl_rates = [], []    \n",
    "        ul_loss_rates, ul_excl_rates = [], []\n",
    "        \n",
    "        for trace_dir_path1, trace_dir_path2 in zip(trace_dir_list1, trace_dir_list2):\n",
    "\n",
    "            dl_file_path1 = os.path.join(trace_dir_path1, 'data/udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path1 = os.path.join(trace_dir_path1, 'data/udp_uplk_loss_latency.csv')\n",
    "            dl_file_path2 = os.path.join(trace_dir_path2, 'data/udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path2 = os.path.join(trace_dir_path2, 'data/udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_rate, dl_excl_rate = count_loss_excl_rate_dual(dl_file_path1, dl_file_path2)\n",
    "            ul_loss_rate, ul_excl_rate = count_loss_excl_rate_dual(ul_file_path1, ul_file_path2)\n",
    "\n",
    "            dl_loss_rates.append(dl_loss_rate); dl_excl_rates.append(dl_excl_rate)\n",
    "            ul_loss_rates.append(ul_loss_rate); ul_excl_rates.append(ul_excl_rate)\n",
    "        \n",
    "        metrics_dict['dl_loss'] = dl_loss_rates\n",
    "        metrics_dict['dl_excl'] = dl_excl_rates\n",
    "        metrics_dict['ul_loss'] = ul_loss_rates\n",
    "        metrics_dict['ul_excl'] = ul_excl_rates\n",
    "\n",
    "        comb_metric_dict[f'{dev1}+{dev2}'] = metrics_dict\n",
    "    \n",
    "    comb_metric_dicts.append(comb_metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_dict = {'dl_loss': [],'dl_excl': [],'ul_loss': [], 'ul_excl': []}\n",
    "all_comb_metric_dict = {k: copy.deepcopy(empty_dict) for k in comb_metric_dicts[0].keys()}\n",
    "\n",
    "for d in comb_metric_dicts:\n",
    "    for device in d:\n",
    "        for metric in d[device]:\n",
    "            all_comb_metric_dict[device][metric] += d[device][metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comb_metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place calculate the average performance of an \n",
    "# experiment setting and plot the heatmap below.\n",
    " \n",
    "comb_metric_dict = all_comb_metric_dict\n",
    "\n",
    "# num_of_devs = 2\n",
    "num_of_devs = len(settings)\n",
    "data_dl_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "data_dl_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "data_ul_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "data_ul_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "\n",
    "keys = list(comb_metric_dict.keys())\n",
    "count = 0\n",
    "\n",
    "for i in range(num_of_devs):\n",
    "    for j in range(i+1, num_of_devs):\n",
    "\n",
    "        k = keys[count]\n",
    "        \n",
    "        dl_loss = np.mean(comb_metric_dict[k]['dl_loss'])\n",
    "        dl_excl = np.mean(comb_metric_dict[k]['dl_excl'])\n",
    "        ul_loss = np.mean(comb_metric_dict[k]['ul_loss'])\n",
    "        ul_excl = np.mean(comb_metric_dict[k]['ul_excl'])\n",
    "\n",
    "        data_dl_loss[i, j] = dl_loss*100\n",
    "        data_dl_excl[i, j] = dl_excl*100\n",
    "        data_ul_loss[i, j] = ul_loss*100\n",
    "        data_ul_excl[i, j] = ul_excl*100\n",
    "\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(8,6))\n",
    "\n",
    "# labels = ['378', '3']\n",
    "labels = list(settings.values())\n",
    "mask = np.tri(data_dl_loss.shape[0], dtype=bool, k=0)\n",
    "\n",
    "sns.heatmap(data_dl_loss, ax=axes[0,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[0,0].set_xticklabels(labels, rotation=90)\n",
    "axes[0,0].set_yticklabels(labels, rotation=0)\n",
    "axes[0,0].set_title('DL Loss')\n",
    "sns.heatmap(data_dl_excl, ax=axes[0,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[0,1].set_xticklabels(labels, rotation=90)\n",
    "axes[0,1].set_yticklabels(labels, rotation=0)\n",
    "axes[0,1].set_title('DL Excessive Latency')\n",
    "sns.heatmap(data_ul_loss, ax=axes[1,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[1,0].set_xticklabels(labels, rotation=90)\n",
    "axes[1,0].set_yticklabels(labels, rotation=0)\n",
    "axes[1,0].set_title('UL Loss')\n",
    "sns.heatmap(data_ul_excl, ax=axes[1,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "axes[1,1].set_xticklabels(labels, rotation=90)\n",
    "axes[1,1].set_yticklabels(labels, rotation=0)\n",
    "axes[1,1].set_title('UL Excessive Latency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for device in all_comb_metric_dict.keys():\n",
    "    \n",
    "    for metric in all_comb_metric_dict[device].keys():\n",
    "\n",
    "        if metric == 'dl_loss':\n",
    "            L = all_comb_metric_dict[device][metric]\n",
    "            for i in range(len(L)):\n",
    "                if L[i] > 0.005:\n",
    "                    print(metric, i, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place plot the heatmap of every trace\n",
    "# of an experiment.\n",
    " \n",
    "num_of_devs = len(settings)\n",
    "keys = list(comb_metric_dict.keys())\n",
    "\n",
    "for t in range(trace_num):\n",
    "    \n",
    "    data_dl_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_dl_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_ul_loss = np.zeros((num_of_devs, num_of_devs))\n",
    "    data_ul_excl = np.zeros((num_of_devs, num_of_devs))\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(num_of_devs):\n",
    "        for j in range(i+1, num_of_devs):\n",
    "\n",
    "            k = keys[count]\n",
    "            \n",
    "            dl_loss = comb_metric_dict[k]['dl_loss'][t]\n",
    "            dl_excl = comb_metric_dict[k]['dl_excl'][t]\n",
    "            ul_loss = comb_metric_dict[k]['ul_loss'][t]\n",
    "            ul_excl = comb_metric_dict[k]['ul_excl'][t]\n",
    "\n",
    "            data_dl_loss[i, j] = dl_loss*100\n",
    "            data_dl_excl[i, j] = dl_excl*100\n",
    "            data_ul_loss[i, j] = ul_loss*100\n",
    "            data_ul_excl[i, j] = ul_excl*100\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=(8,6))\n",
    "\n",
    "    labels = list(settings.values())\n",
    "    mask = np.tri(data_dl_loss.shape[0], dtype=bool, k=0)\n",
    "\n",
    "    sns.heatmap(data_dl_loss, ax=axes[0,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[0,0].set_xticklabels(labels, rotation=90)\n",
    "    axes[0,0].set_yticklabels(labels, rotation=0)\n",
    "    axes[0,0].set_title('DL Loss')\n",
    "    sns.heatmap(data_dl_excl, ax=axes[0,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[0,1].set_xticklabels(labels, rotation=90)\n",
    "    axes[0,1].set_yticklabels(labels, rotation=0)\n",
    "    axes[0,1].set_title('DL Excessive Latency')\n",
    "    sns.heatmap(data_ul_loss, ax=axes[1,0], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[1,0].set_xticklabels(labels, rotation=90)\n",
    "    axes[1,0].set_yticklabels(labels, rotation=0)\n",
    "    axes[1,0].set_title('UL Loss')\n",
    "    sns.heatmap(data_ul_excl, ax=axes[1,1], mask=mask, annot=True, fmt=\".2f\")\n",
    "    axes[1,1].set_xticklabels(labels, rotation=90)\n",
    "    axes[1,1].set_yticklabels(labels, rotation=0)\n",
    "    axes[1,1].set_title('UL Excessive Latency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(dl_file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/wmnlab/D/database/2023-06-12/Bandlock_8_Schemes_Phone/combo'\n",
    "L = sorted([os.path.join(dir_path, x, 'udp_dnlk_combo_statistics.csv') for x in os.listdir(dir_path)])\n",
    "\n",
    "for f in L:\n",
    "    df = pd.read_csv(f)\n",
    "    print(df['lost_All+B3'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
