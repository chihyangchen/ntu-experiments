{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse every case.\n",
    "EVENTS = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "          'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "          'RLF_II', 'RLF_III', 'SCG_RLF']\n",
    "\n",
    "REs = [r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| O', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+)', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O',\n",
    "       r'\\((\\d+), (\\d+)\\) \\| O -> (\\d+)', \n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> O', \n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> (\\d+)',\n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O',\n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> O']\n",
    "\n",
    "def get_re_from_type(event):\n",
    "    if event in EVENTS:\n",
    "        index = EVENTS.index(event)\n",
    "        return REs[index]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "EVENTS1 = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB']\n",
    "\n",
    "CASES = ['all', 'two_event', 'one_event'] + \\\n",
    "        ['two_RLF', 'two_scg_failure', 'one_RLF_one_scg', 'one_RLF', 'one_scg_failure'] + \\\n",
    "        ['two_exact_identicle_HO'] + [f'two_exact_identicle_{type}' for type in EVENTS] + \\\n",
    "        ['two_identicle_RLF_SN_setup'] + \\\n",
    "        ['pci_earfcn_identicle_HO_eNB'] + [f'pci_earfcn_identicle_{type}' for type in EVENTS1] + \\\n",
    "        ['pci_identicle_HO_eNB'] + [f'pci_identicle_{type}' for type in EVENTS1] + \\\n",
    "        ['pci_identicle_HO_gNB'] + \\\n",
    "        ['pci_earfcn_identicle_RLF'] + ['pci_identicle_RLF'] + ['pci_identicle_sRLF'] + \\\n",
    "        ['pci_identicle_RLF_setup_cause_pci', 'pci_identicle_RLF_setup_cause_pci_earfcn'] + \\\n",
    "        ['SN_setup_of_RLF_MN_HO_to_eNB', 'SN_setup_of_RLF_SN_Rel', 'SN_setup_of_MN_HO_to_eNB_SN_Rel'] + \\\n",
    "        ['SN_setup_same_cause_rlf_pci', 'SN_setup_same_cause_rlf_pci_earfcn'] + \\\n",
    "        ['SN_setup_of_same_cause_MN_HO_to_eNB', 'SN_setup_of_same_cause_SN_Rel', 'SN_setup_same_cause_rlf']\n",
    "\n",
    "ANALYSIS = namedtuple('ANALYSIS', CASES, defaults = [0]*len(CASES))\n",
    "\n",
    "def Analyze(pkgs):\n",
    "    \n",
    "    # Useful functions.\n",
    "    # \n",
    "    def remove_elements(A, B, condition):\n",
    "        indexes_to_remove = [i for i, a in enumerate(A) if condition(a)]\n",
    "        for index in sorted(indexes_to_remove, reverse=True):\n",
    "            del A[index]\n",
    "            del B[index]\n",
    "        return A, B\n",
    "    \n",
    "    # Case functions.\n",
    "    # Case source and target cause type, trans are exactly the same.\n",
    "    # A is cause list and B is trans list. \n",
    "    # Totally identical in trans string.\n",
    "    def find_exact_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2)):\n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "                    \n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans \"pci\" and \"earfcn\" are the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB'].\n",
    "    def find_pci_earfcn_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(1)}, {match.group(2)}) -> ({match.group(3)}, {match.group(4)})'\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in EVENTS1)\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in EVENTS1)\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # Case source and target cause type, trans \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB'].\n",
    "    def find_pci_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(1)} -> {match.group(3)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in EVENTS1)\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in EVENTS1)\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with 'gNB_HO.\n",
    "    def find_pci_identicle_gNB(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(3)}, {match.group(4)})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['SN_HO'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['SN_HO'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans source \"pci\" and \"earfcn\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['RLF_II'] now.\n",
    "    def find_pci_earfcn_identicle_RLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(1)}, {match.group(2)})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['RLF_II'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans source \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['RLF_II'] now.\n",
    "    def find_pci_identicle_RLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(1)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['RLF_II'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_RLF_SN_Setup_Same_Cause_pci(A1, B1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event, seq):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        def extract_coordinates2(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II', 'RLF_III'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        B1_ = [extract_coordinates(b1, a1, 1) for b1, a1 in zip(B1_, A1_)]\n",
    "        C2_ = [extract_coordinates2(c2, r'\\((\\d+), (\\d+)\\)', 1) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and b1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_RLF_SN_Setup_Same_Cause_pci_earfcn(A1, B1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event, seq):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        def extract_coordinates2(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II', 'RLF_III'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        B1_ = [extract_coordinates(b1, a1, [1, 2]) for b1, a1 in zip(B1_, A1_)]\n",
    "        C2_ = [extract_coordinates2(c2, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and b1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_SN_Setup_Same_Cause_pci_earfcn(A1, C1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "        \n",
    "        def extract_coordinates(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; C1_ = [element for element in C1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, C1_ = remove_elements(A1_, C1_, lambda a: a not in ['SN_setup'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        C1_ = [extract_coordinates(c1, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c1 in C1_]\n",
    "        C2_ = [extract_coordinates(c2, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, c1) in enumerate(zip(A1_, C1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and c1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_SN_Setup_Same_Cause_pci(A1, C1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "        \n",
    "        def extract_coordinates(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq[0])}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; C1_ = [element for element in C1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, C1_ = remove_elements(A1_, C1_, lambda a: a not in ['SN_setup'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        C1_ = [extract_coordinates(c1, r'\\((\\d+), (\\d+)\\)', [1]) for c1 in C1_]\n",
    "        C2_ = [extract_coordinates(c2, r'\\((\\d+), (\\d+)\\)', [1]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, c1) in enumerate(zip(A1_, C1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and c1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "\n",
    "    # Case source and target cause type, trans source \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with 'SCG_RLF' now.\n",
    "    def find_pci_identicle_sRLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(3)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['SCG_RLF'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['SCG_RLF'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # Count the number of every case. \n",
    "    nums = {k: 0 for k in CASES}\n",
    "    nums['all'] = len(pkgs)\n",
    "\n",
    "    for pkg in pkgs:\n",
    "        \n",
    "        cause1_string = \"\".join(pkg.cause1)\n",
    "        cause2_string = \"\".join(pkg.cause2)\n",
    "        others1_string = \"\".join(pkg.others1)\n",
    "        others2_string = \"\".join(pkg.others2)\n",
    "\n",
    "        # Event related check.\n",
    "        if cause1_string and cause2_string:\n",
    "            nums['two_event'] += 1\n",
    "        elif cause1_string or cause2_string:\n",
    "            nums['one_event'] += 1\n",
    "\n",
    "        # RLF related case.\n",
    "        if ('RLF_' in cause1_string) and ('RLF_' in cause2_string):   \n",
    "            nums['two_RLF'] += 1\n",
    "\n",
    "            # Nested Events\n",
    "            l1 = find_pci_identicle_RLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "            if len(l1) != 0: \n",
    "                nums['pci_identicle_RLF'] += 1\n",
    "                \n",
    "                l2 = find_pci_earfcn_identicle_RLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "                if len(l2) != 0:\n",
    "                    nums['pci_earfcn_identicle_RLF'] += 1\n",
    "\n",
    "        elif ('RLF_' in cause1_string) and ('SN_setup' in cause2_string and 'Near after RLF' in others2_string):\n",
    "\n",
    "            l1 = find_RLF_SN_Setup_Same_Cause_pci(pkg.cause1, pkg.trans1, pkg.cause2, pkg.others2)\n",
    "            if len(l1) != 0:\n",
    "                nums['pci_identicle_RLF_setup_cause_pci'] += 1\n",
    "\n",
    "                l2 = find_RLF_SN_Setup_Same_Cause_pci_earfcn(pkg.cause1, pkg.trans1, pkg.cause2, pkg.others2)\n",
    "                if len(l1) != 0:\n",
    "                    nums['pci_identicle_RLF_setup_cause_pci_earfcn'] += 1\n",
    "\n",
    "        elif ('SN_setup' in cause1_string and 'Near after RLF' in others1_string) and ('RLF_' in cause2_string):\n",
    "\n",
    "            l1 = find_RLF_SN_Setup_Same_Cause_pci(pkg.cause2, pkg.trans2, pkg.cause1, pkg.others1)\n",
    "            if len(l1) != 0:\n",
    "                nums['pci_identicle_RLF_setup_cause_pci'] += 1\n",
    "\n",
    "                l2 = find_RLF_SN_Setup_Same_Cause_pci_earfcn(pkg.cause2, pkg.trans2, pkg.cause1, pkg.others1)\n",
    "                if len(l1) != 0:\n",
    "                    nums['pci_identicle_RLF_setup_cause_pci_earfcn'] += 1\n",
    "\n",
    "        elif ('RLF_' in cause1_string) or ('RLF_' in cause2_string):  \n",
    "            nums['one_RLF'] += 1\n",
    "\n",
    "        # elif ('SCG_RLF' in cause1_string) and ('SCG_RLF' in cause2_string): \n",
    "        #     nums['two_scg_failure'] += 1\n",
    "        \n",
    "        # elif (('RLF_' in cause1_string) and ('SCG_RLF' in cause2_string) ) or (('SCG_RLF' in cause1_string) and ('RLF_' in cause2_string) ):\n",
    "        #     nums['one_RLF_one_scg'] += 1\n",
    "\n",
    "        # elif ('SCG_RLF' in cause1_string) or ('SCG_RLF' in cause2_string): \n",
    "        #     nums['one_scg_failure'] += 1\n",
    "\n",
    "        # elif ('RLF_' in cause1_string) or ('RLF_' in cause2_string):  \n",
    "        #     nums['one_RLF'] += 1\n",
    "\n",
    "        # SN setup case observation.\n",
    "        if ('SN_setup' in cause1_string) and ('SN_setup' in cause2_string):\n",
    "            \n",
    "            p = r' Near after (.*?)\\.'\n",
    "            \n",
    "            try: match1 = re.search(p, others1_string); s1 = match1.group(1)\n",
    "            except: s1 = 'Unknown'\n",
    "\n",
    "            try: match2 = re.search(p, others2_string); s2 = match2.group(1)\n",
    "            except: s2 = 'Unknown'\n",
    "\n",
    "            if ('RLF' in s1) and ('RLF' in s2):\n",
    "                \n",
    "                nums['SN_setup_same_cause_rlf'] += 1\n",
    "\n",
    "                l1 = find_SN_Setup_Same_Cause_pci(pkg.cause1, pkg.others1, pkg.cause2, pkg.others2)\n",
    "                if len(l1) != 0:\n",
    "                    nums['SN_setup_same_cause_rlf_pci'] += 1\n",
    "\n",
    "                    l2 = find_SN_Setup_Same_Cause_pci_earfcn(pkg.cause1, pkg.others1, pkg.cause2, pkg.others2)\n",
    "                    if len(l2) != 0:\n",
    "                        nums['SN_setup_same_cause_rlf_pci_earfcn'] += 1\n",
    "\n",
    "            elif ('MN_HO_to_eNB' in s1) and ('MN_HO_to_eNB' in s2):\n",
    "                nums['SN_setup_of_same_cause_MN_HO_to_eNB'] += 1\n",
    "            elif ('SN_Rel' in s1) and ('SN_Rel' in s2):\n",
    "                nums['SN_setup_of_same_cause_SN_Rel'] += 1\n",
    "            elif ('RLF' in s1 or 'RLF' in s2) and ('MN_HO_to_eNB' in s1 or 'MN_HO_to_eNB' in s2):\n",
    "                nums['SN_setup_of_RLF_MN_HO_to_eNB'] += 1\n",
    "            elif ('RLF' in s1 or 'RLF' in s2) and ('SN_Rel' in s1 or 'SN_Rel' in s2):\n",
    "                nums['SN_setup_of_RLF_SN_Rel'] += 1\n",
    "            elif ('MN_HO_to_eNB' in s1 or 'MN_HO_to_eNB' in s2) and ('SN_Rel' in s1 or 'SN_Rel' in s2):\n",
    "                nums['SN_setup_of_MN_HO_to_eNB_SN_Rel'] += 1\n",
    "\n",
    "        # Identicle HO related case.\n",
    "        # Case Exact Identicle.\n",
    "        L1 = find_exact_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "        if len(L1) != 0: nums['two_exact_identicle_HO'] += 1\n",
    "        \n",
    "        identicle_types = []\n",
    "        for (i, _) in L1:\n",
    "            _, ho_type = pkg.cause1[i].split(' ')\n",
    "            identicle_types.append(ho_type)\n",
    "            nums[f'two_exact_identicle_{ho_type}'] += 1\n",
    "\n",
    "        if ('RLF_II' in identicle_types or 'RLF_III' in identicle_types) and ('SN_setup' in identicle_types):\n",
    "            nums[f'two_identicle_RLF_SN_setup'] += 1\n",
    "\n",
    "        # eNB HO related\n",
    "        L2 = find_pci_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        \n",
    "        if len(L2) != 0: \n",
    "            nums['pci_identicle_HO_eNB'] += 1\n",
    "\n",
    "            l1 = find_pci_earfcn_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "            if len(l1) != 0: \n",
    "                nums['pci_earfcn_identicle_HO_eNB'] += 1\n",
    "\n",
    "        # gNB related\n",
    "        L3 = find_pci_identicle_gNB(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        if len(L3) != 0: nums['pci_identicle_HO_gNB'] += 1\n",
    "\n",
    "        # srlf related cases.\n",
    "        L4 = find_pci_identicle_sRLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        if len(L4) != 0: nums['pci_identicle_sRLF'] += 1\n",
    "\n",
    "    return ANALYSIS(*nums.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place give a XXXX-XX-XX.md file and find the experiment directory path\n",
    "# and the corresponding band settings. It will be presented by a list of special\n",
    "# instance EXPERIMENTs.\n",
    "md_files = [\n",
    "    # '/home/wmnlab/D/database/2023-08-29/2023-08-29.md', \n",
    "    # '/home/wmnlab/D/database/2023-09-12_1/2023-09-12.md',\n",
    "    # '/home/wmnlab/D/database/2023-09-22/2023-09-22.md',\n",
    "    # '/home/wmnlab/D/database/2023-10-24/2023-10-24.md',\n",
    "    '/home/wmnlab/D/database/2023-11-21/2023-11-21.md',\n",
    "    # '/home/wmnlab/D/database/2023-12-26/2023-12-26.md'\n",
    "    ]\n",
    "EXPs = []\n",
    "\n",
    "for md_file_path in md_files:\n",
    "\n",
    "    date_dir_path = os.path.dirname(md_file_path)\n",
    "\n",
    "    with open(md_file_path) as f:\n",
    "\n",
    "        exp = f.readline()[:-1]\n",
    "        settings = f.readline()[:-1]\n",
    "\n",
    "        while exp != '#endif' and settings:\n",
    "            E = EXPERIMENT(os.path.join(date_dir_path, exp), settings)\n",
    "            EXPs.append(E)\n",
    "            exp = f.readline()[:-1]\n",
    "            settings = f.readline()[:-1]\n",
    "\n",
    "pprint(EXPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code evaluate the nearby event of a packet loss/ excl packet of a single radio performance.\n",
    "\n",
    "# Record\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "cases = ANALYSIS._fields\n",
    "analysis_dict_all = {}\n",
    "for k in keys:\n",
    "    analysis_dict_all[k] = {case: 0 for case in cases}\n",
    "\n",
    "analysis_dicts = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Some settings\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "count_events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "          'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "          'RLF_II', 'RLF_III', 'SCG_RLF', 'Conn_Req']\n",
    "bdr = ['Before', 'During', 'After']\n",
    "\n",
    "comb = itertools.product(bdr, count_events)\n",
    "count_bdr_events = [t+' '+ho for t, ho in comb]\n",
    "\n",
    "analysis_dicts = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Record how many total packet.\n",
    "total_Modem_Action_Test = {k:0 for k in keys}\n",
    "total_Modem_Action_Test_v2_1 = {k:0 for k in keys}\n",
    "toral_Modem_Action_Test_v2_2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group = {k:0 for k in keys}\n",
    "total_Modem_Control_Group2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group3 = {k:0 for k in keys}\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "    date = exp_dir_path.split('/')[-2]\n",
    "    name = exp_dir_path.split('/')[-1]\n",
    "\n",
    "    # Below can be used to filtering.\n",
    "    # if name != 'Modem_Action_Test':\n",
    "    #     continue\n",
    "\n",
    "    for dev_dir_path in dev_dir_list:\n",
    "        \n",
    "        dev = dev_dir_path.split('/')[-1]\n",
    "        trace_dir_list = find_trace_under_device(dev_dir_path)\n",
    "\n",
    "        for trace_dir_path in trace_dir_list:\n",
    "\n",
    "            trace = trace_dir_path.split('/')[-1]\n",
    "            print(date, name, dev, trace)\n",
    "            corresponding_list.append((date, name, dev, trace))\n",
    "\n",
    "            data_dir_path = os.path.join(trace_dir_path, 'data')\n",
    "            rrc_file = [p for p in os.listdir(data_dir_path) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path = os.path.join(data_dir_path, rrc_file)\n",
    "            dl_file_path = os.path.join(data_dir_path, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path = os.path.join(data_dir_path, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_pkgs, dl_excl_pkgs  = loss_excl_cause(dl_file_path, rrc_file_path)\n",
    "            ul_loss_pkgs, ul_excl_pkgs = loss_excl_cause(ul_file_path, rrc_file_path)\n",
    "        \n",
    "            # Count total number\n",
    "            counts = [len(dl_loss_pkgs), len(dl_excl_pkgs), len(ul_loss_pkgs), len(ul_excl_pkgs)]\n",
    "\n",
    "            for k, num in zip(keys, counts):\n",
    "                if name == 'Modem_Action_Test':\n",
    "                    total_Modem_Action_Test[k] += num\n",
    "                elif name == 'Modem_Action_Test_v2_1':\n",
    "                    total_Modem_Action_Test_v2_1[k] += num\n",
    "                elif name == 'Modem_Action_Test_v2_2':\n",
    "                    toral_Modem_Action_Test_v2_2[k] += num\n",
    "                elif name == 'Modem_Control_Group':\n",
    "                    total_Modem_Control_Group[k] += num\n",
    "                elif name == 'Modem_Control_Group2':\n",
    "                    total_Modem_Control_Group2[k] += num\n",
    "                elif name == 'Modem_Control_Group3':\n",
    "                    total_Modem_Control_Group3[k] += num\n",
    "                    \n",
    "            # Count event caused number\n",
    "            analysis_dict = {}\n",
    "\n",
    "            for pkgs, k in zip([dl_loss_pkgs, dl_excl_pkgs, ul_loss_pkgs, ul_excl_pkgs], keys):\n",
    "                \n",
    "                d = {e: 0 for e in count_bdr_events}\n",
    "                \n",
    "                for pkg in pkgs:\n",
    "                    for possible_cause, other in zip(pkg.cause, pkg.others):\n",
    "                        \n",
    "                        for count_e in count_bdr_events:                            \n",
    "                            if count_e == possible_cause:\n",
    "\n",
    "                                # This Conn_Req should be count in RLF III end.\n",
    "                                if ('Conn_Req' in count_e) and ('After RLF III.' in other):\n",
    "                                    continue\n",
    "\n",
    "                                d[count_e] += 1\n",
    "            \n",
    "                analysis_dict[k] = d\n",
    "            \n",
    "            analysis_dicts.append(analysis_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corresponding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group = {}\n",
    "action_group_v2_1 = {}\n",
    "action_group_v2_2 = {}\n",
    "control_group = {} # All + All\n",
    "control_group2 = {} # All + LTE\n",
    "control_group3 = {} # Lock Band+Lock Band\n",
    "\n",
    "for k in keys:\n",
    "    action_group[k] = {case: 0 for case in count_bdr_events}\n",
    "    action_group_v2_1[k] = {case: 0 for case in count_bdr_events}\n",
    "    action_group_v2_2[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group2[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group3[k] = {case: 0 for case in count_bdr_events}\n",
    "\n",
    "for analysis_dict, info in zip(analysis_dicts, corresponding_list):\n",
    "\n",
    "    name, dev = info[1], info[3]\n",
    "    \n",
    "    for k, d in analysis_dict.items():\n",
    "\n",
    "        for kk, v in d.items():\n",
    "\n",
    "            if name == 'Modem_Action_Test':\n",
    "                action_group[k][kk] += v\n",
    "            elif name == 'Modem_Action_Test_v2_1':\n",
    "                action_group_v2_1[k][kk] += v\n",
    "            elif name == 'Modem_Action_Test_v2_2':\n",
    "                action_group_v2_2[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group':\n",
    "                control_group[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group2':\n",
    "                control_group2[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group3':\n",
    "                control_group3[k][kk] += v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corresponding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Here\n",
    "group = action_group_v2_1\n",
    "total = total_Modem_Action_Test_v2_1\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12, 8))\n",
    "\n",
    "metric_categories = []\n",
    "metric_values = []\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    target = group[k]\n",
    "    total_num = total[k]\n",
    "\n",
    "    categories = []\n",
    "    values = []\n",
    "\n",
    "    for e in count_events:\n",
    "        num = 0\n",
    "        for t in bdr:\n",
    "            num += target[t+' '+e]\n",
    "        categories.append(e)\n",
    "        values.append(num/total_num)\n",
    "\n",
    "    metric_categories.append(categories)\n",
    "    metric_values.append(values)\n",
    "\n",
    "# Change Conn_req to Change_Band\n",
    "for categories in metric_categories:\n",
    "    idx = categories.index('Conn_Req')\n",
    "    categories[idx] = 'Change_Band'\n",
    "\n",
    "titles = ['DL Loss', 'DL Excessive Latency', 'Ul Loss', 'UL Excessive Latency']\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    x = np.arange(len(metric_categories[i]))\n",
    "    ax.bar(x, metric_values[i], width=0.6)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_categories[i], rotation=45)\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "    # For not see change band.\n",
    "    ax.set_xlim([0-0.6,9-0.6])\n",
    "\n",
    "\n",
    "# plt.xticks(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Here\n",
    "group = action_group_v2_1\n",
    "total = total_Modem_Action_Test_v2_1\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12, 8))\n",
    "\n",
    "metric_categories = []\n",
    "metric_values = []\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    target = group[k]\n",
    "    total_num = total[k]\n",
    "    \n",
    "    categories = []\n",
    "    values = []\n",
    "\n",
    "    for e in count_events:\n",
    "        \n",
    "        v_split = []\n",
    "        for t in bdr:\n",
    "            num = target[t+' '+e]\n",
    "            v_split.append(num/total_num)\n",
    "        categories.append(e)\n",
    "        values.append(v_split)\n",
    "\n",
    "    metric_categories.append(categories)\n",
    "    metric_values.append(values)\n",
    "\n",
    "# Change Conn_req to Change_Band\n",
    "for categories in metric_categories:\n",
    "    idx = categories.index('Conn_Req')\n",
    "    categories[idx] = 'Change_Band'\n",
    "\n",
    "titles = ['DL Loss', 'DL Excessive Latency', 'Ul Loss', 'UL Excessive Latency']\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "    x1 = [j-0.25 for j in range(len(metric_categories[i]))]\n",
    "    x2 = [j for j in range(len(metric_categories[i]))]\n",
    "    x3 = [j+0.25 for j in range(len(metric_categories[i]))]\n",
    "\n",
    "    v1 = [x[0] for x in metric_values[i]]\n",
    "    v2 = [x[1] for x in metric_values[i]]\n",
    "    v3 = [x[2] for x in metric_values[i]]\n",
    "\n",
    "    ax.bar(x1, v1, width=0.25)\n",
    "    ax.bar(x2, v2, width=0.25)\n",
    "    ax.bar(x3, v3, width=0.25)\n",
    "    ax.set_xticks(x2)\n",
    "    ax.set_xticklabels(metric_categories[i], rotation=45)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.legend(bdr)\n",
    "\n",
    "    # For not see change band.\n",
    "    ax.set_xlim([0-0.6,9-0.6])\n",
    "\n",
    "\n",
    "# plt.xticks(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code counts the number of occurrences for each special case for dual radio transmission.\n",
    "# Still need to revise here.\n",
    "\n",
    "# Record\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "cases = ANALYSIS._fields\n",
    "analysis_dict_all = {}\n",
    "for k in keys:\n",
    "    analysis_dict_all[k] = {case: 0 for case in cases}\n",
    "\n",
    "# # Some settings\n",
    "# count_events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "#           'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "#           'RLF_II', 'RLF_III', 'SCG_RLF', 'Conn_Req']\n",
    "# bdr = ['Before', 'During', 'After']\n",
    "\n",
    "# comb = itertools.product(bdr, count_events)\n",
    "# count_events = [t+' '+ho for t, ho in comb]\n",
    "\n",
    "analysis_dicts = []\n",
    "analysis_dicts2 = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Record how many total packet.\n",
    "total_Modem_Action_Test = {k:0 for k in keys}\n",
    "total_Modem_Action_Test_v2_1 = {k:0 for k in keys}\n",
    "toral_Modem_Action_Test_v2_2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group = {k:0 for k in keys}\n",
    "total_Modem_Control_Group2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group3 = {k:0 for k in keys}\n",
    "\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "    comb = itertools.combinations(dev_dir_list, 2)\n",
    "    date = exp_dir_path.split('/')[-2]\n",
    "    name = exp_dir_path.split('/')[-1]\n",
    "    \n",
    "    for dev_dir_path1, dev_dir_path2 in comb:\n",
    "        \n",
    "        dev1 = dev_dir_path1.split('/')[-1]\n",
    "        dev2 = dev_dir_path2.split('/')[-1]\n",
    "        \n",
    "        trace_dir_list1 = find_trace_under_device(dev_dir_path1)\n",
    "        trace_dir_list2 = find_trace_under_device(dev_dir_path2)\n",
    "\n",
    "        for trace_dir_path1, trace_dir_path2 in zip(trace_dir_list1, trace_dir_list2):\n",
    "\n",
    "            trace = trace_dir_path1.split('/')[-1]\n",
    "            print(date, name, trace, dev1, dev2)\n",
    "\n",
    "            data_dir_path1 = os.path.join(trace_dir_path1, 'data')\n",
    "            rrc_file1 = [p for p in os.listdir(data_dir_path1) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path1 = os.path.join(data_dir_path1, rrc_file1)\n",
    "            dl_file_path1 = os.path.join(data_dir_path1, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path1 = os.path.join(data_dir_path1, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            data_dir_path2 = os.path.join(trace_dir_path2, 'data')\n",
    "            rrc_file2 = [p for p in os.listdir(data_dir_path2) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path2 = os.path.join(data_dir_path2, rrc_file2)\n",
    "            dl_file_path2 = os.path.join(data_dir_path2, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path2 = os.path.join(data_dir_path2, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_pkgs, dl_excl_pkgs  = loss_excl_cause_dual(dl_file_path1, dl_file_path2, rrc_file_path1, rrc_file_path2)\n",
    "            ul_loss_pkgs, ul_excl_pkgs = loss_excl_cause_dual(ul_file_path1, ul_file_path2, rrc_file_path1, rrc_file_path2)\n",
    "\n",
    "            # Special case analysis.\n",
    "            values = [Analyze(dl_loss_pkgs), Analyze(dl_excl_pkgs), Analyze(ul_loss_pkgs), Analyze(ul_excl_pkgs)]\n",
    "            analysis_dict = {k: v for k, v in zip(keys, values)}\n",
    "            \n",
    "            # Count total number\n",
    "            counts = [len(dl_loss_pkgs), len(dl_excl_pkgs), len(ul_loss_pkgs), len(ul_excl_pkgs)]\n",
    "\n",
    "            for k, num in zip(keys, counts):\n",
    "                if name == 'Modem_Action_Test':\n",
    "                    total_Modem_Action_Test[k] += num\n",
    "                elif name == 'Modem_Action_Test_v2_1':\n",
    "                    total_Modem_Action_Test_v2_1[k] += num\n",
    "                elif name == 'Modem_Action_Test_v2_2':\n",
    "                    toral_Modem_Action_Test_v2_2[k] += num\n",
    "                elif name == 'Modem_Control_Group':\n",
    "                    total_Modem_Control_Group[k] += num\n",
    "                elif name == 'Modem_Control_Group2':\n",
    "                    total_Modem_Control_Group2[k] += num\n",
    "                elif name == 'Modem_Control_Group3':\n",
    "                    total_Modem_Control_Group3[k] += num\n",
    "\n",
    "            # Count caused event combinations.\n",
    "            analysis_dict2 = {}\n",
    "\n",
    "            for pkgs, k in zip([dl_loss_pkgs, dl_excl_pkgs, ul_loss_pkgs, ul_excl_pkgs], keys):\n",
    "                \n",
    "                d = {}\n",
    "                \n",
    "                for pkg in pkgs:\n",
    "\n",
    "                    cause1 = [c.split(' ')[-1] for c in pkg.cause1]\n",
    "                    cause2 = [c.split(' ')[-1] for c in pkg.cause2]\n",
    "\n",
    "                    def remove_duplicates(lst):\n",
    "                        return list(set(lst))\n",
    "\n",
    "                    cause1 = remove_duplicates(cause1)\n",
    "                    cause2 = remove_duplicates(cause2)\n",
    "                    counted = []\n",
    "\n",
    "                    for c1, o1 in zip(cause1, pkg.others1):\n",
    "                        if (c1 == 'Conn_Req') and ('After RLF III.' in o1):\n",
    "                            print('working')\n",
    "                            continue\n",
    "                        \n",
    "                        for c2, o2 in zip(cause2, pkg.others2):\n",
    "                            if (c2 == 'Conn_Req') and ('After RLF III.' in o2):\n",
    "                                print('working')\n",
    "                                continue\n",
    "                            \n",
    "                            new_k = ' | '.join(sorted([c1,c2]))\n",
    "\n",
    "                            if new_k  in d.keys() and new_k not in counted:\n",
    "                                d[new_k] += 1\n",
    "                                counted.append(new_k)\n",
    "                            else:\n",
    "                                d[new_k] = 1\n",
    "                                counted.append(new_k)\n",
    "            \n",
    "                analysis_dict2[k] = d\n",
    "            \n",
    "            analysis_dicts2.append(analysis_dict2)\n",
    "\n",
    "            for k in keys:\n",
    "                for i, case in enumerate(cases):\n",
    "                    analysis_dict_all[k][case] += analysis_dict[k][i]\n",
    "            \n",
    "            analysis_dicts.append(analysis_dict)\n",
    "            corresponding_list.append((date, name, trace, dev1, dev2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group = {}\n",
    "action_group_v2_1 = {}\n",
    "action_group_v2_2 = {}\n",
    "control_group = {}\n",
    "control_group2 = {}\n",
    "control_group3 = {}\n",
    "\n",
    "\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "for k in keys:\n",
    "    action_group[k] = {}\n",
    "    action_group_v2_1[k] = {}\n",
    "    action_group_v2_2[k] = {}\n",
    "    control_group[k] = {}\n",
    "    control_group2[k] = {}\n",
    "    control_group3[k] = {}\n",
    "\n",
    "for d2, info in zip(analysis_dicts2, corresponding_list):\n",
    "\n",
    "    name = info[1]\n",
    "\n",
    "    for k in keys:\n",
    "\n",
    "        target = d2[k]\n",
    "        \n",
    "        for event_comb, v in target.items():\n",
    "\n",
    "            if name == 'Modem_Action_Test':\n",
    "                \n",
    "                if event_comb in action_group[k].keys():\n",
    "                    action_group[k][event_comb] += v\n",
    "                else:\n",
    "                    action_group[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Action_Test_v2_1':\n",
    "                \n",
    "                if event_comb in action_group_v2_1[k].keys():\n",
    "                    action_group_v2_1[k][event_comb] += v\n",
    "                else:\n",
    "                    action_group_v2_1[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Action_Test_v2_2':\n",
    "                \n",
    "                if event_comb in action_group_v2_2[k].keys():\n",
    "                    action_group_v2_2[k][event_comb] += v\n",
    "                else:\n",
    "                    action_group_v2_2[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Control_Group':\n",
    "                \n",
    "                if event_comb in control_group[k].keys():\n",
    "                    control_group[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group[k][event_comb] = v\n",
    "            \n",
    "            elif name == 'Modem_Control_Group2':\n",
    "                \n",
    "                if event_comb in control_group2[k].keys():\n",
    "                    control_group2[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group2[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Control_Group3':\n",
    "                \n",
    "                if event_comb in control_group3[k].keys():\n",
    "                    control_group3[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group3[k][event_comb] = v\n",
    "\n",
    "#  test, c1, c2\n",
    "# 9, 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_dicts2[4], analysis_dicts2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group_v2_1, action_group_v2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group = control_group3\n",
    "total = total_Modem_Control_Group3\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    data = target_group[k]\n",
    "    num = total[k]\n",
    "\n",
    "    sorted_items = sorted(data.items(), key=lambda x: -x[1])\n",
    "    sorted_items = [(a, round(b/num, 2)) for (a, b) in sorted_items]\n",
    "    # sorted_items = sorted_items[:10]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items # DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_group, total_Modem_Control_Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_group2, total_Modem_Control_Group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_group, total_Modem_Action_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = ['two_scg_failure', 'one_RLF_one_scg', 'one_scg_failure'] + \\\n",
    "                ['pci_earfcn_identicle_LTE_HO', 'pci_earfcn_identicle_MN_HO', 'pci_earfcn_identicle_MN_HO_to_eNB'] + \\\n",
    "                ['pci_identicle_LTE_HO', 'pci_identicle_MN_HO', 'pci_identicle_MN_HO_to_eNB']\n",
    "\n",
    "for inner_dict in analysis_dict_all.values():\n",
    "    for key in keys_to_remove:\n",
    "        inner_dict.pop(key, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "T = 'Dual radio loss of B1B3+B7B8' # Title\n",
    "categories = [x[0].replace(' | ','+') for x in sorted_items]\n",
    "values = [x[1] for x in sorted_items]\n",
    "\n",
    "# Seaborn\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(x=values, y=categories, palette='viridis')  # barplot'viridis'\n",
    "plt.xlabel('Proportion')  # x\n",
    "plt.title(T)  # \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
