{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import swifter\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files\n",
    "# This function input the path of experiment directory and output a list of device directories of the experiment directory.\n",
    "def find_device_under_exp(exp_dir_path):\n",
    "    dev_dir_list = sorted([os.path.join(exp_dir_path, d) for d in os.listdir(exp_dir_path) if d.startswith('qc') or d.startswith('sm')])\n",
    "    return dev_dir_list\n",
    "\n",
    "def find_trace_under_device(dev_dir_path):\n",
    "    trace_dir_list = sorted([os.path.join(dev_dir_path, d) for d in os.listdir(dev_dir_path)])\n",
    "    return trace_dir_list\n",
    "\n",
    "\n",
    "# Convenience instance\n",
    "class EXPERIMENT():\n",
    "    def __init__(self, exp_dir_path, settings):\n",
    "        self.path = exp_dir_path\n",
    "        self.settings = json.loads(settings)\n",
    "    def __repr__(self):\n",
    "        return f'EXP: {self.path} -> {self.settings}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mi_ho(f):\n",
    "\n",
    "    df = pd.read_csv(f)\n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8)) \n",
    "    nr_pci = 'O'\n",
    "    scells = []\n",
    "\n",
    "    def NR_OTA(idx):\n",
    "\n",
    "        if df[\"type_id\"].iloc[idx] == \"5G_NR_RRC_OTA_Packet\": return True\n",
    "        else: return False\n",
    "    \n",
    "    def LTE_SERV_INFO(idx):\n",
    "\n",
    "        if df[\"type_id\"].iloc[idx] == \"LTE_RRC_Serv_Cell_Info\": return True\n",
    "        else: return False\n",
    "    \n",
    "\n",
    "    def find_1st_after(start_idx, target, look_after=1):\n",
    "        for j in range(start_idx, len(df)):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t_ - t).total_seconds() > look_after:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0'] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_1st_before(start_idx, target, look_before=1):\n",
    "        for j in range(start_idx, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0'] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_1st_before_with_special_value(start_idx, target, target_value, look_before=1):\n",
    "        for j in range(start_idx, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if NR_OTA(j) or LTE_SERV_INFO(j):\n",
    "                continue\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] in [target_value] and not np.isnan(df[target].iloc[j]):\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_in_D_exact(targets):\n",
    "\n",
    "        l = []\n",
    "        # In l : (second, ho_type)\n",
    "        for target in targets:\n",
    "            for ho in D[target]:\n",
    "                l.append(((t - ho.start).total_seconds(), target))\n",
    "\n",
    "        if len(l) != 0:\n",
    "            for x in l:\n",
    "                if (x[0]== 0):\n",
    "                    return x[1]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find_in_D_first_before(targets, look_before=1):\n",
    "\n",
    "        l = []\n",
    "        # In l : (second, ho_type)\n",
    "        for target in targets:\n",
    "            for ho in D[target]:\n",
    "                l.append(((t - ho.end).total_seconds(), target, ho))\n",
    "\n",
    "        if len(l) != 0:\n",
    "            closest = min(filter(lambda x: x[0] > 0, l), key=lambda x: x[0])\n",
    "            if 0 <= closest[0] < look_before:\n",
    "                return closest[1], closest[2]\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "    HO = namedtuple('HO',['start', 'end', 'others', 'trans'], defaults=[None,None,'',''])\n",
    "    \n",
    "    D = {\n",
    "        'Conn_Rel':[], \n",
    "        'Conn_Req':[], # Setup\n",
    "        'LTE_HO': [], # LTE -> newLTE\n",
    "        'MN_HO': [], # LTE + NR -> newLTE + NR\n",
    "        'MN_HO_to_eNB': [], # LTE + NR -> newLTE\n",
    "        'SN_setup': [], # LTE -> LTE + NR => NR setup\n",
    "        'SN_Rel': [], # LTE + NR -> LTE\n",
    "        'SN_HO': [], # LTE + NR -> LTE + newNR  \n",
    "        'RLF_II': [],\n",
    "        'RLF_III': [],\n",
    "        'SCG_RLF': [],\n",
    "        'Add_SCell': [],\n",
    "        }\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # Pass NR RRC packet. In NSA mode, LTE RRC packet include NR packet message.\n",
    "        if NR_OTA(i) or LTE_SERV_INFO(i):\n",
    "            continue\n",
    "\n",
    "        try: lte_pci, lte_earfcn\n",
    "        except: \n",
    "            lte_pci = df[\"PCI\"].iloc[i]\n",
    "            lte_earfcn = int(df[\"Freq\"].iloc[i])\n",
    "\n",
    "        others = ''\n",
    "        t = df[\"Timestamp\"].iloc[i]\n",
    "\n",
    "        if df[\"rrcConnectionRelease\"].iloc[i] == 1:\n",
    "            D['Conn_Rel'].append(HO(start=t))\n",
    "            nr_pci = 'O'\n",
    "\n",
    "        if df[\"rrcConnectionRequest\"].iloc[i] == 1:\n",
    "            \n",
    "            # Define end of rrcConnectionRequest to be rrcConnectionReconfigurationComplete or securityModeComplete.\n",
    "            a = find_1st_after(i, 'rrcConnectionReconfigurationComplete',look_after=2)[0]\n",
    "            b = find_1st_after(i, 'securityModeComplete',look_after=2)[0]\n",
    "        \n",
    "            if a is None and b is None: end = None\n",
    "            elif a is None and b is not None: end = b\n",
    "            elif a is not None and b is None: end = a \n",
    "            else: end = a if a > b else b\n",
    "            \n",
    "            _, idx = find_1st_after(i, 'ueCapabilityInformation',look_after=1)\n",
    "            if idx is not None:\n",
    "                sup_band = df['bandEUTRA'].iloc[idx]\n",
    "                others += f' supported band: {sup_band}.' \n",
    "\n",
    "            serv_cell, serv_freq = df[\"PCI\"].iloc[i], int(df[\"Freq\"].iloc[i])\n",
    "            trans = f'({lte_pci}, {lte_earfcn}) -> ({serv_cell}, {serv_freq})'\n",
    "            \n",
    "            # Check if caused by RLF III.\n",
    "            a, idx = find_1st_before(i, 'rrcConnectionReestablishmentReject', look_before=1)\n",
    "            if a is not None:\n",
    "                others += ' After RLF III.'\n",
    "\n",
    "            D['Conn_Req'].append(HO(start=t,end=end,trans=trans, others=others))\n",
    "\n",
    "            nr_pci = 'O'\n",
    "            lte_pci = serv_cell\n",
    "            lte_earfcn = serv_freq\n",
    "            \n",
    "        if df[\"lte-rrc.t304\"].iloc[i] == 1:\n",
    "            \n",
    "            end, _ = find_1st_after(i, 'rrcConnectionReconfigurationComplete')\n",
    "            serv_cell, target_cell = df[\"PCI\"].iloc[i], int(df['lte_targetPhysCellId'].iloc[i])\n",
    "            serv_freq, target_freq = int(df[\"Freq\"].iloc[i]), int(df['dl-CarrierFreq'].iloc[i])\n",
    "\n",
    "            lte_pci = target_cell\n",
    "            lte_earfcn = target_freq\n",
    "\n",
    "            if df[\"SCellToAddMod-r10\"].iloc[i] == 1:\n",
    "                n =len(str(df[\"SCellIndex-r10.1\"].iloc[i]).split('@'))\n",
    "                others += f' Set up {n} SCell.'\n",
    "            else:\n",
    "                scells = []\n",
    "            \n",
    "            if serv_freq != target_freq:\n",
    "                a,b = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 1)\n",
    "                others += \" Inter frequency HO.\"\n",
    "                if a is not None:\n",
    "                    others += \" Near after RLF.\"\n",
    "                \n",
    "            if df[\"nr-rrc.t304\"].iloc[i] == 1 and df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "\n",
    "                    a, _ = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 2)\n",
    "                    \n",
    "                    if a is not None:\n",
    "\n",
    "                        ho_type, ho = find_in_D_first_before(['RLF_II', 'RLF_III'], 2)\n",
    "                        others += f' Near after RLF of trans: {ho.trans}.'\n",
    "\n",
    "                    else:\n",
    "                        \n",
    "                        ho_type, _ = find_in_D_first_before(['MN_HO_to_eNB', 'SN_Rel'], 2)\n",
    "                        if ho_type is not None:\n",
    "                            others += f' Near after {ho_type}.'\n",
    "\n",
    "                    ori_serv = nr_pci\n",
    "                    nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "                    trans = f'({serv_cell}, {serv_freq}) | {ori_serv} -> {nr_pci}'\n",
    "                    D['SN_setup'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "                    trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | {nr_pci}'\n",
    "                    D['MN_HO'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "            else:\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "\n",
    "                    a, b = find_1st_before(i, \"scgFailureInformationNR-r15\")\n",
    "                    if a is not None:\n",
    "                        others += \" Caused by scg-failure.\"\n",
    "                    \n",
    "                    orig_serv = nr_pci\n",
    "                    nr_pci = 'O'\n",
    "                    trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                    D['SN_Rel'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "                    \n",
    "                else:\n",
    "\n",
    "                    a, _ = find_1st_before(i,\"rrcConnectionSetup\",3)\n",
    "                    if a is not None:\n",
    "                        others += ' Near After connection setup.'\n",
    "                    if nr_pci == 'O':\n",
    "                        trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | {nr_pci}'\n",
    "                        D['LTE_HO'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "                    else:\n",
    "                        orig_serv = nr_pci\n",
    "                        nr_pci = 'O'\n",
    "                        trans = f'({serv_cell}, {serv_freq}) -> ({target_cell}, {target_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                        D['MN_HO_to_eNB'].append(HO(start=t, end=end, others=others, trans=trans))\n",
    "\n",
    "\n",
    "        if df[\"nr-rrc.t304\"].iloc[i] == 1 and not df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "\n",
    "            end, _ = find_1st_after(i,'rrcConnectionReconfigurationComplete')\n",
    "        \n",
    "            serv_cell, serv_freq = df[\"PCI\"].iloc[i], int(df[\"Freq\"].iloc[i])\n",
    "            orig_serv = nr_pci\n",
    "            nr_pci = int(df['nr_physCellId'].iloc[i])\n",
    "            trans = f'({serv_cell}, {serv_freq}) | {orig_serv} -> {nr_pci}'\n",
    "            D['SN_HO'].append(HO(start=t,end=end,trans=trans))\n",
    "\n",
    "\n",
    "        if df[\"rrcConnectionReestablishmentRequest\"].iloc[i] == 1:\n",
    "\n",
    "            end1, _ = find_1st_after(i, 'rrcConnectionReestablishmentComplete', look_after=1)\n",
    "            b, _ = find_1st_after(i, 'rrcConnectionReestablishmentReject', look_after=1)\n",
    "            end2, _ = find_1st_after(i, 'securityModeComplete',look_after=3)\n",
    "\n",
    "            others += ' ' + df[\"reestablishmentCause\"].iloc[i] + '.'\n",
    "            scells = []\n",
    "\n",
    "            c, _ = find_1st_before(i, 'scgFailureInformationNR-r15', 1)\n",
    "            if c != None:\n",
    "                others  += ' caused by scgfailure.'\n",
    "                \n",
    "            serv_cell, rlf_cell = df[\"PCI\"].iloc[i], int(df['physCellId.3'].iloc[i])\n",
    "            serv_freq = int(df['Freq'].iloc[i])\n",
    "            \n",
    "            # Type II & Type III\n",
    "            if end1 is not None: \n",
    "\n",
    "                orig_serv = nr_pci\n",
    "                nr_pci = 'O'\n",
    "                _, idx = find_1st_before_with_special_value(i, 'PCI', rlf_cell, look_before=10)\n",
    "                rlf_freq = int(df['Freq'].iloc[idx])\n",
    "                trans = f'({rlf_cell}, {rlf_freq}) -> ({serv_cell}, {serv_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                D['RLF_II'].append(HO(start=t,end=end1,others=others,trans=trans))\n",
    "\n",
    "                lte_pci = serv_cell\n",
    "                lte_earfcn = serv_freq\n",
    "\n",
    "            elif b is not None and end2 is not None:\n",
    "                \n",
    "                orig_serv = nr_pci\n",
    "                nr_pci = 'O'\n",
    "                _, idx = find_1st_before_with_special_value(i, 'PCI', rlf_cell, look_before=10)\n",
    "                rlf_freq = int(df['Freq'].iloc[idx])\n",
    "\n",
    "                _, idx = find_1st_after(i, \"rrcConnectionRequest\", 2)\n",
    "                recon_cell, recon_freq = df['PCI'].iloc[idx], int(float(df['Freq'].iloc[idx]))\n",
    "                \n",
    "                trans = f'({rlf_cell}, {rlf_freq}) -> ({recon_cell}, {recon_freq}) | {orig_serv} -> {nr_pci}'\n",
    "                D['RLF_III'].append(HO(start=t,end=end2,others=others,trans=trans)) \n",
    "\n",
    "                # lte_pci, lte_earfcn will be updated in rrcConnectionRequest.     \n",
    "                \n",
    "            else:\n",
    "\n",
    "                others+=' No end.'\n",
    "                D['RLF_II'].append(HO(start=t,others=others))\n",
    "                print('No end for RLF')\n",
    "\n",
    "        if df[\"scgFailureInformationNR-r15\"].iloc[i] == 1:\n",
    "\n",
    "            others += ' ' + df[\"failureType-r15\"].iloc[i] + '.'\n",
    "            a, idx1 = find_1st_after(i, \"rrcConnectionReestablishmentRequest\", look_after=1)\n",
    "            b, idx2 = find_1st_after(i, \"lte-rrc.t304\", look_after=10)\n",
    "\n",
    "            if a is not None:\n",
    "\n",
    "                end1, _ = find_1st_after(idx1, 'rrcConnectionReestablishmentComplete', look_after=1)\n",
    "                b, _ = find_1st_after(idx1, 'rrcConnectionReestablishmentReject', look_after=1)\n",
    "                end2 = find_1st_after(idx1, 'securityModeComplete',look_after=3)[0]\n",
    "\n",
    "                others += ' Result in rrcReestablishment.'\n",
    "                    \n",
    "                # Type II & Type III Result\n",
    "                if end1 is not None: \n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end1,others=others))\n",
    "                elif b is not None and end2 is not None: \n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end2,others=others))\n",
    "                else:\n",
    "                    others += ' No end.'\n",
    "                    D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "                    print('No end for scg failure result in rrcReestablishment.')\n",
    "\n",
    "            elif b is not None:\n",
    "\n",
    "                end, _ = find_1st_after(idx2, 'rrcConnectionReconfigurationComplete')\n",
    "                serv_cell, target_cell = df[\"PCI\"].iloc[idx2], df['lte_targetPhysCellId'].iloc[idx2]\n",
    "                serv_freq, target_freq = int(df[\"Freq\"].iloc[idx2]), df['dl-CarrierFreq'].iloc[idx2]\n",
    "                # We do not change nr_pci here. Instead, we will change it at gNB_Rel event.\n",
    "                trans = f'({serv_cell}, {serv_freq}) | {nr_pci} -> O'\n",
    "                \n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    others += ' Result in gNB release.'\n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end,others=others,trans=trans))\n",
    "                else:\n",
    "                    others += ' Result in MN HO to eNB.'\n",
    "                    D['SCG_RLF'].append(HO(start=t,end=end,others=others,trans=trans))                  \n",
    "\n",
    "            else:\n",
    "\n",
    "                print('No end for scg failure.')\n",
    "                others += ' No end.'\n",
    "                D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "        \n",
    "        if df['SCellToAddMod-r10'].iloc[i] == 1 and df['physCellId-r10'].iloc[i] != 'nr or cqi report':\n",
    "\n",
    "            others = ''\n",
    "            pcis = str(df[\"physCellId-r10\"].iloc[i]).split('@')\n",
    "            freqs = str(df[\"dl-CarrierFreq-r10\"].iloc[i]).split('@')\n",
    "            orig_scells = scells\n",
    "            scells = [(int(float(pci)), int(float(freq))) for pci, freq in zip(pcis, freqs)]\n",
    "\n",
    "            others += f' Set up {len(scells)} SCell.'\n",
    "            trans = f'{orig_scells} -> {scells}'\n",
    "\n",
    "            end, _ = find_1st_after(i,'rrcConnectionReconfigurationComplete')\n",
    "            \n",
    "            a, _ = find_1st_before(i, \"rrcConnectionReestablishmentRequest\", 3)\n",
    "            if a is not None:\n",
    "                others += ' Near after RLF.'\n",
    "\n",
    "            a = find_in_D_exact(['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', 'SN_Rel'])\n",
    "            if a is not None:\n",
    "                others += f' With {a}.'\n",
    "\n",
    "            D['Add_SCell'].append(HO(start=t,end=end,others=others, trans=trans))\n",
    "    \n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_excl_cause(loss_lat_file_path, rrc_file_path):\n",
    "\n",
    "    loss_lat_df = pd.read_csv(loss_lat_file_path)\n",
    "\n",
    "    loss_cond = loss_lat_df['lost'] == True\n",
    "    loss_packets = loss_lat_df[loss_cond]\n",
    "    loss_packets = loss_packets.reset_index(drop=True)\n",
    "    loss_packets['Timestamp'] = pd.to_datetime(loss_packets['Timestamp'])\n",
    "\n",
    "    exc_lat = 0.1\n",
    "    excl_cond = (loss_cond==False) & (loss_lat_df['latency'] > exc_lat)\n",
    "    excl_packets = loss_lat_df[excl_cond]\n",
    "    excl_packets = excl_packets.reset_index(drop=True)\n",
    "    excl_packets['Timestamp'] = pd.to_datetime(excl_packets['Timestamp'])\n",
    "\n",
    "    HO_dict = parse_mi_ho(rrc_file_path)\n",
    "    events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', \n",
    "              'SN_Rel', 'SN_HO', 'RLF_II', 'RLF_III', 'SCG_RLF',\n",
    "              'Conn_Req']\n",
    "    slots = [dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1),\n",
    "             dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=2), dt.timedelta(seconds=2), dt.timedelta(seconds=2),\n",
    "             dt.timedelta(seconds=1)]\n",
    "    \n",
    "    LOSS_PKT = namedtuple('LOSS_PKT',['timestamp', 'seq', 'cause', 'trans', 'trans_time', 'others'], defaults=['', 0, [], [], [], []])\n",
    "\n",
    "    LOSS_PKTs = []\n",
    "\n",
    "    for i in range(len(loss_packets)):\n",
    "\n",
    "        loss_packet = loss_packets.iloc[i]\n",
    "        loss_packet_timestamp = loss_packet['Timestamp']\n",
    "        seq = loss_packet['seq']\n",
    "        \n",
    "        cause = []\n",
    "        trans = []\n",
    "        others = []\n",
    "        trans_time = []\n",
    "\n",
    "        for HO_type, slot in zip(events, slots):\n",
    "            \n",
    "            HOs = HO_dict[HO_type]  \n",
    "\n",
    "            for h in HOs:\n",
    "                \n",
    "                if h.start - slot < loss_packet_timestamp < h.start:\n",
    "                    cause.append(f'Before {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append(h.start)\n",
    "                    others.append(h.others)\n",
    "                elif (h.end is not None) and (h.start < loss_packet_timestamp < h.end):\n",
    "                    cause.append(f'During {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append((h.start, h.end))\n",
    "                    others.append(h.others)\n",
    "                elif (h.end is not None) and (h.end < loss_packet_timestamp < h.end + slot):\n",
    "                    cause.append(f'After {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append(h.end)\n",
    "                    others.append(h.others)\n",
    "\n",
    "        LOSS_PKTs.append(LOSS_PKT(timestamp=loss_packet_timestamp, seq=seq, cause=cause, others=others))\n",
    "                \n",
    "    EXCL_PKT = namedtuple('EXCL_PKT',['timestamp', 'seq', 'cause', 'trans', 'trans_time', 'others'], defaults=['', 0, [], [], [], []])\n",
    "\n",
    "    events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', \n",
    "              'SN_Rel', 'SN_HO', 'RLF_II', 'RLF_III', 'SCG_RLF',\n",
    "              'Conn_Req']\n",
    "    slots = [dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1),\n",
    "             dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=2), dt.timedelta(seconds=2), dt.timedelta(seconds=1),\n",
    "             dt.timedelta(seconds=1)]\n",
    "    \n",
    "    EXCL_PKTs = []\n",
    "\n",
    "    for i in range(len(excl_packets)):\n",
    "\n",
    "        excl_packet = excl_packets.iloc[i]\n",
    "        excl_packet_timestamp = excl_packet['Timestamp']\n",
    "        seq = excl_packet['seq']\n",
    "\n",
    "        cause = []\n",
    "        trans = []\n",
    "        trans_time = []\n",
    "        others = []\n",
    "\n",
    "        for HO_type, slot in zip(events, slots):\n",
    "            \n",
    "            HOs = HO_dict[HO_type]   \n",
    "            for h in HOs:\n",
    "                \n",
    "                if h.start - slot < excl_packet_timestamp < h.start:\n",
    "                    cause.append(f'Before {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append(h.start)\n",
    "                    others.append(h.others)\n",
    "                elif (h.end is not None) and (h.start < excl_packet_timestamp < h.end):\n",
    "                    cause.append(f'During {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append((h.start, h.end))\n",
    "                    others.append(h.others)\n",
    "                elif (h.end is not None) and (h.end < excl_packet_timestamp < h.end + slot):\n",
    "                    cause.append(f'After {HO_type}') \n",
    "                    trans.append(h.trans)\n",
    "                    trans_time.append(h.end)\n",
    "                    others.append(h.others)\n",
    "\n",
    "        EXCL_PKTs.append(EXCL_PKT(timestamp=excl_packet_timestamp, seq=seq, cause=cause, trans=trans, trans_time=trans_time, others=others))\n",
    "    \n",
    "    return LOSS_PKTs, EXCL_PKTs\n",
    "\n",
    "#\n",
    "\n",
    "def loss_excl_cause_dual(loss_lat_file_path1, loss_lat_file_path2, rrc_file_path1, rrc_file_path2):\n",
    "\n",
    "    df1 = pd.read_csv(loss_lat_file_path1)\n",
    "    df2 = pd.read_csv(loss_lat_file_path2)\n",
    "\n",
    "    start_seq = df1['seq'].iloc[0] if df1['seq'].iloc[0] >=  df2['seq'].iloc[0] else df2['seq'].iloc[0]\n",
    "    end_seq = df1['seq'].iloc[-1] if df1['seq'].iloc[-1] <=  df2['seq'].iloc[-1] else df2['seq'].iloc[-1]\n",
    "\n",
    "    cond1 = (df1['seq'] >= start_seq) & (df1['seq'] <= end_seq)\n",
    "    df1 = df1[cond1]\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    cond2 = (df2['seq'] >= start_seq) & (df2['seq'] <= end_seq)\n",
    "    df2 = df2[cond2]\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "\n",
    "    # Loss calculate for dual radios redundant packets.\n",
    "    loss_cond = (df1['lost'] == True) & (df2['lost'] == True)\n",
    "\n",
    "    loss_packets1 = df1[loss_cond]\n",
    "    loss_packets1 = loss_packets1.reset_index(drop=True)\n",
    "    loss_packets1['Timestamp'] = pd.to_datetime(loss_packets1['Timestamp'])\n",
    "\n",
    "    loss_packets2 = df2[loss_cond]\n",
    "    loss_packets2 = loss_packets2.reset_index(drop=True)\n",
    "    loss_packets2['Timestamp'] = pd.to_datetime(loss_packets2['Timestamp'])\n",
    "\n",
    "    # Excexxive latency calculate for dual radios redundant packets.\n",
    "    exc_lat = 0.1 \n",
    "    excl_cond1 = (loss_cond==False) & (df1['latency'] > exc_lat)\n",
    "    excl_cond2 = (loss_cond==False) & (df2['latency'] > exc_lat)\n",
    "    excl_cond = (excl_cond1 == True) & (excl_cond2 == True)\n",
    "    \n",
    "    excl_packets1 = df1[excl_cond]\n",
    "    excl_packets1 = excl_packets1.reset_index(drop=True)\n",
    "    excl_packets1['Timestamp'] = pd.to_datetime(excl_packets1['Timestamp'])\n",
    "\n",
    "    excl_packets2 = df2[excl_cond]\n",
    "    excl_packets2 = excl_packets2.reset_index(drop=True)\n",
    "    excl_packets2['Timestamp'] = pd.to_datetime(excl_packets2['Timestamp'])\n",
    "\n",
    "    HO_dict1 = parse_mi_ho(rrc_file_path1)\n",
    "    HO_dict2 = parse_mi_ho(rrc_file_path2)\n",
    "    \n",
    "    events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', \n",
    "              'SN_Rel', 'SN_HO', 'RLF_II', 'RLF_III', 'SCG_RLF',\n",
    "              'Conn_Req']\n",
    "    slots = [dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1),\n",
    "             dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=2), dt.timedelta(seconds=2), dt.timedelta(seconds=2),\n",
    "             dt.timedelta(seconds=1)]\n",
    "    \n",
    "    LOSS_PKT_DUAL = namedtuple('LOSS_PKT_DUAL',\n",
    "                               ['timestamp1', 'timestamp2', 'seq', 'cause1', 'cause2', 'trans1', 'trans2', 'others1', 'others2','trans1_time', 'trans2_time'], \n",
    "                               defaults=['', '', 0, [], [], [], [], [], [], [], []])\n",
    "\n",
    "    LOSS_PKT_DUALs = []\n",
    "\n",
    "    for i in range(len(loss_packets1)):\n",
    "\n",
    "        loss_packet1 = loss_packets1.iloc[i]\n",
    "        loss_packet1_timestamp = loss_packet1['Timestamp']\n",
    "\n",
    "        loss_packet2 = loss_packets2.iloc[i]\n",
    "        loss_packet2_timestamp = loss_packet2['Timestamp']\n",
    "\n",
    "        seq = loss_packet1['seq']\n",
    "        \n",
    "        cause1, cause2 = [], []\n",
    "        trans1, trans2 = [], []\n",
    "        others1, others2 = [], []\n",
    "        trans1_time, trans2_time = [], []\n",
    "\n",
    "        for HO_type, slot in zip(events, slots):\n",
    "            \n",
    "            HOs1 = HO_dict1[HO_type]\n",
    "            HOs2 = HO_dict2[HO_type]   \n",
    "\n",
    "            for h in HOs1:\n",
    "                \n",
    "                if h.start - slot < loss_packet1_timestamp < h.start:\n",
    "                    cause1.append(f'Before {HO_type}') \n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append(h.start)\n",
    "                    others1.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.start < loss_packet1_timestamp < h.end):\n",
    "                    cause1.append(f'During {HO_type}')\n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append((h.start, h.end))\n",
    "                    others1.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.end < loss_packet1_timestamp < h.end + slot):\n",
    "                    cause1.append(f'After {HO_type}')\n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append(h.end)\n",
    "                    others1.append(h.others)\n",
    "            \n",
    "            for h in HOs2:\n",
    "                \n",
    "                if h.start - slot < loss_packet2_timestamp < h.start:\n",
    "                    cause2.append(f'Before {HO_type}') \n",
    "                    trans2.append(h.trans)\n",
    "                    trans2_time.append(h.start)\n",
    "                    others2.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.start < loss_packet2_timestamp < h.end):\n",
    "                    cause2.append(f'During {HO_type}')\n",
    "                    trans2.append(h.trans)\n",
    "                    trans2_time.append((h.start, h.end))\n",
    "                    others2.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.end < loss_packet2_timestamp < h.end + slot):\n",
    "                    cause2.append(f'After {HO_type}')\n",
    "                    trans2.append(h.trans)\n",
    "                    trans2_time.append(h.end)\n",
    "                    others2.append(h.others)\n",
    "    \n",
    "        LOSS_PKT_DUALs.append(LOSS_PKT_DUAL(timestamp1=loss_packet1_timestamp, timestamp2=loss_packet2_timestamp, seq=seq, \n",
    "        cause1=cause1, cause2=cause2, trans1=trans1, trans2=trans2, others1=others1, others2=others2, trans1_time=trans1_time, trans2_time=trans2_time))\n",
    "                \n",
    "    slot = dt.timedelta(seconds=2)\n",
    "    EXCL_PKT_DUAL = namedtuple('EXCL_PKT_DUAL',\n",
    "                               ['timestamp1', 'timestamp2', 'seq', 'cause1', 'cause2', 'trans1', 'trans2', 'others1', 'others2','trans1_time', 'trans2_time'], \n",
    "                               defaults=['', '', 0, [], [], [], [], [], [], [], []])\n",
    "\n",
    "    events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', 'SN_setup', \n",
    "              'SN_Rel', 'SN_HO', 'RLF_II', 'RLF_III', 'SCG_RLF',\n",
    "              'Conn_Req']\n",
    "    slots = [dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=1),\n",
    "             dt.timedelta(seconds=1), dt.timedelta(seconds=1), dt.timedelta(seconds=2), dt.timedelta(seconds=2), dt.timedelta(seconds=2),\n",
    "             dt.timedelta(seconds=1)]\n",
    "    \n",
    "    \n",
    "    EXCL_PKT_DUALs = []\n",
    "\n",
    "    for i in range(len(excl_packets1)):\n",
    "\n",
    "        excl_packet1 = excl_packets1.iloc[i]\n",
    "        excl_packet1_timestamp = excl_packet1['Timestamp']\n",
    "        excl_packet2 = excl_packets2.iloc[i]\n",
    "        excl_packet2_timestamp = excl_packet2['Timestamp']\n",
    "\n",
    "        seq = excl_packet1['seq']\n",
    "\n",
    "        cause1, cause2 = [], []\n",
    "        trans1, trans2 = [], []\n",
    "        others1, others2 = [], []\n",
    "        trans1_time, trans2_time = [], []\n",
    "\n",
    "        for HO_type, slot in zip(events, slots):\n",
    "            \n",
    "            HOs1 = HO_dict1[HO_type]\n",
    "            HOs2 = HO_dict2[HO_type]\n",
    "\n",
    "            for h in HOs1:\n",
    "                \n",
    "                if h.start - slot < excl_packet1_timestamp < h.start:\n",
    "                    cause1.append(f'Before {HO_type}') \n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append(h.start)\n",
    "                    others1.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.start < excl_packet1_timestamp < h.end):\n",
    "                    cause1.append(f'During {HO_type}')\n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append((h.start, h.end))\n",
    "                    others1.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.end < excl_packet1_timestamp < h.end + slot):\n",
    "                    cause1.append(f'After {HO_type}')\n",
    "                    trans1.append(h.trans)\n",
    "                    trans1_time.append(h.end)\n",
    "                    others1.append(h.others)\n",
    "\n",
    "            for h in HOs2:\n",
    "                \n",
    "                if h.start - slot < excl_packet2_timestamp < h.start:\n",
    "                    cause2.append(f'Before {HO_type}') \n",
    "                    trans2.append(h.trans)\n",
    "                    trans2_time.append(h.start)\n",
    "                    others2.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.start < excl_packet2_timestamp < h.end):\n",
    "                    cause2.append(f'During {HO_type}')\n",
    "                    trans2.append(h.trans)\n",
    "                    trans2_time.append((h.start, h.end))\n",
    "                    others2.append(h.others)\n",
    "\n",
    "                elif (h.end is not None) and (h.end < excl_packet2_timestamp < h.end + slot):\n",
    "                    cause2.append(f'After {HO_type}')\n",
    "                    trans2.append(h.trans)                   \n",
    "                    trans2_time.append(h.end)\n",
    "                    others2.append(h.others)\n",
    "\n",
    "        EXCL_PKT_DUALs.append(EXCL_PKT_DUAL(timestamp1=excl_packet1_timestamp, timestamp2=excl_packet2_timestamp, seq=seq, \n",
    "        cause1=cause1, cause2=cause2, trans1=trans1, trans2=trans2, others1=others1, others2=others2, trans1_time=trans1_time, trans2_time=trans2_time))\n",
    "\n",
    "    return LOSS_PKT_DUALs, EXCL_PKT_DUALs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse every case.\n",
    "EVENTS = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "          'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "          'RLF_II', 'RLF_III', 'SCG_RLF']\n",
    "\n",
    "REs = [r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| O', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+)', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O',\n",
    "       r'\\((\\d+), (\\d+)\\) \\| O -> (\\d+)', \n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> O', \n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> (\\d+)',\n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O', \n",
    "       r'\\((\\d+), (\\d+)\\) -> \\((\\d+), (\\d+)\\) \\| (\\d+) -> O',\n",
    "       r'\\((\\d+), (\\d+)\\) \\| (\\d+) -> O']\n",
    "\n",
    "def get_re_from_type(event):\n",
    "    if event in EVENTS:\n",
    "        index = EVENTS.index(event)\n",
    "        return REs[index]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "EVENTS1 = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB']\n",
    "\n",
    "CASES = ['all', 'two_event', 'one_event'] + \\\n",
    "        ['two_RLF', 'two_scg_failure', 'one_RLF_one_scg', 'one_RLF', 'one_scg_failure'] + \\\n",
    "        ['two_exact_identicle_HO'] + [f'two_exact_identicle_{type}' for type in EVENTS] + \\\n",
    "        ['two_identicle_RLF_SN_setup'] + \\\n",
    "        ['pci_earfcn_identicle_HO_eNB'] + [f'pci_earfcn_identicle_{type}' for type in EVENTS1] + \\\n",
    "        ['pci_identicle_HO_eNB'] + [f'pci_identicle_{type}' for type in EVENTS1] + \\\n",
    "        ['pci_identicle_HO_gNB'] + \\\n",
    "        ['pci_earfcn_identicle_RLF'] + ['pci_identicle_RLF'] + ['pci_identicle_sRLF'] + \\\n",
    "        ['pci_identicle_RLF_setup_cause_pci', 'pci_identicle_RLF_setup_cause_pci_earfcn'] + \\\n",
    "        ['SN_setup_of_RLF_MN_HO_to_eNB', 'SN_setup_of_RLF_SN_Rel', 'SN_setup_of_MN_HO_to_eNB_SN_Rel'] + \\\n",
    "        ['SN_setup_same_cause_rlf_pci', 'SN_setup_same_cause_rlf_pci_earfcn'] + \\\n",
    "        ['SN_setup_of_same_cause_MN_HO_to_eNB', 'SN_setup_of_same_cause_SN_Rel', 'SN_setup_same_cause_rlf']\n",
    "\n",
    "ANALYSIS = namedtuple('ANALYSIS', CASES, defaults = [0]*len(CASES))\n",
    "\n",
    "def Analyze(pkgs):\n",
    "    \n",
    "    # Useful functions.\n",
    "    # \n",
    "    def remove_elements(A, B, condition):\n",
    "        indexes_to_remove = [i for i, a in enumerate(A) if condition(a)]\n",
    "        for index in sorted(indexes_to_remove, reverse=True):\n",
    "            del A[index]\n",
    "            del B[index]\n",
    "        return A, B\n",
    "    \n",
    "    # Case functions.\n",
    "    # Case source and target cause type, trans are exactly the same.\n",
    "    # A is cause list and B is trans list. \n",
    "    # Totally identical in trans string.\n",
    "    def find_exact_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2)):\n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "                    \n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans \"pci\" and \"earfcn\" are the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB'].\n",
    "    def find_pci_earfcn_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(1)}, {match.group(2)}) -> ({match.group(3)}, {match.group(4)})'\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in EVENTS1)\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in EVENTS1)\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # Case source and target cause type, trans \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB'].\n",
    "    def find_pci_identicle(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(1)} -> {match.group(3)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in EVENTS1)\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in EVENTS1)\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with 'gNB_HO.\n",
    "    def find_pci_identicle_gNB(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(3)}, {match.group(4)})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['SN_HO'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['SN_HO'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans source \"pci\" and \"earfcn\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['RLF_II'] now.\n",
    "    def find_pci_earfcn_identicle_RLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(1)}, {match.group(2)})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['RLF_II'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # Case source and target cause type, trans source \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with ['RLF_II'] now.\n",
    "    def find_pci_identicle_RLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(1)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['RLF_II'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "    \n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_RLF_SN_Setup_Same_Cause_pci(A1, B1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event, seq):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        def extract_coordinates2(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II', 'RLF_III'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        B1_ = [extract_coordinates(b1, a1, 1) for b1, a1 in zip(B1_, A1_)]\n",
    "        C2_ = [extract_coordinates2(c2, r'\\((\\d+), (\\d+)\\)', 1) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and b1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_RLF_SN_Setup_Same_Cause_pci_earfcn(A1, B1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event, seq):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        def extract_coordinates2(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['RLF_II', 'RLF_III'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        B1_ = [extract_coordinates(b1, a1, [1, 2]) for b1, a1 in zip(B1_, A1_)]\n",
    "        C2_ = [extract_coordinates2(c2, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and b1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_SN_Setup_Same_Cause_pci_earfcn(A1, C1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "        \n",
    "        def extract_coordinates(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'({match.group(seq[0])}, {match.group(seq[1])})'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; C1_ = [element for element in C1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, C1_ = remove_elements(A1_, C1_, lambda a: a not in ['SN_setup'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        C1_ = [extract_coordinates(c1, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c1 in C1_]\n",
    "        C2_ = [extract_coordinates(c2, r'\\((\\d+), (\\d+)\\)', [1, 2]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, c1) in enumerate(zip(A1_, C1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and c1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "\n",
    "    # This case checks whether a loss/excl dual packet with \n",
    "    # cause1 of RLF and cause2 of SN_setup has the same origin \n",
    "    # of rlf cell.\n",
    "    def find_SN_Setup_Same_Cause_pci(A1, C1, A2, C2):\n",
    "\n",
    "        L = []\n",
    "        \n",
    "        def extract_coordinates(input_string, pattern, seq):\n",
    "\n",
    "            match = re.search(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(seq[0])}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; C1_ = [element for element in C1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; C2_ = [element for element in C2]\n",
    "        A1_, C1_ = remove_elements(A1_, C1_, lambda a: a not in ['SN_setup'])\n",
    "        A2_, C2_ = remove_elements(A2_, C2_, lambda a: a not in ['SN_setup'])\n",
    "        C1_ = [extract_coordinates(c1, r'\\((\\d+), (\\d+)\\)', [1]) for c1 in C1_]\n",
    "        C2_ = [extract_coordinates(c2, r'\\((\\d+), (\\d+)\\)', [1]) for c2 in C2_]\n",
    "\n",
    "        for i, (a1, c1) in enumerate(zip(A1_, C1_)):    \n",
    "            for j, (a2, c2) in enumerate(zip(A2_, C2_)):                \n",
    "                if a1 == a2 and c1 == c2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "\n",
    "    # Case source and target cause type, trans source \"pci\" are exactly the same.\n",
    "    # A is cause list and B is trans list.\n",
    "    # This case only deal with 'SCG_RLF' now.\n",
    "    def find_pci_identicle_sRLF(A1, B1, A2, B2):\n",
    "\n",
    "        L = []\n",
    "\n",
    "        def extract_coordinates(input_string, event):\n",
    "            pattern = get_re_from_type(event)\n",
    "            match = re.match(pattern, input_string)\n",
    "            \n",
    "            if match:\n",
    "                return f'{match.group(3)}'\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        # Take out before, during, and after prefix.\n",
    "        A1_ = [a1.split(' ')[-1] for a1 in A1]; B1_ = [element for element in B1]\n",
    "        A2_ = [a2.split(' ')[-1] for a2 in A2]; B2_ = [element for element in B2]\n",
    "        A1_, B1_ = remove_elements(A1_, B1_, lambda a: a not in ['SCG_RLF'])\n",
    "        A2_, B2_ = remove_elements(A2_, B2_, lambda a: a not in ['SCG_RLF'])\n",
    "        B1_ = [extract_coordinates(b1, a1) for b1, a1 in zip(B1_, A1_)]\n",
    "        B2_ = [extract_coordinates(b2, a2) for b2, a2 in zip(B2_, A2_)]\n",
    "\n",
    "        for i, (a1, b1) in enumerate(zip(A1_, B1_)):    \n",
    "            for j, (a2, b2) in enumerate(zip(A2_, B2_)):                \n",
    "                if a1 == a2 and b1 == b2:\n",
    "                    L.append((i, j))\n",
    "\n",
    "        return L\n",
    "\n",
    "    # Count the number of every case. \n",
    "    nums = {k: 0 for k in CASES}\n",
    "    nums['all'] = len(pkgs)\n",
    "\n",
    "    for pkg in pkgs:\n",
    "        \n",
    "        cause1_string = \"\".join(pkg.cause1)\n",
    "        cause2_string = \"\".join(pkg.cause2)\n",
    "        others1_string = \"\".join(pkg.others1)\n",
    "        others2_string = \"\".join(pkg.others2)\n",
    "\n",
    "        # Event related check.\n",
    "        if cause1_string and cause2_string:\n",
    "            nums['two_event'] += 1\n",
    "        elif cause1_string or cause2_string:\n",
    "            nums['one_event'] += 1\n",
    "\n",
    "        # RLF related case.\n",
    "        if ('RLF_' in cause1_string) and ('RLF_' in cause2_string):   \n",
    "            nums['two_RLF'] += 1\n",
    "\n",
    "            # Nested Events\n",
    "            l1 = find_pci_identicle_RLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "            if len(l1) != 0: \n",
    "                nums['pci_identicle_RLF'] += 1\n",
    "                \n",
    "                l2 = find_pci_earfcn_identicle_RLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "                if len(l2) != 0:\n",
    "                    nums['pci_earfcn_identicle_RLF'] += 1\n",
    "\n",
    "        elif ('RLF_' in cause1_string) and ('SN_setup' in cause2_string and 'Near after RLF' in others2_string):\n",
    "\n",
    "            l1 = find_RLF_SN_Setup_Same_Cause_pci(pkg.cause1, pkg.trans1, pkg.cause2, pkg.others2)\n",
    "            if len(l1) != 0:\n",
    "                nums['pci_identicle_RLF_setup_cause_pci'] += 1\n",
    "\n",
    "                l2 = find_RLF_SN_Setup_Same_Cause_pci_earfcn(pkg.cause1, pkg.trans1, pkg.cause2, pkg.others2)\n",
    "                if len(l1) != 0:\n",
    "                    nums['pci_identicle_RLF_setup_cause_pci_earfcn'] += 1\n",
    "\n",
    "        elif ('SN_setup' in cause1_string and 'Near after RLF' in others1_string) and ('RLF_' in cause2_string):\n",
    "\n",
    "            l1 = find_RLF_SN_Setup_Same_Cause_pci(pkg.cause2, pkg.trans2, pkg.cause1, pkg.others1)\n",
    "            if len(l1) != 0:\n",
    "                nums['pci_identicle_RLF_setup_cause_pci'] += 1\n",
    "\n",
    "                l2 = find_RLF_SN_Setup_Same_Cause_pci_earfcn(pkg.cause2, pkg.trans2, pkg.cause1, pkg.others1)\n",
    "                if len(l1) != 0:\n",
    "                    nums['pci_identicle_RLF_setup_cause_pci_earfcn'] += 1\n",
    "\n",
    "        elif ('RLF_' in cause1_string) or ('RLF_' in cause2_string):  \n",
    "            nums['one_RLF'] += 1\n",
    "\n",
    "        # elif ('SCG_RLF' in cause1_string) and ('SCG_RLF' in cause2_string): \n",
    "        #     nums['two_scg_failure'] += 1\n",
    "        \n",
    "        # elif (('RLF_' in cause1_string) and ('SCG_RLF' in cause2_string) ) or (('SCG_RLF' in cause1_string) and ('RLF_' in cause2_string) ):\n",
    "        #     nums['one_RLF_one_scg'] += 1\n",
    "\n",
    "        # elif ('SCG_RLF' in cause1_string) or ('SCG_RLF' in cause2_string): \n",
    "        #     nums['one_scg_failure'] += 1\n",
    "\n",
    "        # elif ('RLF_' in cause1_string) or ('RLF_' in cause2_string):  \n",
    "        #     nums['one_RLF'] += 1\n",
    "\n",
    "        # SN setup case observation.\n",
    "        if ('SN_setup' in cause1_string) and ('SN_setup' in cause2_string):\n",
    "            \n",
    "            p = r' Near after (.*?)\\.'\n",
    "            \n",
    "            try: match1 = re.search(p, others1_string); s1 = match1.group(1)\n",
    "            except: s1 = 'Unknown'\n",
    "\n",
    "            try: match2 = re.search(p, others2_string); s2 = match2.group(1)\n",
    "            except: s2 = 'Unknown'\n",
    "\n",
    "            if ('RLF' in s1) and ('RLF' in s2):\n",
    "                \n",
    "                nums['SN_setup_same_cause_rlf'] += 1\n",
    "\n",
    "                l1 = find_SN_Setup_Same_Cause_pci(pkg.cause1, pkg.others1, pkg.cause2, pkg.others2)\n",
    "                if len(l1) != 0:\n",
    "                    nums['SN_setup_same_cause_rlf_pci'] += 1\n",
    "\n",
    "                    l2 = find_SN_Setup_Same_Cause_pci_earfcn(pkg.cause1, pkg.others1, pkg.cause2, pkg.others2)\n",
    "                    if len(l2) != 0:\n",
    "                        nums['SN_setup_same_cause_rlf_pci_earfcn'] += 1\n",
    "\n",
    "            elif ('MN_HO_to_eNB' in s1) and ('MN_HO_to_eNB' in s2):\n",
    "                nums['SN_setup_of_same_cause_MN_HO_to_eNB'] += 1\n",
    "            elif ('SN_Rel' in s1) and ('SN_Rel' in s2):\n",
    "                nums['SN_setup_of_same_cause_SN_Rel'] += 1\n",
    "            elif ('RLF' in s1 or 'RLF' in s2) and ('MN_HO_to_eNB' in s1 or 'MN_HO_to_eNB' in s2):\n",
    "                nums['SN_setup_of_RLF_MN_HO_to_eNB'] += 1\n",
    "            elif ('RLF' in s1 or 'RLF' in s2) and ('SN_Rel' in s1 or 'SN_Rel' in s2):\n",
    "                nums['SN_setup_of_RLF_SN_Rel'] += 1\n",
    "            elif ('MN_HO_to_eNB' in s1 or 'MN_HO_to_eNB' in s2) and ('SN_Rel' in s1 or 'SN_Rel' in s2):\n",
    "                nums['SN_setup_of_MN_HO_to_eNB_SN_Rel'] += 1\n",
    "\n",
    "        # Identicle HO related case.\n",
    "        # Case Exact Identicle.\n",
    "        L1 = find_exact_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2)\n",
    "        if len(L1) != 0: nums['two_exact_identicle_HO'] += 1\n",
    "        \n",
    "        identicle_types = []\n",
    "        for (i, _) in L1:\n",
    "            _, ho_type = pkg.cause1[i].split(' ')\n",
    "            identicle_types.append(ho_type)\n",
    "            nums[f'two_exact_identicle_{ho_type}'] += 1\n",
    "\n",
    "        if ('RLF_II' in identicle_types or 'RLF_III' in identicle_types) and ('SN_setup' in identicle_types):\n",
    "            nums[f'two_identicle_RLF_SN_setup'] += 1\n",
    "\n",
    "        # eNB HO related\n",
    "        L2 = find_pci_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        \n",
    "        if len(L2) != 0: \n",
    "            nums['pci_identicle_HO_eNB'] += 1\n",
    "\n",
    "            l1 = find_pci_earfcn_identicle(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "            if len(l1) != 0: \n",
    "                nums['pci_earfcn_identicle_HO_eNB'] += 1\n",
    "\n",
    "        # gNB related\n",
    "        L3 = find_pci_identicle_gNB(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        if len(L3) != 0: nums['pci_identicle_HO_gNB'] += 1\n",
    "\n",
    "        # srlf related cases.\n",
    "        L4 = find_pci_identicle_sRLF(pkg.cause1, pkg.trans1, pkg.cause2, pkg.trans2) \n",
    "        if len(L4) != 0: nums['pci_identicle_sRLF'] += 1\n",
    "\n",
    "    return ANALYSIS(*nums.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This place give a XXXX-XX-XX.md file and find the experiment directory path\n",
    "# and the corresponding band settings. It will be presented by a list of special\n",
    "# instance EXPERIMENTs.\n",
    "md_files = [\n",
    "    # '/home/wmnlab/D/database/2023-08-29/2023-08-29.md', \n",
    "    # '/home/wmnlab/D/database/2023-09-12_1/2023-09-12.md',\n",
    "    # '/home/wmnlab/D/database/2023-09-22/2023-09-22.md',\n",
    "    # '/home/wmnlab/D/database/2023-10-24/2023-10-24.md',\n",
    "    '/home/wmnlab/D/database/2023-11-21/2023-11-21.md'\n",
    "    ]\n",
    "EXPs = []\n",
    "\n",
    "for md_file_path in md_files:\n",
    "\n",
    "    date_dir_path = os.path.dirname(md_file_path)\n",
    "\n",
    "    with open(md_file_path) as f:\n",
    "\n",
    "        exp = f.readline()[:-1]\n",
    "        settings = f.readline()[:-1]\n",
    "\n",
    "        while exp != '#endif' and settings:\n",
    "            E = EXPERIMENT(os.path.join(date_dir_path, exp), settings)\n",
    "            EXPs.append(E)\n",
    "            exp = f.readline()[:-1]\n",
    "            settings = f.readline()[:-1]\n",
    "\n",
    "pprint(EXPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code evaluate the nearby event of a packet loss/ excl packet of a single radio performance.\n",
    "\n",
    "# Record\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "cases = ANALYSIS._fields\n",
    "analysis_dict_all = {}\n",
    "for k in keys:\n",
    "    analysis_dict_all[k] = {case: 0 for case in cases}\n",
    "\n",
    "analysis_dicts = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Some settings\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "count_events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "          'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "          'RLF_II', 'RLF_III', 'SCG_RLF', 'Conn_Req']\n",
    "bdr = ['Before', 'During', 'After']\n",
    "\n",
    "comb = itertools.product(bdr, count_events)\n",
    "count_bdr_events = [t+' '+ho for t, ho in comb]\n",
    "\n",
    "analysis_dicts = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Record how many total packet.\n",
    "total_Modem_Action_Test = {k:0 for k in keys}\n",
    "total_Modem_Control_Group = {k:0 for k in keys}\n",
    "total_Modem_Control_Group2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group3 = {k:0 for k in keys}\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "    date = exp_dir_path.split('/')[-2]\n",
    "    name = exp_dir_path.split('/')[-1]\n",
    "\n",
    "    # Below can be used to filtering.\n",
    "    # if name != 'Modem_Action_Test':\n",
    "    #     continue\n",
    "\n",
    "    for dev_dir_path in dev_dir_list:\n",
    "        \n",
    "        dev = dev_dir_path.split('/')[-1]\n",
    "        trace_dir_list = find_trace_under_device(dev_dir_path)\n",
    "\n",
    "        for trace_dir_path in trace_dir_list:\n",
    "\n",
    "            trace = trace_dir_path.split('/')[-1]\n",
    "            print(date, name, dev, trace)\n",
    "            corresponding_list.append((date, name, dev, trace))\n",
    "\n",
    "            data_dir_path = os.path.join(trace_dir_path, 'data')\n",
    "            rrc_file = [p for p in os.listdir(data_dir_path) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path = os.path.join(data_dir_path, rrc_file)\n",
    "            dl_file_path = os.path.join(data_dir_path, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path = os.path.join(data_dir_path, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_pkgs, dl_excl_pkgs  = loss_excl_cause(dl_file_path, rrc_file_path)\n",
    "            ul_loss_pkgs, ul_excl_pkgs = loss_excl_cause(ul_file_path, rrc_file_path)\n",
    "        \n",
    "            # Count total number\n",
    "            counts = [len(dl_loss_pkgs), len(dl_excl_pkgs), len(ul_loss_pkgs), len(ul_excl_pkgs)]\n",
    "\n",
    "            for k, num in zip(keys, counts):\n",
    "                if name == 'Modem_Action_Test':\n",
    "                    total_Modem_Action_Test[k] += num\n",
    "                elif name == 'Modem_Control_Group':\n",
    "                    total_Modem_Control_Group[k] += num\n",
    "                elif name == 'Modem_Control_Group2':\n",
    "                    total_Modem_Control_Group2[k] += num\n",
    "                elif name == 'Modem_Control_Group3':\n",
    "                    total_Modem_Control_Group3[k] += num\n",
    "                    \n",
    "            # Count event caused number\n",
    "            analysis_dict = {}\n",
    "\n",
    "            for pkgs, k in zip([dl_loss_pkgs, dl_excl_pkgs, ul_loss_pkgs, ul_excl_pkgs], keys):\n",
    "                \n",
    "                d = {e: 0 for e in count_bdr_events}\n",
    "                \n",
    "                for pkg in pkgs:\n",
    "                    for possible_cause, other in zip(pkg.cause, pkg.others):\n",
    "                        \n",
    "                        for count_e in count_bdr_events:                            \n",
    "                            if count_e == possible_cause:\n",
    "\n",
    "                                # This Conn_Req should be count in RLF III end.\n",
    "                                if ('Conn_Req' in count_e) and ('After RLF III.' in other):\n",
    "                                    continue\n",
    "\n",
    "                                d[count_e] += 1\n",
    "            \n",
    "                analysis_dict[k] = d\n",
    "            \n",
    "            analysis_dicts.append(analysis_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corresponding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group = {}\n",
    "control_group = {} # All + All\n",
    "control_group2 = {} # All + LTE\n",
    "control_group3 = {} # Lock Band+Lock Band\n",
    "\n",
    "for k in keys:\n",
    "    action_group[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group2[k] = {case: 0 for case in count_bdr_events}\n",
    "    control_group3[k] = {case: 0 for case in count_bdr_events}\n",
    "\n",
    "for analysis_dict, info in zip(analysis_dicts, corresponding_list):\n",
    "\n",
    "    name, dev = info[1], info[3]\n",
    "    \n",
    "    for k, d in analysis_dict.items():\n",
    "\n",
    "        for kk, v in d.items():\n",
    "\n",
    "            if name == 'Modem_Action_Test':\n",
    "                action_group[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group':\n",
    "                control_group[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group2':\n",
    "                control_group2[k][kk] += v\n",
    "            elif name == 'Modem_Control_Group3':\n",
    "                control_group3[k][kk] += v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corresponding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Here\n",
    "group = control_group\n",
    "total = total_Modem_Control_Group\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12, 8))\n",
    "\n",
    "metric_categories = []\n",
    "metric_values = []\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    target = group[k]\n",
    "    total_num = total[k]\n",
    "\n",
    "    categories = []\n",
    "    values = []\n",
    "\n",
    "    for e in count_events:\n",
    "        num = 0\n",
    "        for t in bdr:\n",
    "            num += target[t+' '+e]\n",
    "        categories.append(e)\n",
    "        values.append(num/total_num)\n",
    "\n",
    "    metric_categories.append(categories)\n",
    "    metric_values.append(values)\n",
    "\n",
    "# Change Conn_req to Change_Band\n",
    "for categories in metric_categories:\n",
    "    idx = categories.index('Conn_Req')\n",
    "    categories[idx] = 'Change_Band'\n",
    "\n",
    "titles = ['DL Loss', 'DL Excessive Latency', 'Ul Loss', 'UL Excessive Latency']\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "\n",
    "    x = np.arange(len(metric_categories[i]))\n",
    "    ax.bar(x, metric_values[i], width=0.6)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metric_categories[i], rotation=45)\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "    # For not see change band.\n",
    "    ax.set_xlim([0-0.6,9-0.6])\n",
    "\n",
    "\n",
    "# plt.xticks(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Here\n",
    "group = control_group\n",
    "total = total_Modem_Control_Group\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize=(12, 8))\n",
    "\n",
    "metric_categories = []\n",
    "metric_values = []\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    target = group[k]\n",
    "    total_num = total[k]\n",
    "    \n",
    "    categories = []\n",
    "    values = []\n",
    "\n",
    "    for e in count_events:\n",
    "        \n",
    "        v_split = []\n",
    "        for t in bdr:\n",
    "            num = target[t+' '+e]\n",
    "            v_split.append(num/total_num)\n",
    "        categories.append(e)\n",
    "        values.append(v_split)\n",
    "\n",
    "    metric_categories.append(categories)\n",
    "    metric_values.append(values)\n",
    "\n",
    "# Change Conn_req to Change_Band\n",
    "for categories in metric_categories:\n",
    "    idx = categories.index('Conn_Req')\n",
    "    categories[idx] = 'Change_Band'\n",
    "\n",
    "titles = ['DL Loss', 'DL Excessive Latency', 'Ul Loss', 'UL Excessive Latency']\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "    x1 = [j-0.25 for j in range(len(metric_categories[i]))]\n",
    "    x2 = [j for j in range(len(metric_categories[i]))]\n",
    "    x3 = [j+0.25 for j in range(len(metric_categories[i]))]\n",
    "\n",
    "    v1 = [x[0] for x in metric_values[i]]\n",
    "    v2 = [x[1] for x in metric_values[i]]\n",
    "    v3 = [x[2] for x in metric_values[i]]\n",
    "\n",
    "    ax.bar(x1, v1, width=0.25)\n",
    "    ax.bar(x2, v2, width=0.25)\n",
    "    ax.bar(x3, v3, width=0.25)\n",
    "    ax.set_xticks(x2)\n",
    "    ax.set_xticklabels(metric_categories[i], rotation=45)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.legend(bdr)\n",
    "\n",
    "    # For not see change band.\n",
    "    ax.set_xlim([0-0.6,9-0.6])\n",
    "\n",
    "\n",
    "# plt.xticks(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Radio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code counts the number of occurrences for each special case for dual radio transmission.\n",
    "# Still need to revise here.\n",
    "\n",
    "# Record\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "cases = ANALYSIS._fields\n",
    "analysis_dict_all = {}\n",
    "for k in keys:\n",
    "    analysis_dict_all[k] = {case: 0 for case in cases}\n",
    "\n",
    "# # Some settings\n",
    "# count_events = ['LTE_HO', 'MN_HO', 'MN_HO_to_eNB', \n",
    "#           'SN_setup', 'SN_Rel', 'SN_HO', \n",
    "#           'RLF_II', 'RLF_III', 'SCG_RLF', 'Conn_Req']\n",
    "# bdr = ['Before', 'During', 'After']\n",
    "\n",
    "# comb = itertools.product(bdr, count_events)\n",
    "# count_events = [t+' '+ho for t, ho in comb]\n",
    "\n",
    "analysis_dicts = []\n",
    "analysis_dicts2 = []\n",
    "corresponding_list = []\n",
    "\n",
    "# Record how many total packet.\n",
    "total_Modem_Action_Test = {k:0 for k in keys}\n",
    "total_Modem_Control_Group = {k:0 for k in keys}\n",
    "total_Modem_Control_Group2 = {k:0 for k in keys}\n",
    "total_Modem_Control_Group3 = {k:0 for k in keys}\n",
    "\n",
    "\n",
    "for EXP in EXPs:\n",
    "\n",
    "    exp_dir_path = EXP.path\n",
    "    settings = EXP.settings \n",
    "\n",
    "    dev_dir_list = find_device_under_exp(exp_dir_path)\n",
    "    comb = itertools.combinations(dev_dir_list, 2)\n",
    "    date = exp_dir_path.split('/')[-2]\n",
    "    name = exp_dir_path.split('/')[-1]\n",
    "    \n",
    "    for dev_dir_path1, dev_dir_path2 in comb:\n",
    "        \n",
    "        dev1 = dev_dir_path1.split('/')[-1]\n",
    "        dev2 = dev_dir_path2.split('/')[-1]\n",
    "        \n",
    "        trace_dir_list1 = find_trace_under_device(dev_dir_path1)\n",
    "        trace_dir_list2 = find_trace_under_device(dev_dir_path2)\n",
    "\n",
    "        for trace_dir_path1, trace_dir_path2 in zip(trace_dir_list1, trace_dir_list2):\n",
    "\n",
    "            trace = trace_dir_path1.split('/')[-1]\n",
    "            print(date, name, trace, dev1, dev2)\n",
    "\n",
    "            data_dir_path1 = os.path.join(trace_dir_path1, 'data')\n",
    "            rrc_file1 = [p for p in os.listdir(data_dir_path1) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path1 = os.path.join(data_dir_path1, rrc_file1)\n",
    "            dl_file_path1 = os.path.join(data_dir_path1, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path1 = os.path.join(data_dir_path1, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            data_dir_path2 = os.path.join(trace_dir_path2, 'data')\n",
    "            rrc_file2 = [p for p in os.listdir(data_dir_path2) if p.endswith('_rrc.csv')][0]\n",
    "            rrc_file_path2 = os.path.join(data_dir_path2, rrc_file2)\n",
    "            dl_file_path2 = os.path.join(data_dir_path2, 'udp_dnlk_loss_latency.csv')\n",
    "            ul_file_path2 = os.path.join(data_dir_path2, 'udp_uplk_loss_latency.csv')\n",
    "\n",
    "            dl_loss_pkgs, dl_excl_pkgs  = loss_excl_cause_dual(dl_file_path1, dl_file_path2, rrc_file_path1, rrc_file_path2)\n",
    "            ul_loss_pkgs, ul_excl_pkgs = loss_excl_cause_dual(ul_file_path1, ul_file_path2, rrc_file_path1, rrc_file_path2)\n",
    "\n",
    "            # Special case analysis.\n",
    "            values = [Analyze(dl_loss_pkgs), Analyze(dl_excl_pkgs), Analyze(ul_loss_pkgs), Analyze(ul_excl_pkgs)]\n",
    "            analysis_dict = {k: v for k, v in zip(keys, values)}\n",
    "            \n",
    "            # Count total number\n",
    "            counts = [len(dl_loss_pkgs), len(dl_excl_pkgs), len(ul_loss_pkgs), len(ul_excl_pkgs)]\n",
    "\n",
    "            for k, num in zip(keys, counts):\n",
    "                if name == 'Modem_Action_Test':\n",
    "                    total_Modem_Action_Test[k] += num\n",
    "                elif name == 'Modem_Control_Group':\n",
    "                    total_Modem_Control_Group[k] += num\n",
    "                elif name == 'Modem_Control_Group2':\n",
    "                    total_Modem_Control_Group2[k] += num\n",
    "                elif name == 'Modem_Control_Group3':\n",
    "                    total_Modem_Control_Group3[k] += num\n",
    "\n",
    "            # Count caused event combinations.\n",
    "            analysis_dict2 = {}\n",
    "\n",
    "            for pkgs, k in zip([dl_loss_pkgs, dl_excl_pkgs, ul_loss_pkgs, ul_excl_pkgs], keys):\n",
    "                \n",
    "                d = {}\n",
    "                \n",
    "                for pkg in pkgs:\n",
    "\n",
    "                    cause1 = [c.split(' ')[-1] for c in pkg.cause1]\n",
    "                    cause2 = [c.split(' ')[-1] for c in pkg.cause2]\n",
    "\n",
    "                    def remove_duplicates(lst):\n",
    "                        return list(set(lst))\n",
    "\n",
    "                    cause1 = remove_duplicates(cause1)\n",
    "                    cause2 = remove_duplicates(cause2)\n",
    "                    counted = []\n",
    "\n",
    "                    for c1, o1 in zip(cause1, pkg.others1):\n",
    "                        if (c1 == 'Conn_Req') and ('After RLF III.' in o1):\n",
    "                            print('working')\n",
    "                            continue\n",
    "                        \n",
    "                        for c2, o2 in zip(cause2, pkg.others2):\n",
    "                            if (c2 == 'Conn_Req') and ('After RLF III.' in o2):\n",
    "                                print('working')\n",
    "                                continue\n",
    "                            \n",
    "                            new_k = ' | '.join(sorted([c1,c2]))\n",
    "\n",
    "                            if new_k  in d.keys() and new_k not in counted:\n",
    "                                d[new_k] += 1\n",
    "                                counted.append(new_k)\n",
    "                            else:\n",
    "                                d[new_k] = 1\n",
    "                                counted.append(new_k)\n",
    "            \n",
    "                analysis_dict2[k] = d\n",
    "            \n",
    "            analysis_dicts2.append(analysis_dict2)\n",
    "\n",
    "            for k in keys:\n",
    "                for i, case in enumerate(cases):\n",
    "                    analysis_dict_all[k][case] += analysis_dict[k][i]\n",
    "            \n",
    "            analysis_dicts.append(analysis_dict)\n",
    "            corresponding_list.append((date, name, trace, dev1, dev2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group = {}\n",
    "control_group = {}\n",
    "control_group2 = {}\n",
    "control_group3 = {}\n",
    "\n",
    "\n",
    "keys = ['dl_loss', 'dl_excl', 'ul_loss', 'ul_excl']\n",
    "for k in keys:\n",
    "    action_group[k] = {}\n",
    "    control_group[k] = {}\n",
    "    control_group2[k] = {}\n",
    "    control_group3[k] = {}\n",
    "\n",
    "for d2, info in zip(analysis_dicts2, corresponding_list):\n",
    "\n",
    "    name = info[1]\n",
    "\n",
    "    for k in keys:\n",
    "\n",
    "        target = d2[k]\n",
    "        \n",
    "        for event_comb, v in target.items():\n",
    "\n",
    "            if name == 'Modem_Action_Test':\n",
    "                \n",
    "                if event_comb in action_group[k].keys():\n",
    "                    action_group[k][event_comb] += v\n",
    "                else:\n",
    "                    action_group[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Control_Group':\n",
    "                \n",
    "                if event_comb in control_group[k].keys():\n",
    "                    control_group[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group[k][event_comb] = v\n",
    "            \n",
    "            elif name == 'Modem_Control_Group2':\n",
    "                \n",
    "                if event_comb in control_group2[k].keys():\n",
    "                    control_group2[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group2[k][event_comb] = v\n",
    "\n",
    "            elif name == 'Modem_Control_Group3':\n",
    "                \n",
    "                if event_comb in control_group3[k].keys():\n",
    "                    control_group3[k][event_comb] += v\n",
    "                else:\n",
    "                    control_group3[k][event_comb] = v\n",
    "\n",
    "#  test, c1, c2\n",
    "# 9, 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group = control_group3\n",
    "total = total_Modem_Control_Group3\n",
    "\n",
    "for k in keys:\n",
    "    \n",
    "    data = target_group[k]\n",
    "    num = total[k]\n",
    "\n",
    "    sorted_items = sorted(data.items(), key=lambda x: -x[1])\n",
    "    sorted_items = [(a, round(b/num, 2)) for (a, b) in sorted_items]\n",
    "    # sorted_items = sorted_items[:10]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_items # DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_group, total_Modem_Control_Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control_group2, total_Modem_Control_Group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_group, total_Modem_Action_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = ['two_scg_failure', 'one_RLF_one_scg', 'one_scg_failure'] + \\\n",
    "                ['pci_earfcn_identicle_LTE_HO', 'pci_earfcn_identicle_MN_HO', 'pci_earfcn_identicle_MN_HO_to_eNB'] + \\\n",
    "                ['pci_identicle_LTE_HO', 'pci_identicle_MN_HO', 'pci_identicle_MN_HO_to_eNB']\n",
    "\n",
    "for inner_dict in analysis_dict_all.values():\n",
    "    for key in keys_to_remove:\n",
    "        inner_dict.pop(key, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "T = 'Dual radio loss of B1B3+B7B8' # Title\n",
    "categories = [x[0].replace(' | ','+') for x in sorted_items]\n",
    "values = [x[1] for x in sorted_items]\n",
    "\n",
    "# 使用Seaborn绘制横条图\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(x=values, y=categories, palette='viridis')  # 使用barplot函数绘制横条图，设置颜色主题为'viridis'\n",
    "plt.xlabel('Proportion')  # x轴标签\n",
    "plt.title(T)  # 图表标题\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
