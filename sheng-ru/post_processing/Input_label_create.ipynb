{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(L):\n",
    "    return sum(L)/len(L)\n",
    "\n",
    "def mi_event_parsing(miinfofile):\n",
    "    def nr_pci_track():\n",
    "        if miinfofile.loc[i, \"PCI\"] == 65535: ## 65535 is for samgsung phone.\n",
    "            nr_pci = '-'\n",
    "        else:\n",
    "            nr_pci = miinfofile.loc[i, \"PCI\"]\n",
    "        return nr_pci\n",
    "\n",
    "    nr_pci = None ## Initial Unknown\n",
    "     \n",
    "    lte_4G_handover_list = []   #4G 狀態下LTE eNB 的 handover\n",
    "    \n",
    "    nr_setup_list = []          #gNB cell addition\n",
    "    nr_handover_list = []       #gNB cell changes (eNB stays the same)\n",
    "    nr_removal_list = []        #gNB cell removal\n",
    "        \n",
    "    lte_5G_handover_list = []   #(eNB1, gNB1) -> (eNB2, gNB1) #gNB stays the same\n",
    "    nr_lte_handover_list = []   #both NR cell and LTE cell have handover\n",
    "    \n",
    "    eNB_to_MN_list = []\n",
    "    MN_to_eNB_list = []\n",
    "    \n",
    "    scg_failure_list = []       #gNB handover failure\n",
    "    reestablish_list_type2 = [] #eNB handover failure\n",
    "    reestablish_list_type3 = []\n",
    "    \n",
    "    nr_handover = 0\n",
    "    nr_handover_start_index = None\n",
    "    lte_handover = 0\n",
    "    lte_handover_start_index = None\n",
    "    nr_release = 0\n",
    "    nr_release_start_index = None\n",
    "    \n",
    "    lte_failure = 0\n",
    "    lte_failure_start_index = None\n",
    "    \n",
    "    handover_num = 0\n",
    "    \n",
    "    for i in range(len(miinfofile)):\n",
    "        if miinfofile.loc[i, \"type_id\"] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            nr_pci = nr_pci_track()\n",
    "            continue\n",
    "            \n",
    "        if miinfofile.loc[i, \"nr-rrc.t304\"]:\n",
    "            if nr_handover == 0:    \n",
    "                nr_handover = 1\n",
    "                nr_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"lte-rrc.t304\"]:\n",
    "            if lte_handover == 0:\n",
    "                lte_handover = 1\n",
    "                lte_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"nr-Config-r15: release (0)\"]:\n",
    "            if nr_release == 0:\n",
    "                nr_release = 1\n",
    "                nr_release_start_index = i\n",
    "           \n",
    "        if (nr_handover or lte_handover or nr_release) and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            handover_num +=1\n",
    "        \n",
    "        \n",
    "        #handover 種類分類\n",
    "        #------------------------------------------------------------------------------\n",
    "        if lte_handover and not nr_handover and not nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just lte cell handover event\n",
    "            lte_handover = 0\n",
    "            lte_4G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "\n",
    "        if lte_handover and not nr_handover and nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:    # LTE Ho and nr release \n",
    "            lte_handover = 0\n",
    "            nr_release = 0\n",
    "            MN_to_eNB_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "        \n",
    "        if nr_handover and not lte_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just nr cell handover event\n",
    "            nr_handover = 0\n",
    "            if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                nr_setup_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "            else:\n",
    "                nr_handover_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            #additional judgement:\n",
    "            #----------------------------\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"] and nr_pci != None:\n",
    "            #    print(\"Warning: dualConnectivityPHR setup may not mean nr cell addition\", mi_file, i)\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]==0 and not (nr_pci != None and nr_pci != miinfofile.loc[nr_handover_start_index, \"nr_pci\"]): \n",
    "            #    print(\"Warning: nr-rrc.t304 without dualConnectivityPHR setup may not mean nr cell handover\", mi_file, i, nr_handover_start_index, miinfofile.loc[nr_handover_start_index, \"nr_pci\"], nr_pci)\n",
    "                \n",
    "        if lte_handover and nr_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:      # both nr cell and lte cell handover event\n",
    "            lte_handover = 0\n",
    "            nr_handover = 0\n",
    "            if nr_pci == miinfofile.loc[lte_handover_start_index, \"nr_physCellId\"]: \n",
    "                lte_5G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            else:\n",
    "                ##############\n",
    "                if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                    eNB_to_MN_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "                else:\n",
    "                    nr_lte_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if not lte_handover and  nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            nr_release=0\n",
    "            nr_removal_list.append([miinfofile.loc[nr_release_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if miinfofile.loc[i, \"scgFailureInformationNR-r15\"]:\n",
    "            scg_failure_list.append([miinfofile.loc[i, \"time\"], miinfofile.loc[i, \"time\"]]) \n",
    "            \n",
    "        if miinfofile.loc[i, \"rrcConnectionReestablishmentRequest\"]:\n",
    "            if lte_failure == 0:\n",
    "                lte_failure = 1\n",
    "                lte_failure_start_index = i\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishment\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type2.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishmentReject\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type3.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "            \n",
    "    return [lte_4G_handover_list, nr_setup_list, nr_handover_list, nr_removal_list, lte_5G_handover_list, nr_lte_handover_list, eNB_to_MN_list, MN_to_eNB_list, scg_failure_list, reestablish_list_type2, reestablish_list_type3], handover_num\n",
    "\n",
    "def collect_ho_event(mi_rrc_df):\n",
    "        l, _ = mi_event_parsing(mi_rrc_df)\n",
    "        for i in range(0, 11):\n",
    "            l[i] = [j[0] for j in l[i]]\n",
    "        d = {'lte': l[0], 'nr_setup': l[1], 'gNB_ho': l[2], 'nr_rel': l[3], \"MN_changed\": l[4],\"MN_SN_changed\": l[5], \"eNB_to_MN_changed\": l[6], \"MN_to_eNB_changed\": l[7], \"gNB_fail\": l[8], \"type2_fail\": l[9], \"type3_fail\": l[10]}\n",
    "        return d\n",
    "\n",
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_dict(band, d):\n",
    "    D = d.copy()\n",
    "    for key in list(d.keys()):\n",
    "        if not key.endswith(' '+band):\n",
    "            D.pop(key)\n",
    "    return D\n",
    "\n",
    "class ss_dict:\n",
    "    def __init__(self,pd_data=None,d=None): ## Input pd_df.iloc[index]\n",
    "        self.dict = {'PCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    def serv_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        serv_cell_id = pd_data[\"Serving Cell Index\"]\n",
    "        pci = pd_data[\"PCI\"]\n",
    "        rsrp = float(pd_data[\"RSRP(dBm)\"])\n",
    "        rsrq = float(pd_data[\"RSRQ(dB)\"])\n",
    "        t = pd_data[\"time\"]\n",
    "        if serv_cell_id == \"PCell\":\n",
    "            self.dict['PCell'][0].append(rsrp)\n",
    "            self.dict['PCell'][1].append(rsrq)\n",
    "            self.dict['PCell'][2].append(t)\n",
    "            self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "        else:\n",
    "            self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "            # s = pci + ' ' + self.earfcn\n",
    "            # if s in \n",
    "    def nei_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        t = pd_data[\"time\"]\n",
    "        for i in range(9, len(pd_data), 3):\n",
    "            if pd_data[i] == '-':\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[pd_data[i]+' '+earfcn] = [[rsrp], [rsrq], [t]]              \n",
    "    \n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] = d1[key][0] + d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return ss_dict(d=d1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def sort_dict_by_time(self):\n",
    "        def sort_element(element):\n",
    "            d1 = [ [element[0][i], element[1][i], element[2][i]] for i in range(len(element[0]))]\n",
    "            d1.sort(key=lambda data:data[2])\n",
    "            RSRP = [i[0] for i in d1]\n",
    "            RSRQ = [i[1] for i in d1]\n",
    "            T = [i[2] for i in d1]\n",
    "            return [RSRP, RSRQ, T]\n",
    "        sorted_D = {}\n",
    "        for k in list(self.dict.keys()):\n",
    "            sorted_D[k] = sort_element(self.dict[k])\n",
    "        self.dict = sorted_D\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tcp file\n",
    "file_dir = '/home/wmnlab/Code_Test_Space/tcp_label'\n",
    "pcap_csv_files = os.listdir(file_dir)\n",
    "pcap_csv_files = [i for i in pcap_csv_files if 'label' not in i]\n",
    "pcap_csv_files.sort()\n",
    "pcap_csv_files.insert(0, pcap_csv_files.pop(-1))\n",
    "pcap_csv_files = [os.path.join(file_dir, i) for i in pcap_csv_files]\n",
    "\n",
    "print(pcap_csv_files)\n",
    "pcap_df_list = []\n",
    "Retransmission = []\n",
    "\n",
    "for file in pcap_csv_files:\n",
    "    tcp_df = pd.read_csv(file, sep='@')\n",
    "    tcp_df[\"frame.time\"] = tcp_df[\"frame.time\"].apply(lambda x: pd.to_datetime(x))\n",
    "    tcp_df[\"frame.time\"] = tcp_df[\"frame.time\"].apply(lambda x: x.tz_convert(None)) + dt.timedelta(hours=8)\n",
    "    A = tcp_df.drop(tcp_df.index[tcp_df['tcp.analysis.ack_rtt'] == '-'])\n",
    "    A = A.drop(A.index[A['ip.dst'] != '192.168.1.248'])\n",
    "    pcap_df_list.append(A)\n",
    "    B = [tcp_df['frame.time'].iloc[i] for i in range(len(tcp_df)) if tcp_df['tcp.analysis.retransmission'].iloc[i] == '1' or tcp_df['tcp.analysis.fast_retransmission'].iloc[i] == '1']\n",
    "    Retransmission.append(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對齊前後\n",
    "front_cut = 0\n",
    "back_cut = 30\n",
    "\n",
    "for i in range(len(pcap_df_list)):\n",
    "    if i == 0:\n",
    "        start = pcap_df_list[0][\"frame.time\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "        end = pcap_df_list[0][\"frame.time\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "    else:\n",
    "        if pcap_df_list[i][\"frame.time\"].iloc[0] > start:\n",
    "            start = pcap_df_list[i][\"frame.time\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "        if pcap_df_list[i][\"frame.time\"].iloc[-1] < end:\n",
    "            end = pcap_df_list[i][\"frame.time\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "            \n",
    "start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "n = int((end - start).total_seconds())\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DL 數據選擇\n",
    "setup = {'sm05': 'All', 'sm06': 'B1+B3', 'sm07': 'B3+B28', 'sm08': 'B1+B28'}\n",
    "\n",
    "TS = 30 # Time Slot (sec)\n",
    "tp_range = 30 # Every time point look forward range (sec)\n",
    "excessive_latency_value = 0.1 # sec\n",
    "S = {}\n",
    "for i in range(len(setup)):\n",
    "    key = list(setup.keys())[i]\n",
    "    for j in range(len(pcap_csv_files)):\n",
    "        if key in pcap_csv_files[j]:\n",
    "            S[j] = setup[key]\n",
    "print(S)\n",
    "\n",
    "f = open('/home/wmnlab/Code_Test_Space/tcp_label/label.csv', 'w')\n",
    "print(pcap_csv_files)\n",
    "f.write(','.join(['time'] + ['avg_rtt', 'excessive_num', 'excessive latency rate','ret_num']*4 + ['label'+'\\n']))\n",
    "for time_point in [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)]:\n",
    "\n",
    "    write_list = []\n",
    "    for df, ret in zip(pcap_df_list, Retransmission):        \n",
    "        # Calculate avgRTT and excessive RTT\n",
    "        R = [] ## Latency\n",
    "        excessive_latency = []\n",
    "        for i in range(len(df)):\n",
    "            t = df['frame.time'].iloc[i]\n",
    "            if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                rtt = float(df['tcp.analysis.ack_rtt'].iloc[i])\n",
    "                # re = df['tcp.analysis.retransmission'].iloc[i]\n",
    "                R.append(rtt)\n",
    "                if rtt >  excessive_latency_value:\n",
    "                    excessive_latency.append(rtt)\n",
    "                # if re == '1':\n",
    "                #     Ret.append(re)\n",
    "            elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                break\n",
    "\n",
    "        # Calculate Retransmission\n",
    "        RE = [] ## Retransmission\n",
    "        for i in range(len(ret)):\n",
    "            t = ret[i]\n",
    "            if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                RE.append(t)\n",
    "            elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                break\n",
    "\n",
    "        if len(R) == 0:\n",
    "            write_list.append('-')\n",
    "        else:\n",
    "            write_list.append(Average(R))\n",
    "        write_list.append(len(excessive_latency))\n",
    "        write_list.append(len(excessive_latency)/len(R)) ## Excessive latency rate\n",
    "        write_list.append(len(RE))\n",
    "    \n",
    "    write_list = [str(i) for i in write_list]\n",
    "    # Calculate Label\n",
    "    E = [float(write_list[i]) for i in range(2,2+4*len(setup), 4)]\n",
    "    ind = E.index(min(E))\n",
    "    label = S[ind]\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    f.write(','.join([str(time_point)] + write_list + [label+'\\n']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDP label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trace #1\n",
      "Data loading for trace #1 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/label#1.csv\n",
      "Trace #1 Done.\n",
      "Processing trace #2\n",
      "Data loading for trace #2 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/label#2.csv\n",
      "Trace #2 Done.\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "file = [\"dwnlnk_udp_latency.csv\", \"uplnk_udp_latency.csv\", \"dwnlnk_udp_loss_timestamp.csv\",\"uplnk_udp_loss_timestamp.csv\", ]\n",
    "# file = [\"udp_dnlk_latency.csv\", \"udp_uplk_latency.csv\", \"udp_dnlk_loss_timestamp.csv\",\"udp_dnlk_loss_timestamp.csv\", ]\n",
    "\n",
    "All_DL_latency_files = []\n",
    "All_UL_latency_files = []\n",
    "All_DL_loss_files = []\n",
    "All_UL_loss_files = []\n",
    "\n",
    "redo=0\n",
    "start_end = []\n",
    "for a in sorted(os.listdir(dir)):\n",
    "    if a == 'ml_data':\n",
    "        redo=1\n",
    "        continue\n",
    "    d0 = os.path.join(dir, a)\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        d1 = os.path.join(d0, b, 'analysis', file[0])\n",
    "        d2 = os.path.join(d0, b, 'analysis', file[1])\n",
    "        d3 = os.path.join(d0, b, 'analysis', file[2])\n",
    "        d4 = os.path.join(d0, b, 'analysis', file[3])\n",
    "        # print(d1,d2,d3,sep='\\n')\n",
    "        All_DL_latency_files.append(d1)\n",
    "        All_UL_latency_files.append(d2)\n",
    "        All_DL_loss_files.append(d3)\n",
    "        All_UL_loss_files.append(d4)\n",
    "\n",
    "p_num = len(os.listdir(dir))-redo # How many device\n",
    "t_num = len(os.listdir(d0)) # How many trace\n",
    "\n",
    "DL_latency_files = []\n",
    "UL_latency_files = []\n",
    "DL_loss_files = []\n",
    "UL_loss_files = []\n",
    "\n",
    "for i in range(t_num):\n",
    "    A = [All_DL_latency_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    B = [All_UL_latency_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    C = [All_DL_loss_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    D = [All_UL_loss_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    DL_latency_files.append(A)\n",
    "    UL_latency_files.append(B)\n",
    "    DL_loss_files.append(C)\n",
    "    UL_loss_files.append(D)\n",
    "\n",
    "for tt in range(t_num):\n",
    "    print(f\"Processing trace #{tt+1}\")\n",
    "    DL_latency_dfs = []\n",
    "    for csv in DL_latency_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x))\n",
    "        DL_latency_dfs.append(df)\n",
    "\n",
    "    UL_latency_dfs = []\n",
    "    for csv in UL_latency_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x))\n",
    "        UL_latency_dfs.append(df)\n",
    "\n",
    "    DL_loss_dfs = []\n",
    "    for csv in DL_loss_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x))\n",
    "        DL_loss_dfs.append(df)\n",
    "\n",
    "    UL_loss_dfs = []\n",
    "    for csv in UL_loss_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x))\n",
    "        UL_loss_dfs.append(df)\n",
    "\n",
    "    print(f\"Data loading for trace #{tt+1} done.\")\n",
    "    # 對齊前後\n",
    "    # loop interate 調整\n",
    "    front_cut = 30\n",
    "    back_cut = 30\n",
    "\n",
    "    for i in range(len(DL_latency_dfs)):\n",
    "        if i == 0:\n",
    "            start = DL_latency_dfs[0][\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "            end = DL_latency_dfs[0][\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "        else:\n",
    "            if DL_latency_dfs[i][\"Timestamp\"].iloc[0] > start:\n",
    "                start = DL_latency_dfs[i][\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "            if DL_latency_dfs[i][\"Timestamp\"].iloc[-1] < end:\n",
    "                end = DL_latency_dfs[i][\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "                \n",
    "    start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "    start_end.append((start, end))\n",
    "    # print(start, end)\n",
    "\n",
    "    # DL \n",
    "    # 實驗設定\n",
    "    n = 4\n",
    "    setup = {'sm05': 'All', 'sm06': 'B1', 'sm07': 'B3', 'sm08': 'B28'}\n",
    "\n",
    "    TS = 30 # Time Slot (sec)\n",
    "    tp_range = 30 # Every time point look forward range (sec)\n",
    "    N = int((end - start).total_seconds()) # How many time_point\n",
    "    excessive_latency_value = 0.1 # sec\n",
    "    \n",
    "    Lambda = 1\n",
    "    \n",
    "    ###########################################################\n",
    "    try:\n",
    "        os.listdir(os.path.join(dir,'ml_data'))\n",
    "    except:\n",
    "        os.system(f\"mkdir {os.path.join(dir,'ml_data')}\")\n",
    "    f = open(os.path.join(dir,'ml_data',f'label#{tt+1}.csv'), 'w') ## Out file !!!!!!!!\n",
    "    print(os.path.join(dir,'ml_data',f'label#{tt+1}.csv'))\n",
    "    ###########################################################\n",
    "    f.write(','.join(['time'] + ['avg_latency', 'excessive_num', 'excessive latency rate','loss_num', 'loss_rate']*n + ['label'+'\\n']))\n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "        write_list = []\n",
    "        for lat_df, loss_df in zip(DL_latency_dfs, DL_loss_dfs):        \n",
    "            # Calculate avgLatency and excessive Latency\n",
    "            R = [] ## Latency\n",
    "            excessive_latency = []\n",
    "            for j in range(len(lat_df)):\n",
    "                t = lat_df['Timestamp'].iloc[j]\n",
    "                if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                    lat = float(lat_df['latency'].iloc[j])\n",
    "                    R.append(lat)\n",
    "                    if lat >  excessive_latency_value:\n",
    "                        excessive_latency.append(lat)\n",
    "                elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                    break\n",
    "\n",
    "            # Loss\n",
    "            LOSS = [] ## Packet Loss\n",
    "            for j in range(len(loss_df)):\n",
    "                t = loss_df[\"Timestamp\"].iloc[j]\n",
    "                if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                    LOSS.append(t)        \n",
    "                elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                    break\n",
    "            \n",
    "            if len(R) == 0:\n",
    "                for j in range(5):\n",
    "                    write_list.append('-')\n",
    "            else:\n",
    "                write_list.append(Average(R))\n",
    "                write_list.append(len(excessive_latency))\n",
    "                write_list.append(len(excessive_latency)/len(R)) ## Excessive latency rate\n",
    "                write_list.append(len(LOSS))\n",
    "                write_list.append(len(LOSS)/(len(LOSS)+len(R))) ## Loss rate = (loss_pzcket#)/(arrive_packet# + loss_packet#)\n",
    "\n",
    "        write_list = [str(j) for j in write_list]\n",
    "        # Calculate Label\n",
    "        E = [float(write_list[j])+Lambda*float(write_list[j+2]) for j in range(2,2+4*n, 5)] # Excessive latency rate + lambda * loss_rate\n",
    "        ind = E.index(min(E))\n",
    "        for j, key in enumerate(setup):\n",
    "            if j == ind:\n",
    "                label = setup[key]\n",
    "        ############################################################################\n",
    "        if time_point == [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)][-1]:\n",
    "            f.write(','.join([str(time_point)] + write_list + [label]))\n",
    "        else:\n",
    "            f.write(','.join([str(time_point)] + write_list + [label+'\\n']))\n",
    "    f.close()\n",
    "    print(f\"Trace #{tt+1} Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input create from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trace #1\n",
      "Data loading for trace #1 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#1.csv\n",
      "Trace 1 done.\n",
      "Processing trace #2\n",
      "Data loading for trace #2 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#2.csv\n",
      "Trace 2 done.\n"
     ]
    }
   ],
   "source": [
    "def ss_append(d, key):\n",
    "        ss_related.append(Average(d[key][0])) ## Avg RSRP of PCell\n",
    "        ss_related.append(Average(d[key][1])) ## Avg RSRQ of PCell\n",
    "        ss_related.append(d[key][0][0]) ## RSRP of first measure\n",
    "        ss_related.append(d[key][0][-1]) ## RSRP of of last measure\n",
    "        ss_related.append(d[key][1][0]) ## RSRQ of first measure\n",
    "        ss_related.append(d[key][1][-1]) ## RSRQ of last measure\n",
    "\n",
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_ml1_files = []\n",
    "CI_files = []\n",
    "\n",
    "redo=0\n",
    "CI_down = False\n",
    "for a in sorted(os.listdir(dir)):\n",
    "    if a == 'ml_data':\n",
    "        redo=1\n",
    "        continue\n",
    "    d0 = os.path.join(dir, a)\n",
    "    # CI_down = False\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        d00 = os.path.join(d0, b, 'data')\n",
    "        for c in sorted(os.listdir(d00)): \n",
    "            if c.endswith('txt_ml1_new.csv'):\n",
    "                d5 = os.path.join(d00, c)\n",
    "            if c.startswith('cimon') and c.endswith('new.csv') and CI_down == False:\n",
    "                CI_files.append(os.path.join(d00,c))\n",
    "                CI_down = True\n",
    "        All_ml1_files.append(d5)\n",
    "\n",
    "p_num = len(os.listdir(dir))-redo # How many device\n",
    "t_num = len(os.listdir(d0)) # How many trace\n",
    "\n",
    "ml1_files = []\n",
    "\n",
    "for i in range(t_num):\n",
    "    A = [All_ml1_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    ml1_files.append(A)\n",
    "\n",
    "CI_dfs = []\n",
    "for i in range(1):\n",
    "    df = pd.read_csv(CI_files[i])\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x))\n",
    "    CI_dfs.append(df)\n",
    "\n",
    "for tt in range(t_num):\n",
    "    print(f\"Processing trace #{tt+1}\")\n",
    "    \n",
    "    ml1_df_list = []\n",
    "    for file in ml1_files[tt]:\n",
    "        mi_ml1_df = pd.read_csv(file)\n",
    "        mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "        mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x)+dt.timedelta(hours=8))\n",
    "        ml1_df_list.append(mi_ml1_df)\n",
    "    \n",
    "    print(f\"Data loading for trace #{tt+1} done.\")\n",
    "    \n",
    "    start, end = start_end[tt]\n",
    "    TS = 30 # Time Slot (sec)\n",
    "    tp_range = 30 # Every time point look back range (sec)\n",
    "    num_cell = 3\n",
    "    #####################################################\n",
    "    try:\n",
    "        os.listdir(os.path.join(dir,'ml_data'))\n",
    "    except:\n",
    "        os.system(f\"mkdir {os.path.join(dir,'ml_data')}\")\n",
    "    f = open(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'), 'w') ## Out file !!!!!!!!\n",
    "    print(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'))\n",
    "    #####################################################\n",
    "    n = int((end - start).total_seconds())\n",
    "    # HO = collect_ho_event(mi_rrc_df)\n",
    "    # A = list(HO.keys())\n",
    "    signal_strength = [\"avg_rsrp\", \"avg_rsrq\", \"rsrp_1st\", \"rsrp_last\", \"rsrq_1st\", \"rsrq_last\"]\n",
    "    gps = ['Latitude', 'Longtitude', 'GPSSpeed1','GPSSpeed2','GPSSpeed3','GPSSpeed4']\n",
    "    B = [\"B1 num\"] + signal_strength * num_cell + [\"B3 num\"] + signal_strength * num_cell + [\"B28 num\"] + signal_strength * num_cell\n",
    "    B[-1] += '\\n'\n",
    "    f.write(','.join(['time']+gps + B))\n",
    "\n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)]:\n",
    "        ######################################################################\n",
    "        # ho_time = list(np.zeros(len(A)))\n",
    "        # for i in range(len(A)):\n",
    "        #     for t in HO[A[i]]:\n",
    "        #         if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "        #             ho_time[i] += 1\n",
    "        # ho_time = [str(i) for i in ho_time]\n",
    "        #######################################################################\n",
    "        # GPS\n",
    "        CI_times = []\n",
    "        gps_lat = []\n",
    "        gps_lon = []\n",
    "        gps_speed = []\n",
    "\n",
    "        ci_df = CI_dfs[0]\n",
    "        for i in range(len(ci_df)):\n",
    "            t = ci_df[\"Date\"].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                CI_times.append(t)\n",
    "                gps_lat.append(ci_df[\"GPSLat\"].iloc[i])\n",
    "                gps_lon.append(ci_df[\"GPSLon\"].iloc[i])\n",
    "                gps_speed.append(ci_df[\"GPSSpeed\"].iloc[i])\n",
    "            elif t > time_point:\n",
    "                break\n",
    "\n",
    "        x = abs((CI_times[0] - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds()) \n",
    "        latitude = gps_lat[0]\n",
    "        lontitude = gps_lon[0]\n",
    "\n",
    "        for i, t in enumerate(CI_times):\n",
    "            x_ = abs((t - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds())\n",
    "            if x_ < x:\n",
    "                x = x_\n",
    "                latitude = gps_lat[i]\n",
    "                lontitude = gps_lon[i]\n",
    "        \n",
    "        \n",
    "        ind = list(np.linspace(0,len(gps_speed)-1,4))\n",
    "        ind = [round(i) for i in ind]\n",
    "        speed_related = [gps_speed[i] for i in ind]\n",
    "\n",
    "        GPS_related = [latitude, lontitude] + speed_related\n",
    "        GPS_related = [str(i) for i in GPS_related]\n",
    "        #######################################################################\n",
    "        # Signal Strength\n",
    "        d = ss_dict()\n",
    "        for df in ml1_df_list:\n",
    "            for i in range(len(df)):\n",
    "                t = df[\"time\"].iloc[i]\n",
    "                if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                    d += ss_dict(df.iloc[i])\n",
    "                elif t > time_point:\n",
    "                    break\n",
    "        d.sort_dict_by_time()\n",
    "        d = d.dict\n",
    "\n",
    "        ss_related = []\n",
    "        # ss_append(d, \"PCell\")\n",
    "        b1 = pop_dict('275',d)\n",
    "        b3 = pop_dict('1275',d)\n",
    "        b28 = pop_dict('9560',d)\n",
    "\n",
    "        for Band in [b1,b3,b28]:\n",
    "            ss_related.append(len(Band)) ## Num of detected serv + nei cell\n",
    "            N = num_cell\n",
    "            while N > 0:\n",
    "                if len(Band) == 0:\n",
    "                    for i in range(6):\n",
    "                        ss_related.append('-')\n",
    "                    N -= 1\n",
    "                else:\n",
    "                    a = max(Band, key=lambda x:Average(Band[x][0]))\n",
    "                    ss_append(Band, a)\n",
    "                    Band.pop(a)\n",
    "                    N -= 1\n",
    "\n",
    "        ss_related = [str(i) for i in ss_related]\n",
    "        if time_point != [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)][-1]:\n",
    "            ss_related[-1] += '\\n'\n",
    "        #######################################################################\n",
    "        \n",
    "        f.write(','.join([str(time_point)] + GPS_related + ss_related))\n",
    "    f.close()\n",
    "    print(f'Trace {tt+1} done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label input of all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace 1 done.\n",
      "Trace 2 done.\n",
      "Original unlabeled file deleted.\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_input_files = []\n",
    "\n",
    "input_files = []\n",
    "label_files = []\n",
    "if 'ml_data' in os.listdir(dir):\n",
    "    d0 = os.path.join(dir, 'ml_data')\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        if 'input' in b and not 'label' in b and not '.csv#' in b:\n",
    "            input_files.append(os.path.join(d0, b))\n",
    "        elif 'label' in b and not 'input' in b:\n",
    "            label_files.append(os.path.join(d0, b))\n",
    "else:\n",
    "    print(\"Error, no dir ml_data\")\n",
    "\n",
    "input_files.sort()\n",
    "label_files.sort()\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    input_file = input_files[i]\n",
    "    f1 = open(input_file, 'r')\n",
    "\n",
    "    label_file = label_files[i]\n",
    "    f2 = open(label_file, 'r')\n",
    "    ##############################################\n",
    "    labeld_input = input_files[i][:-4]+'_labeled.csv'\n",
    "    f3 = open(labeld_input, 'w')\n",
    "    ##############################################\n",
    "\n",
    "    A = f1.readlines()\n",
    "    B = f2.readlines()\n",
    "\n",
    "    for j, (a,b) in enumerate(zip(A,B)):\n",
    "        if j != len(A):\n",
    "            f3.write(a[:-1] + ','+b.split(',')[-1])\n",
    "        else:   \n",
    "            f3.write(a[:] + ','+b.split(','))\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    print(f'Trace {i+1} done.')\n",
    "\n",
    "for f in input_files:\n",
    "    os.system(f'rm {f}')\n",
    "print('Original unlabeled file deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
