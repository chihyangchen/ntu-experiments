{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import swifter\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(L):\n",
    "    return sum(L)/len(L)\n",
    "\n",
    "def mi_event_parsing(miinfofile):\n",
    "    def nr_pci_track():\n",
    "        if miinfofile.loc[i, \"PCI\"] == 65535: ## 65535 is for samgsung phone.\n",
    "            nr_pci = '-'\n",
    "        else:\n",
    "            nr_pci = miinfofile.loc[i, \"PCI\"]\n",
    "        return nr_pci\n",
    "\n",
    "    nr_pci = None ## Initial Unknown\n",
    "     \n",
    "    lte_4G_handover_list = []   #4G 狀態下LTE eNB 的 handover\n",
    "    \n",
    "    nr_setup_list = []          #gNB cell addition\n",
    "    nr_handover_list = []       #gNB cell changes (eNB stays the same)\n",
    "    nr_removal_list = []        #gNB cell removal\n",
    "        \n",
    "    lte_5G_handover_list = []   #(eNB1, gNB1) -> (eNB2, gNB1) #gNB stays the same\n",
    "    nr_lte_handover_list = []   #both NR cell and LTE cell have handover\n",
    "    \n",
    "    eNB_to_MN_list = []\n",
    "    MN_to_eNB_list = []\n",
    "    \n",
    "    scg_failure_list = []       #gNB handover failure\n",
    "    reestablish_list_type2 = [] #eNB handover failure\n",
    "    reestablish_list_type3 = []\n",
    "    \n",
    "    nr_handover = 0\n",
    "    nr_handover_start_index = None\n",
    "    lte_handover = 0\n",
    "    lte_handover_start_index = None\n",
    "    nr_release = 0\n",
    "    nr_release_start_index = None\n",
    "    \n",
    "    lte_failure = 0\n",
    "    lte_failure_start_index = None\n",
    "    \n",
    "    handover_num = 0\n",
    "    \n",
    "    for i in range(len(miinfofile)):\n",
    "        if miinfofile.loc[i, \"type_id\"] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            nr_pci = nr_pci_track()\n",
    "            continue\n",
    "            \n",
    "        if miinfofile.loc[i, \"nr-rrc.t304\"]:\n",
    "            if nr_handover == 0:    \n",
    "                nr_handover = 1\n",
    "                nr_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"lte-rrc.t304\"]:\n",
    "            if lte_handover == 0:\n",
    "                lte_handover = 1\n",
    "                lte_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"nr-Config-r15: release (0)\"]:\n",
    "            if nr_release == 0:\n",
    "                nr_release = 1\n",
    "                nr_release_start_index = i\n",
    "           \n",
    "        if (nr_handover or lte_handover or nr_release) and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            handover_num +=1\n",
    "        \n",
    "        \n",
    "        #handover 種類分類\n",
    "        #------------------------------------------------------------------------------\n",
    "        if lte_handover and not nr_handover and not nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just lte cell handover event\n",
    "            lte_handover = 0\n",
    "            lte_4G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "\n",
    "        if lte_handover and not nr_handover and nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:    # LTE Ho and nr release \n",
    "            lte_handover = 0\n",
    "            nr_release = 0\n",
    "            MN_to_eNB_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "        \n",
    "        if nr_handover and not lte_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just nr cell handover event\n",
    "            nr_handover = 0\n",
    "            if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                nr_setup_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "            else:\n",
    "                nr_handover_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            #additional judgement:\n",
    "            #----------------------------\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"] and nr_pci != None:\n",
    "            #    print(\"Warning: dualConnectivityPHR setup may not mean nr cell addition\", mi_file, i)\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]==0 and not (nr_pci != None and nr_pci != miinfofile.loc[nr_handover_start_index, \"nr_pci\"]): \n",
    "            #    print(\"Warning: nr-rrc.t304 without dualConnectivityPHR setup may not mean nr cell handover\", mi_file, i, nr_handover_start_index, miinfofile.loc[nr_handover_start_index, \"nr_pci\"], nr_pci)\n",
    "                \n",
    "        if lte_handover and nr_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:      # both nr cell and lte cell handover event\n",
    "            lte_handover = 0\n",
    "            nr_handover = 0\n",
    "            if nr_pci == miinfofile.loc[lte_handover_start_index, \"nr_physCellId\"]: \n",
    "                lte_5G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            else:\n",
    "                ##############\n",
    "                if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                    eNB_to_MN_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "                else:\n",
    "                    nr_lte_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if not lte_handover and  nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            nr_release=0\n",
    "            nr_removal_list.append([miinfofile.loc[nr_release_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if miinfofile.loc[i, \"scgFailureInformationNR-r15\"]:\n",
    "            scg_failure_list.append([miinfofile.loc[i, \"time\"], miinfofile.loc[i, \"time\"]]) \n",
    "            \n",
    "        if miinfofile.loc[i, \"rrcConnectionReestablishmentRequest\"]:\n",
    "            if lte_failure == 0:\n",
    "                lte_failure = 1\n",
    "                lte_failure_start_index = i\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishment\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type2.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishmentReject\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type3.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "            \n",
    "    return [lte_4G_handover_list, nr_setup_list, nr_handover_list, nr_removal_list, lte_5G_handover_list, nr_lte_handover_list, eNB_to_MN_list, MN_to_eNB_list, scg_failure_list, reestablish_list_type2, reestablish_list_type3], handover_num\n",
    "\n",
    "def collect_ho_event(mi_rrc_df):\n",
    "        l, _ = mi_event_parsing(mi_rrc_df)\n",
    "        for i in range(0, 11):\n",
    "            l[i] = [j[0] for j in l[i]]\n",
    "        d = {'lte': l[0], 'nr_setup': l[1], 'gNB_ho': l[2], 'nr_rel': l[3], \"MN_changed\": l[4],\"MN_SN_changed\": l[5], \"eNB_to_MN_changed\": l[6], \"MN_to_eNB_changed\": l[7], \"gNB_fail\": l[8], \"type2_fail\": l[9], \"type3_fail\": l[10]}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mi_ho(df):\n",
    "    def NR_OTA():\n",
    "        if df[\"type_id\"].iloc[i] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def find_1st_after(target, look_after=1):\n",
    "        for j in range(i, len(df)):\n",
    "            t_ = df[\"time\"].iloc[j]\n",
    "            if (t_ - t).total_seconds() > look_after:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "\n",
    "    def find_1st_before(target, look_before=1):\n",
    "        for j in range(i, -1, -1):\n",
    "            t_ = df[\"time\"].iloc[j]\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "\n",
    "    HO = namedtuple('HO','start, end, others', defaults=(None,None))\n",
    "\n",
    "    D = {\n",
    "        'Conn_Rel':[], \n",
    "        'Conn_Req':[], # Setup\n",
    "        'LTE_HO': [], # LTE -> newLTE\n",
    "        'MN_HO': [], # LTE + NR -> newLTE + NR\n",
    "        'eNB_to_ENDC': [], # LTE -> LTE + NR => NR setup\n",
    "        'gNB_Rel': [], # LTE + NR -> LTE\n",
    "        'gNB_HO': [], # LTE + NR -> LTE + newNR\n",
    "        # 'HOF': [], # Didn't defined yet.\n",
    "        'RLF_II': [],\n",
    "        'RLF_III': [],\n",
    "        'SCG_RLF': [],\n",
    "        }\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if NR_OTA():\n",
    "            continue\n",
    "\n",
    "        t = df[\"time\"].iloc[i]\n",
    "        \n",
    "        if df[\"rrcConnectionRelease\"].iloc[i] == 1:\n",
    "            D['Conn_Rel'].append(HO(start=t))\n",
    "\n",
    "        if df[\"rrcConnectionRequest\"].iloc[i] == 1:\n",
    "            a = find_1st_after('rrcConnectionReconfigurationComplete',look_after=2)[0]\n",
    "            b = find_1st_after('securityModeComplete',look_after=2)[0]\n",
    "            end = a if a > b else b\n",
    "            D['Conn_Req'].append(HO(start=t,end=end))\n",
    "        \n",
    "        if df[\"lte-rrc.t304\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            serv_cell, target_cell = df[\"PCI\"].iloc[i], df['lte_targetPhysCellId'].iloc[i]\n",
    "            serv_freq, target_freq = df[\"Freq\"].iloc[i], df['dl-CarrierFreq'].iloc[i]\n",
    "            if df[\"SCellToAddMod-r10\"].iloc[i] == 1:\n",
    "                n =len(str(df[\"SCellIndex-r10.1\"].iloc[i]).split('@'))\n",
    "                others=f'Set up {n} SCell.'\n",
    "            else:\n",
    "                others=None\n",
    "            \n",
    "            if serv_freq != target_freq:\n",
    "                others += \" Inter freq. HO\"\n",
    "            if df[\"nr-rrc.t304\"].iloc[i] == 1 and df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    D['eNB_to_ENDC'].append(HO(start=t, end=end, others=others))\n",
    "                    # print(1, t, f\"Serving Cell: {serv_cell}->{target_cell}\")  \n",
    "                else:    \n",
    "                    D['MN_HO'].append(HO(start=t, end=end, others=others))\n",
    "            else:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    a, b = find_1st_before(\"scgFailureInformationNR-r15\")\n",
    "                    if a is not None:\n",
    "                        others += \" Caused by scg-failure.\"\n",
    "                    D['gNB_Rel'].append(HO(start=t, end=end, others=others))\n",
    "                else:\n",
    "                    D['LTE_HO'].append(HO(start=t, end=end, others=others))\n",
    "\n",
    "        if df[\"nr-rrc.t304\"].iloc[i] == 1 and not df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            D['gNB_HO'].append(HO(start=t,end=end))\n",
    "\n",
    "        if df[\"rrcConnectionReestablishmentRequest\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReestablishmentComplete', look_after=1)\n",
    "            b, _ = find_1st_after('rrcConnectionReestablishmentReject', look_after=1)\n",
    "            others = df[\"reestablishmentCause\"].iloc[i]\n",
    "            if end is not None: \n",
    "                # Type II\n",
    "                D['RLF_II'].append(HO(start=t,end=end,others=others))\n",
    "            else: \n",
    "                # Type III\n",
    "                D['RLF_III'].append(HO(start=t,end=b,others=others)) # End for Type III?\n",
    "            \n",
    "        if df[\"scgFailureInformationNR-r15\"].iloc[i] == 1:\n",
    "            others = df[\"failureType-r15\"].iloc[i]\n",
    "            D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_dict(band, d):\n",
    "    D = d.copy()\n",
    "    for key in list(d.keys()):\n",
    "        if not key.endswith(' '+band):\n",
    "            D.pop(key)\n",
    "    return D\n",
    "\n",
    "class ss_dict:\n",
    "    def __init__(self,pd_data=None,d=None): ## Input pd_df.iloc[index]\n",
    "        self.dict = {'PCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    def serv_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        serv_cell_id = pd_data[\"Serving Cell Index\"]\n",
    "        pci = pd_data[\"PCI\"]\n",
    "        rsrp = float(pd_data[\"RSRP(dBm)\"])\n",
    "        rsrq = float(pd_data[\"RSRQ(dB)\"])\n",
    "        t = pd_data[\"time\"]\n",
    "        if serv_cell_id == \"PCell\":\n",
    "            self.dict['PCell'][0].append(rsrp)\n",
    "            self.dict['PCell'][1].append(rsrq)\n",
    "            self.dict['PCell'][2].append(t)\n",
    "            # self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "        else:\n",
    "            self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "            # s = pci + ' ' + self.earfcn\n",
    "            # if s in \n",
    "    def nei_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        t = pd_data[\"time\"]\n",
    "        for i in range(9, len(pd_data), 3):\n",
    "            if pd_data[i] == '-':\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[pd_data[i]+' '+earfcn] = [[rsrp], [rsrq], [t]]              \n",
    "    \n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] = d1[key][0] + d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return ss_dict(d=d1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def sort_dict_by_time(self):\n",
    "        def sort_element(element):\n",
    "            d1 = [ [element[0][i], element[1][i], element[2][i]] for i in range(len(element[0]))]\n",
    "            d1.sort(key=lambda data:data[2])\n",
    "            RSRP = [i[0] for i in d1]\n",
    "            RSRQ = [i[1] for i in d1]\n",
    "            T = [i[2] for i in d1]\n",
    "            return [RSRP, RSRQ, T]\n",
    "        sorted_D = {}\n",
    "        for k in list(self.dict.keys()):\n",
    "            sorted_D[k] = sort_element(self.dict[k])\n",
    "        self.dict = sorted_D\n",
    "\n",
    "\n",
    "class nr_ss_dict:\n",
    "    def __init__(self, pd_data=None, d=None):\n",
    "        self.dict = {'PSCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    \n",
    "    def serv_cell(self, pd_data):\n",
    "        self.pscell = pd_data[\"Serving Cell PCI\"]\n",
    "        do = False\n",
    "        for cell in self.dict.keys():\n",
    "            if self.pscell == cell:\n",
    "                self.dict[\"PSCell\"][0] += self.dict[cell][0]\n",
    "                self.dict[\"PSCell\"][1] += self.dict[cell][1]\n",
    "                self.dict[\"PSCell\"][2] += self.dict[cell][2]\n",
    "                do,x = True, cell\n",
    "                break\n",
    "        if do:\n",
    "            self.dict.pop(x)\n",
    "            \n",
    "    def nei_cell(self, pd_data):\n",
    "        arfcn = pd_data[\"Raster ARFCN\"]\n",
    "        t = pd_data[\"time\"]\n",
    "        for i in range(6, len(pd_data), 3):\n",
    "            if pd_data[i] == '-':\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[pd_data[i]] = [[rsrp], [rsrq], [t]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] += d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return nr_ss_dict(d=d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCP label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tcp file\n",
    "file_dir = '/home/wmnlab/Code_Test_Space/tcp_label'\n",
    "pcap_csv_files = os.listdir(file_dir)\n",
    "pcap_csv_files = [i for i in pcap_csv_files if 'label' not in i]\n",
    "pcap_csv_files.sort()\n",
    "pcap_csv_files.insert(0, pcap_csv_files.pop(-1))\n",
    "pcap_csv_files = [os.path.join(file_dir, i) for i in pcap_csv_files]\n",
    "\n",
    "print(pcap_csv_files)\n",
    "pcap_df_list = []\n",
    "Retransmission = []\n",
    "\n",
    "for file in pcap_csv_files:\n",
    "    tcp_df = pd.read_csv(file, sep='@')\n",
    "    tcp_df[\"frame.time\"] = tcp_df[\"frame.time\"].apply(lambda x: pd.to_datetime(x))\n",
    "    tcp_df[\"frame.time\"] = tcp_df[\"frame.time\"].apply(lambda x: x.tz_convert(None)) + dt.timedelta(hours=8)\n",
    "    A = tcp_df.drop(tcp_df.index[tcp_df['tcp.analysis.ack_rtt'] == '-'])\n",
    "    A = A.drop(A.index[A['ip.dst'] != '192.168.1.248'])\n",
    "    pcap_df_list.append(A)\n",
    "    B = [tcp_df['frame.time'].iloc[i] for i in range(len(tcp_df)) if tcp_df['tcp.analysis.retransmission'].iloc[i] == '1' or tcp_df['tcp.analysis.fast_retransmission'].iloc[i] == '1']\n",
    "    Retransmission.append(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對齊前後\n",
    "front_cut = 0\n",
    "back_cut = 30\n",
    "\n",
    "for i in range(len(pcap_df_list)):\n",
    "    if i == 0:\n",
    "        start = pcap_df_list[0][\"frame.time\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "        end = pcap_df_list[0][\"frame.time\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "    else:\n",
    "        if pcap_df_list[i][\"frame.time\"].iloc[0] > start:\n",
    "            start = pcap_df_list[i][\"frame.time\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "        if pcap_df_list[i][\"frame.time\"].iloc[-1] < end:\n",
    "            end = pcap_df_list[i][\"frame.time\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "            \n",
    "start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "n = int((end - start).total_seconds())\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DL 數據選擇\n",
    "setup = {'sm05': 'All', 'sm06': 'B1+B3', 'sm07': 'B3+B28', 'sm08': 'B1+B28'}\n",
    "\n",
    "TS = 30 # Time Slot (sec)\n",
    "tp_range = 30 # Every time point look forward range (sec)\n",
    "excessive_latency_value = 0.1 # sec\n",
    "S = {}\n",
    "for i in range(len(setup)):\n",
    "    key = list(setup.keys())[i]\n",
    "    for j in range(len(pcap_csv_files)):\n",
    "        if key in pcap_csv_files[j]:\n",
    "            S[j] = setup[key]\n",
    "print(S)\n",
    "\n",
    "f = open('/home/wmnlab/Code_Test_Space/tcp_label/label.csv', 'w')\n",
    "print(pcap_csv_files)\n",
    "f.write(','.join(['time'] + ['avg_rtt', 'excessive_num', 'excessive latency rate','ret_num']*4 + ['label'+'\\n']))\n",
    "for time_point in [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)]:\n",
    "\n",
    "    write_list = []\n",
    "    for df, ret in zip(pcap_df_list, Retransmission):        \n",
    "        # Calculate avgRTT and excessive RTT\n",
    "        R = [] ## Latency\n",
    "        excessive_latency = []\n",
    "        for i in range(len(df)):\n",
    "            t = df['frame.time'].iloc[i]\n",
    "            if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                rtt = float(df['tcp.analysis.ack_rtt'].iloc[i])\n",
    "                # re = df['tcp.analysis.retransmission'].iloc[i]\n",
    "                R.append(rtt)\n",
    "                if rtt >  excessive_latency_value:\n",
    "                    excessive_latency.append(rtt)\n",
    "                # if re == '1':\n",
    "                #     Ret.append(re)\n",
    "            elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                break\n",
    "\n",
    "        # Calculate Retransmission\n",
    "        RE = [] ## Retransmission\n",
    "        for i in range(len(ret)):\n",
    "            t = ret[i]\n",
    "            if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                RE.append(t)\n",
    "            elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                break\n",
    "\n",
    "        if len(R) == 0:\n",
    "            write_list.append('-')\n",
    "        else:\n",
    "            write_list.append(Average(R))\n",
    "        write_list.append(len(excessive_latency))\n",
    "        write_list.append(len(excessive_latency)/len(R)) ## Excessive latency rate\n",
    "        write_list.append(len(RE))\n",
    "    \n",
    "    write_list = [str(i) for i in write_list]\n",
    "    # Calculate Label\n",
    "    E = [float(write_list[i]) for i in range(2,2+4*len(setup), 4)]\n",
    "    ind = E.index(min(E))\n",
    "    label = S[ind]\n",
    "\n",
    "    ############################################################################\n",
    "\n",
    "    f.write(','.join([str(time_point)] + write_list + [label+'\\n']))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDP label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trace #1\n",
      "Data loading for trace #1 done.\n",
      "From 2022-12-22 15:05:08 to 2022-12-22 15:04:16.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/ml_data/label#1.csv\n",
      "Trace #1 Done.\n",
      "Processing trace #2\n",
      "Data loading for trace #2 done.\n",
      "From 2022-12-22 15:13:19 to 2022-12-22 15:18:34.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/ml_data/label#2.csv\n",
      "Trace #2 Done.\n",
      "Processing trace #3\n",
      "Data loading for trace #3 done.\n",
      "From 2022-12-22 15:20:27 to 2022-12-22 15:25:12.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/ml_data/label#3.csv\n",
      "Trace #3 Done.\n",
      "Processing trace #4\n",
      "Data loading for trace #4 done.\n",
      "From 2022-12-22 15:27:33 to 2022-12-22 15:33:31.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/ml_data/label#4.csv\n",
      "Trace #4 Done.\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3\"\n",
    "# file = [\"dwnlnk_udp_latency.csv\", \"uplnk_udp_latency.csv\", \"dwnlnk_udp_loss_timestamp.csv\",\"uplnk_udp_loss_timestamp.csv\", ]\n",
    "file = [\"udp_dnlk_latency.csv\", \"udp_uplk_latency.csv\", \"udp_dnlk_loss_timestamp.csv\",\"udp_dnlk_loss_timestamp.csv\", ]\n",
    "\n",
    "All_DL_latency_files = []\n",
    "All_UL_latency_files = []\n",
    "All_DL_loss_files = []\n",
    "All_UL_loss_files = []\n",
    "\n",
    "redo=0\n",
    "start_end = []\n",
    "for a in sorted(os.listdir(dir)):\n",
    "    if a == 'ml_data':\n",
    "        redo=1\n",
    "        continue\n",
    "    d0 = os.path.join(dir, a)\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        d1 = os.path.join(d0, b, 'data', file[0]) ## analysis -> data/'\n",
    "        d2 = os.path.join(d0, b, 'data', file[1])\n",
    "        d3 = os.path.join(d0, b, 'data', file[2])\n",
    "        d4 = os.path.join(d0, b, 'data', file[3])\n",
    "        # print(d1,d2,d3,sep='\\n')\n",
    "        All_DL_latency_files.append(d1)\n",
    "        All_UL_latency_files.append(d2)\n",
    "        All_DL_loss_files.append(d3)\n",
    "        All_UL_loss_files.append(d4)\n",
    "\n",
    "p_num = len(os.listdir(dir))-redo # How many device\n",
    "t_num = len(os.listdir(d0)) # How many trace\n",
    "\n",
    "DL_latency_files = []\n",
    "UL_latency_files = []\n",
    "DL_loss_files = []\n",
    "UL_loss_files = []\n",
    "\n",
    "for i in range(t_num):\n",
    "    A = [All_DL_latency_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    B = [All_UL_latency_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    C = [All_DL_loss_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    D = [All_UL_loss_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    DL_latency_files.append(A)\n",
    "    UL_latency_files.append(B)\n",
    "    DL_loss_files.append(C)\n",
    "    UL_loss_files.append(D)\n",
    "\n",
    "for tt in range(t_num):\n",
    "    print(f\"Processing trace #{tt+1}\")\n",
    "    DL_latency_dfs = []\n",
    "    for csv in DL_latency_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "        DL_latency_dfs.append(df)\n",
    "\n",
    "    UL_latency_dfs = []\n",
    "    for csv in UL_latency_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "        UL_latency_dfs.append(df)\n",
    "\n",
    "    DL_loss_dfs = []\n",
    "    for csv in DL_loss_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "        DL_loss_dfs.append(df)\n",
    "\n",
    "    UL_loss_dfs = []\n",
    "    for csv in UL_loss_files[tt]:\n",
    "        df = pd.read_csv(csv)\n",
    "        df[\"Timestamp\"] = df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "        UL_loss_dfs.append(df)\n",
    "\n",
    "    print(f\"Data loading for trace #{tt+1} done.\")\n",
    "    # 對齊前後\n",
    "    # loop interate 調整\n",
    "    front_cut = 30\n",
    "    back_cut = 30\n",
    "\n",
    "    for i in range(len(DL_latency_dfs)):\n",
    "        if i == 0:\n",
    "            start = DL_latency_dfs[0][\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "            end = DL_latency_dfs[0][\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "        else:\n",
    "            if DL_latency_dfs[i][\"Timestamp\"].iloc[0] > start:\n",
    "                start = DL_latency_dfs[i][\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "            if DL_latency_dfs[i][\"Timestamp\"].iloc[-1] < end:\n",
    "                end = DL_latency_dfs[i][\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "                \n",
    "    start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "    start_end.append((start, end))\n",
    "    print(f'From {start} to {end}.')\n",
    "\n",
    "    # DL \n",
    "    # 實驗設定\n",
    "    n = 4\n",
    "    setup = {'sm05': 'All', 'sm06': 'B1', 'sm07': 'B3', 'sm08': 'B1+B3'}\n",
    "\n",
    "    TS = 30 # Time Slot (sec)\n",
    "    tp_range = 30 # Every time point look forward range (sec)\n",
    "    N = int((end - start).total_seconds()) # How many time_point\n",
    "    excessive_latency_value = 0.1 # sec\n",
    "    \n",
    "    Lambda = 1\n",
    "    \n",
    "    ###########################################################\n",
    "    try:\n",
    "        os.listdir(os.path.join(dir,'ml_data'))\n",
    "    except:\n",
    "        os.system(f\"mkdir {os.path.join(dir,'ml_data')}\")\n",
    "    f = open(os.path.join(dir,'ml_data',f'label#{tt+1}.csv'), 'w') ## Out file !!!!!!!!\n",
    "    print(os.path.join(dir,'ml_data',f'label#{tt+1}.csv'))\n",
    "    ###########################################################\n",
    "    f.write(','.join(['time'] + ['avg_latency', 'excessive_num', 'excessive latency rate','loss_num', 'loss_rate']*n + ['label'+'\\n']))\n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "        write_list = []\n",
    "        for lat_df, loss_df in zip(DL_latency_dfs, DL_loss_dfs):        \n",
    "            # Calculate avgLatency and excessive Latency\n",
    "            R = [] ## Latency\n",
    "            excessive_latency = []\n",
    "            for j in range(len(lat_df)):\n",
    "                t = lat_df['Timestamp'].iloc[j]\n",
    "                if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                    lat = float(lat_df['latency'].iloc[j])\n",
    "                    R.append(lat)\n",
    "                    if lat >  excessive_latency_value:\n",
    "                        excessive_latency.append(lat)\n",
    "                elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                    break\n",
    "\n",
    "            # Loss\n",
    "            LOSS = [] ## Packet Loss\n",
    "            for j in range(len(loss_df)):\n",
    "                t = loss_df[\"Timestamp\"].iloc[j]\n",
    "                if time_point  < t <= time_point + dt.timedelta(seconds=tp_range):\n",
    "                    LOSS.append(t)        \n",
    "                elif t > time_point + dt.timedelta(seconds=tp_range):\n",
    "                    break\n",
    "            \n",
    "            if len(R) == 0:\n",
    "                for j in range(5):\n",
    "                    write_list.append('-')\n",
    "            else:\n",
    "                write_list.append(Average(R))\n",
    "                write_list.append(len(excessive_latency))\n",
    "                write_list.append(len(excessive_latency)/len(R)) ## Excessive latency rate\n",
    "                write_list.append(len(LOSS))\n",
    "                write_list.append(len(LOSS)/(len(LOSS)+len(R))) ## Loss rate = (loss_packet#)/(arrive_packet# + loss_packet#)\n",
    "\n",
    "        write_list = [str(j) for j in write_list]\n",
    "        # Calculate Label\n",
    "        E = [float(write_list[j])+Lambda*float(write_list[j+2]) for j in range(2,2+4*n, 5)] # Excessive latency rate + lambda * loss_rate\n",
    "        ind = E.index(min(E))\n",
    "        for j, key in enumerate(setup):\n",
    "            if j == ind:\n",
    "                label = setup[key]\n",
    "        ############################################################################\n",
    "        if time_point == [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)][-1]:\n",
    "            f.write(','.join([str(time_point)] + write_list + [label]))\n",
    "        else:\n",
    "            f.write(','.join([str(time_point)] + write_list + [label+'\\n']))\n",
    "    f.close()\n",
    "    print(f\"Trace #{tt+1} Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data for a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2022-12-22 15:04:43 to 2022-12-22 15:10:56.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/sm05/#01\"\n",
    "out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "f = open(out_file, 'w') \n",
    "\n",
    "# Collecting the UDP Latency and Loss information first\n",
    "dir = os.path.join(base_dir,\"data\")\n",
    "\n",
    "dl_lat_file = os.path.join(dir, \"udp_dnlk_latency.csv\")\n",
    "dl_lat_df = pd.read_csv(dl_lat_file)\n",
    "dl_lat_df[\"Timestamp\"] = dl_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "dl_loss_file = os.path.join(dir, \"udp_dnlk_loss_timestamp.csv\")\n",
    "dl_loss_df = pd.read_csv(dl_loss_file)\n",
    "dl_loss_df[\"Timestamp\"] = dl_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_lat_file = os.path.join(dir, \"udp_uplk_latency.csv\")\n",
    "ul_lat_df = pd.read_csv(ul_lat_file)\n",
    "ul_lat_df[\"Timestamp\"] = ul_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_loss_file = os.path.join(dir, \"udp_uplk_loss_timestamp.csv\")\n",
    "ul_loss_df = pd.read_csv(ul_loss_file)\n",
    "ul_loss_df[\"Timestamp\"] = ul_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "# Get timepoint from start to end and get the latency and loss\n",
    "front_cut = 5\n",
    "back_cut = 5\n",
    "TS = 1\n",
    "tp_range = 1\n",
    "\n",
    "start = dl_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "end = dl_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "print(f'From {start} to {end}.')\n",
    "N = int((end - start).total_seconds()) # How many time_point\n",
    "\n",
    "# Collect rsrp infomation\n",
    "matches = filter(lambda x: x.endswith('ml1_new.csv'), os.listdir(dir))\n",
    "ml1_filenames = sorted(list(matches))\n",
    "mi_ml1_file = os.path.join(dir, ml1_filenames[0])\n",
    "mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "nr_mi_ml1_file = os.path.join(dir, ml1_filenames[1])\n",
    "nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "nr_mi_ml1_df[\"time\"] = nr_mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "\n",
    "# Collect gps and gpsspeed from cellinfo\n",
    "dir = os.path.join(base_dir,\"middle\")\n",
    "matches = filter(lambda x: x.startswith('cimon'), os.listdir(dir))\n",
    "ci_filename = list(matches)[0]\n",
    "ci_file = os.path.join(dir, ci_filename)\n",
    "ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "\n",
    "# Collect Ho information\n",
    "matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(dir))\n",
    "mi_rrc_filename = list(matches)[0]\n",
    "mi_rrc_file = os.path.join(dir, mi_rrc_filename)\n",
    "mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "mi_rrc_df[\"time\"] = mi_rrc_df[\"time\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "HO_events = parse_mi_ho(mi_rrc_df)\n",
    "HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "\n",
    "columns = [\n",
    "    \"Timestamp\",\n",
    "    \"lat\", \"long\", \"gpsspedd\",\n",
    "    'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF_II','RLF_III','SCG_RLF',\n",
    "    \"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "    \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    \"DL-lat\", \"DL-lossrate\", \"UL-lat\", \"UL-lossrate\"\n",
    "]\n",
    "f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "i_ = [0,0,0,0,0,0,0] # For increase speed\n",
    "for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "\n",
    "    # ========================================================================\n",
    "    # Get DL/UL latency, loss...\n",
    "    perfermance_related = []\n",
    "\n",
    "    dl_lats = []\n",
    "    for i in range(i_[0], len(dl_lat_df)):\n",
    "        t = dl_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_lat = float(dl_lat_df['latency'].iloc[i])\n",
    "            dl_lats.append(dl_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[0] = i\n",
    "            break\n",
    "\n",
    "    if len(dl_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        # perfermance_related.append('-')\n",
    "        perfermance_related.append(dl_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(dl_avg_lat)\n",
    "\n",
    "    dl_losses = []\n",
    "    for i in range(i_[1], len(dl_loss_df)):\n",
    "        t = dl_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[1] = i\n",
    "            break\n",
    "\n",
    "    if (len(dl_losses)+len(dl_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(dl_losses)/(len(dl_losses)+len(dl_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    ul_lats = []\n",
    "    for i in range(i_[2], len(ul_lat_df)):\n",
    "        t = ul_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_lat = float(ul_lat_df['latency'].iloc[i])\n",
    "            ul_lats.append(ul_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[2] = i\n",
    "            break\n",
    "\n",
    "    if len(ul_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "        perfermance_related.append(ul_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(ul_avg_lat)\n",
    "\n",
    "    ul_losses = []\n",
    "    for i in range(i_[3], len(ul_loss_df)):\n",
    "        t = ul_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[3] = i\n",
    "            break\n",
    "\n",
    "    if (len(ul_losses)+len(ul_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(ul_losses)/(len(ul_losses)+len(ul_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    perfermance_related = [str(feature) for feature in perfermance_related]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Get GPS informations\n",
    "    gps_related = []\n",
    "\n",
    "    for i in range(i_[4], len(ci_df)):\n",
    "        t = ci_df['Date'].iloc[i]\n",
    "        lat = ci_df['GPSLat'].iloc[i]\n",
    "        long = ci_df['GPSLon'].iloc[i]\n",
    "        gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "        elif t > time_point:\n",
    "            i_[4] = i\n",
    "            break\n",
    "    \n",
    "\n",
    "    gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "    gps_related = [str(feature) for feature in gps_related]\n",
    "    # print(f\"{time_point} {gps_info}\")\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Get signal strength informations\n",
    "    ss_related = []\n",
    "\n",
    "    SS_DICT = ss_dict()\n",
    "    for i in range(i_[5], len(mi_ml1_df)):\n",
    "        t = mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "        \n",
    "        if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "            SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "        elif t > time_point:\n",
    "            i_[5] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary serv cell rsrp, rsrq \n",
    "    if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "        pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "        pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "    else:\n",
    "        # pcell_rsrp, pcell_rsrq = '-', '-'\n",
    "        pcell_rsrp, pcell_rsrq = pcell_rsrp, pcell_rsrq # No sample value, use the previous one\n",
    "    SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "        SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "        SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "        # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    NR_SS_DICT = nr_ss_dict()\n",
    "    for i in range(i_[6], len(nr_mi_ml1_df)):\n",
    "        t = nr_mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "        \n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "        elif t > time_point:\n",
    "            i_[6] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary secondary serv cell rsrp, rsrq \n",
    "    if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "        pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "        pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "    else:\n",
    "        # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "        pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "    NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        NR_SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        NR_SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "    \n",
    "    # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    ss_related = [str(feature) for feature in ss_related]\n",
    "\n",
    "    # ================================================================================\n",
    "    # Get HO informations\n",
    "    HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "    for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "        for ho in HO_events[ho_type]:\n",
    "            t = ho.start\n",
    "            if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                HO_related[i] += 1\n",
    "            elif t > time_point:\n",
    "                break\n",
    "    \n",
    "    HO_related = [str(feature) for feature in HO_related]\n",
    "\n",
    "    f.write(\",\".join([str(time_point)]+gps_related+HO_related+ss_related+perfermance_related)+\"\\n\") \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'422': [[-90.109, -92.797, -93.578, -121.141, -103.07], [-15.492, -16.68, -23.008, -28.727, -25.156], [Timestamp('2022-12-22 15:10:55.193070'), Timestamp('2022-12-22 15:10:55.352650'), Timestamp('2022-12-22 15:10:55.672631'), Timestamp('2022-12-22 15:10:55.832622'), Timestamp('2022-12-22 15:10:55.992652')]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NR_SS_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input create from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trace #1\n",
      "Data loading for trace #1 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#1.csv\n",
      "Trace 1 done.\n",
      "Processing trace #2\n",
      "Data loading for trace #2 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#2.csv\n",
      "Trace 2 done.\n"
     ]
    }
   ],
   "source": [
    "def ss_append(d, key):\n",
    "        ss_related.append(Average(d[key][0])) ## Avg RSRP of PCell\n",
    "        ss_related.append(Average(d[key][1])) ## Avg RSRQ of PCell\n",
    "        ss_related.append(d[key][0][0]) ## RSRP of first measure\n",
    "        ss_related.append(d[key][0][-1]) ## RSRP of of last measure\n",
    "        ss_related.append(d[key][1][0]) ## RSRQ of first measure\n",
    "        ss_related.append(d[key][1][-1]) ## RSRQ of last measure\n",
    "\n",
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_ml1_files = []\n",
    "CI_files = []\n",
    "\n",
    "redo=0\n",
    "CI_down = False\n",
    "for a in sorted(os.listdir(dir)):\n",
    "    if a == 'ml_data':\n",
    "        redo=1\n",
    "        continue\n",
    "    d0 = os.path.join(dir, a)\n",
    "    # CI_down = False\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        d00 = os.path.join(d0, b, 'data')\n",
    "        for c in sorted(os.listdir(d00)): \n",
    "            if c.endswith('txt_ml1_new.csv'):\n",
    "                d5 = os.path.join(d00, c)\n",
    "            if c.startswith('cimon') and c.endswith('new.csv') and CI_down == False:\n",
    "                CI_files.append(os.path.join(d00,c))\n",
    "                CI_down = True\n",
    "        All_ml1_files.append(d5)\n",
    "\n",
    "p_num = len(os.listdir(dir))-redo # How many device\n",
    "t_num = len(os.listdir(d0)) # How many trace\n",
    "\n",
    "ml1_files = []\n",
    "\n",
    "for i in range(t_num):\n",
    "    A = [All_ml1_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    ml1_files.append(A)\n",
    "\n",
    "CI_dfs = []\n",
    "for i in range(1):\n",
    "    df = pd.read_csv(CI_files[i])\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x))\n",
    "    CI_dfs.append(df)\n",
    "\n",
    "for tt in range(t_num):\n",
    "    print(f\"Processing trace #{tt+1}\")\n",
    "    \n",
    "    ml1_df_list = []\n",
    "    for file in ml1_files[tt]:\n",
    "        mi_ml1_df = pd.read_csv(file)\n",
    "        mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "        mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x)+dt.timedelta(hours=8))\n",
    "        ml1_df_list.append(mi_ml1_df)\n",
    "    \n",
    "    print(f\"Data loading for trace #{tt+1} done.\")\n",
    "    \n",
    "    start, end = start_end[tt]\n",
    "    TS = 30 # Time Slot (sec)\n",
    "    tp_range = 30 # Every time point look back range (sec)\n",
    "    num_cell = 3\n",
    "    #####################################################\n",
    "    try:\n",
    "        os.listdir(os.path.join(dir,'ml_data'))\n",
    "    except:\n",
    "        os.system(f\"mkdir {os.path.join(dir,'ml_data')}\")\n",
    "    f = open(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'), 'w') ## Out file !!!!!!!!\n",
    "    print(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'))\n",
    "    #####################################################\n",
    "    n = int((end - start).total_seconds())\n",
    "    # HO = collect_ho_event(mi_rrc_df)\n",
    "    # A = list(HO.keys())\n",
    "    signal_strength = [\"avg_rsrp\", \"avg_rsrq\", \"rsrp_1st\", \"rsrp_last\", \"rsrq_1st\", \"rsrq_last\"]\n",
    "    gps = ['Latitude', 'Longtitude', 'GPSSpeed1','GPSSpeed2','GPSSpeed3','GPSSpeed4']\n",
    "    B = [\"B1 num\"] + signal_strength * num_cell + [\"B3 num\"] + signal_strength * num_cell + [\"B28 num\"] + signal_strength * num_cell\n",
    "    B[-1] += '\\n'\n",
    "    f.write(','.join(['time']+gps + B))\n",
    "\n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)]:\n",
    "        ######################################################################\n",
    "        # ho_time = list(np.zeros(len(A)))\n",
    "        # for i in range(len(A)):\n",
    "        #     for t in HO[A[i]]:\n",
    "        #         if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "        #             ho_time[i] += 1\n",
    "        # ho_time = [str(i) for i in ho_time]\n",
    "        #######################################################################\n",
    "        # GPS\n",
    "        CI_times = []\n",
    "        gps_lat = []\n",
    "        gps_lon = []\n",
    "        gps_speed = []\n",
    "\n",
    "        ci_df = CI_dfs[0]\n",
    "        for i in range(len(ci_df)):\n",
    "            t = ci_df[\"Date\"].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                CI_times.append(t)\n",
    "                gps_lat.append(ci_df[\"GPSLat\"].iloc[i])\n",
    "                gps_lon.append(ci_df[\"GPSLon\"].iloc[i])\n",
    "                gps_speed.append(ci_df[\"GPSSpeed\"].iloc[i])\n",
    "            elif t > time_point:\n",
    "                break\n",
    "\n",
    "        x = abs((CI_times[0] - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds()) \n",
    "        latitude = gps_lat[0]\n",
    "        lontitude = gps_lon[0]\n",
    "\n",
    "        for i, t in enumerate(CI_times):\n",
    "            x_ = abs((t - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds())\n",
    "            if x_ < x:\n",
    "                x = x_\n",
    "                latitude = gps_lat[i]\n",
    "                lontitude = gps_lon[i]\n",
    "        \n",
    "        \n",
    "        ind = list(np.linspace(0,len(gps_speed)-1,4))\n",
    "        ind = [round(i) for i in ind]\n",
    "        speed_related = [gps_speed[i] for i in ind]\n",
    "\n",
    "        GPS_related = [latitude, lontitude] + speed_related\n",
    "        GPS_related = [str(i) for i in GPS_related]\n",
    "        #######################################################################\n",
    "        # Signal Strength\n",
    "        d = ss_dict()\n",
    "        for df in ml1_df_list:\n",
    "            for i in range(len(df)):\n",
    "                t = df[\"time\"].iloc[i]\n",
    "                if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                    d += ss_dict(df.iloc[i])\n",
    "                elif t > time_point:\n",
    "                    break\n",
    "        d.sort_dict_by_time()\n",
    "        d = d.dict\n",
    "\n",
    "        ss_related = []\n",
    "        # ss_append(d, \"PCell\")\n",
    "        b1 = pop_dict('275',d)\n",
    "        b3 = pop_dict('1275',d)\n",
    "        b28 = pop_dict('9560',d)\n",
    "\n",
    "        for Band in [b1,b3,b28]:\n",
    "            ss_related.append(len(Band)) ## Num of detected serv + nei cell\n",
    "            N = num_cell\n",
    "            while N > 0:\n",
    "                if len(Band) == 0:\n",
    "                    for i in range(6):\n",
    "                        ss_related.append('-')\n",
    "                    N -= 1\n",
    "                else:\n",
    "                    a = max(Band, key=lambda x:Average(Band[x][0]))\n",
    "                    ss_append(Band, a)\n",
    "                    Band.pop(a)\n",
    "                    N -= 1\n",
    "\n",
    "        ss_related = [str(i) for i in ss_related]\n",
    "        if time_point != [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)][-1]:\n",
    "            ss_related[-1] += '\\n'\n",
    "        #######################################################################\n",
    "        \n",
    "        f.write(','.join([str(time_point)] + GPS_related + ss_related))\n",
    "    f.close()\n",
    "    print(f'Trace {tt+1} done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label input of all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace 1 done.\n",
      "Trace 2 done.\n",
      "Original unlabeled file deleted.\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_input_files = []\n",
    "\n",
    "input_files = []\n",
    "label_files = []\n",
    "if 'ml_data' in os.listdir(dir):\n",
    "    d0 = os.path.join(dir, 'ml_data')\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        if 'input' in b and not 'label' in b and not '.csv#' in b:\n",
    "            input_files.append(os.path.join(d0, b))\n",
    "        elif 'label' in b and not 'input' in b:\n",
    "            label_files.append(os.path.join(d0, b))\n",
    "else:\n",
    "    print(\"Error, no dir ml_data\")\n",
    "\n",
    "input_files.sort()\n",
    "label_files.sort()\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    input_file = input_files[i]\n",
    "    f1 = open(input_file, 'r')\n",
    "\n",
    "    label_file = label_files[i]\n",
    "    f2 = open(label_file, 'r')\n",
    "    ##############################################\n",
    "    labeld_input = input_files[i][:-4]+'_labeled.csv'\n",
    "    f3 = open(labeld_input, 'w')\n",
    "    ##############################################\n",
    "\n",
    "    A = f1.readlines()\n",
    "    B = f2.readlines()\n",
    "\n",
    "    for j, (a,b) in enumerate(zip(A,B)):\n",
    "        if j != len(A):\n",
    "            f3.write(a[:-1] + ','+b.split(',')[-1])\n",
    "        else:   \n",
    "            f3.write(a[:] + ','+b.split(','))\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    print(f'Trace {i+1} done.')\n",
    "\n",
    "for f in input_files:\n",
    "    os.system(f'rm {f}')\n",
    "print('Original unlabeled file deleted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
