{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import swifter\n",
    "from collections import namedtuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(L):\n",
    "    return sum(L)/len(L)\n",
    "\n",
    "def mi_event_parsing(miinfofile):\n",
    "    def nr_pci_track():\n",
    "        if miinfofile.loc[i, \"PCI\"] == 65535: ## 65535 is for samgsung phone.\n",
    "            nr_pci = '-'\n",
    "        else:\n",
    "            nr_pci = miinfofile.loc[i, \"PCI\"]\n",
    "        return nr_pci\n",
    "\n",
    "    nr_pci = None ## Initial Unknown\n",
    "     \n",
    "    lte_4G_handover_list = []   #4G 狀態下LTE eNB 的 handover\n",
    "    \n",
    "    nr_setup_list = []          #gNB cell addition\n",
    "    nr_handover_list = []       #gNB cell changes (eNB stays the same)\n",
    "    nr_removal_list = []        #gNB cell removal\n",
    "        \n",
    "    lte_5G_handover_list = []   #(eNB1, gNB1) -> (eNB2, gNB1) #gNB stays the same\n",
    "    nr_lte_handover_list = []   #both NR cell and LTE cell have handover\n",
    "    \n",
    "    eNB_to_MN_list = []\n",
    "    MN_to_eNB_list = []\n",
    "    \n",
    "    scg_failure_list = []       #gNB handover failure\n",
    "    reestablish_list_type2 = [] #eNB handover failure\n",
    "    reestablish_list_type3 = []\n",
    "    \n",
    "    nr_handover = 0\n",
    "    nr_handover_start_index = None\n",
    "    lte_handover = 0\n",
    "    lte_handover_start_index = None\n",
    "    nr_release = 0\n",
    "    nr_release_start_index = None\n",
    "    \n",
    "    lte_failure = 0\n",
    "    lte_failure_start_index = None\n",
    "    \n",
    "    handover_num = 0\n",
    "    \n",
    "    for i in range(len(miinfofile)):\n",
    "        if miinfofile.loc[i, \"type_id\"] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            nr_pci = nr_pci_track()\n",
    "            continue\n",
    "            \n",
    "        if miinfofile.loc[i, \"nr-rrc.t304\"]:\n",
    "            if nr_handover == 0:    \n",
    "                nr_handover = 1\n",
    "                nr_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"lte-rrc.t304\"]:\n",
    "            if lte_handover == 0:\n",
    "                lte_handover = 1\n",
    "                lte_handover_start_index = i\n",
    "                \n",
    "        if miinfofile.loc[i, \"nr-Config-r15: release (0)\"]:\n",
    "            if nr_release == 0:\n",
    "                nr_release = 1\n",
    "                nr_release_start_index = i\n",
    "           \n",
    "        if (nr_handover or lte_handover or nr_release) and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            handover_num +=1\n",
    "        \n",
    "        \n",
    "        #handover 種類分類\n",
    "        #------------------------------------------------------------------------------\n",
    "        if lte_handover and not nr_handover and not nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just lte cell handover event\n",
    "            lte_handover = 0\n",
    "            lte_4G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "\n",
    "        if lte_handover and not nr_handover and nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:    # LTE Ho and nr release \n",
    "            lte_handover = 0\n",
    "            nr_release = 0\n",
    "            MN_to_eNB_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "        \n",
    "        if nr_handover and not lte_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:  # just nr cell handover event\n",
    "            nr_handover = 0\n",
    "            if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                nr_setup_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "            else:\n",
    "                nr_handover_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            #additional judgement:\n",
    "            #----------------------------\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"] and nr_pci != None:\n",
    "            #    print(\"Warning: dualConnectivityPHR setup may not mean nr cell addition\", mi_file, i)\n",
    "            #if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]==0 and not (nr_pci != None and nr_pci != miinfofile.loc[nr_handover_start_index, \"nr_pci\"]): \n",
    "            #    print(\"Warning: nr-rrc.t304 without dualConnectivityPHR setup may not mean nr cell handover\", mi_file, i, nr_handover_start_index, miinfofile.loc[nr_handover_start_index, \"nr_pci\"], nr_pci)\n",
    "                \n",
    "        if lte_handover and nr_handover and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:      # both nr cell and lte cell handover event\n",
    "            lte_handover = 0\n",
    "            nr_handover = 0\n",
    "            if nr_pci == miinfofile.loc[lte_handover_start_index, \"nr_physCellId\"]: \n",
    "                lte_5G_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            else:\n",
    "                ##############\n",
    "                if miinfofile.loc[nr_handover_start_index, \"dualConnectivityPHR: setup (1)\"]:     #This if-else statement classifies whether it is nr addition or nr handover\n",
    "                    eNB_to_MN_list.append([miinfofile.loc[nr_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])       \n",
    "                else:\n",
    "                    nr_lte_handover_list.append([miinfofile.loc[lte_handover_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if not lte_handover and  nr_release and miinfofile.loc[i, \"rrcConnectionReconfigurationComplete\"]:\n",
    "            nr_release=0\n",
    "            nr_removal_list.append([miinfofile.loc[nr_release_start_index, \"time\"], miinfofile.loc[i, \"time\"]])\n",
    "            \n",
    "        if miinfofile.loc[i, \"scgFailureInformationNR-r15\"]:\n",
    "            scg_failure_list.append([miinfofile.loc[i, \"time\"], miinfofile.loc[i, \"time\"]]) \n",
    "            \n",
    "        if miinfofile.loc[i, \"rrcConnectionReestablishmentRequest\"]:\n",
    "            if lte_failure == 0:\n",
    "                lte_failure = 1\n",
    "                lte_failure_start_index = i\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishment\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type2.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "        if lte_failure and miinfofile.loc[i, \"rrcConnectionReestablishmentReject\"]:\n",
    "            lte_failure = 0\n",
    "            reestablish_list_type3.append([miinfofile.loc[lte_failure_start_index, \"time\"], miinfofile.loc[lte_failure_start_index, \"time\"]])\n",
    "            \n",
    "    return [lte_4G_handover_list, nr_setup_list, nr_handover_list, nr_removal_list, lte_5G_handover_list, nr_lte_handover_list, eNB_to_MN_list, MN_to_eNB_list, scg_failure_list, reestablish_list_type2, reestablish_list_type3], handover_num\n",
    "\n",
    "def collect_ho_event(mi_rrc_df):\n",
    "        l, _ = mi_event_parsing(mi_rrc_df)\n",
    "        for i in range(0, 11):\n",
    "            l[i] = [j[0] for j in l[i]]\n",
    "        d = {'lte': l[0], 'nr_setup': l[1], 'gNB_ho': l[2], 'nr_rel': l[3], \"MN_changed\": l[4],\"MN_SN_changed\": l[5], \"eNB_to_MN_changed\": l[6], \"MN_to_eNB_changed\": l[7], \"gNB_fail\": l[8], \"type2_fail\": l[9], \"type3_fail\": l[10]}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mi_ho(df):\n",
    "    def NR_OTA():\n",
    "        if df[\"type_id\"].iloc[i] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def find_1st_after(target, look_after=1):\n",
    "        for j in range(i, len(df)):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if (t_ - t).total_seconds() > look_after:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "\n",
    "    def find_1st_before(target, look_before=1):\n",
    "        for j in range(i, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "\n",
    "    HO = namedtuple('HO','start, end, others', defaults=(None,None))\n",
    "\n",
    "    D = {\n",
    "        'Conn_Rel':[], \n",
    "        'Conn_Req':[], # Setup\n",
    "        'LTE_HO': [], # LTE -> newLTE\n",
    "        'MN_HO': [], # LTE + NR -> newLTE + NR\n",
    "        'eNB_to_ENDC': [], # LTE -> LTE + NR => NR setup\n",
    "        'gNB_Rel': [], # LTE + NR -> LTE\n",
    "        'gNB_HO': [], # LTE + NR -> LTE + newNR\n",
    "        # 'HOF': [], # Didn't defined yet.\n",
    "        'RLF': [],\n",
    "        'SCG_RLF': [],\n",
    "        }\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if NR_OTA():\n",
    "            continue\n",
    "\n",
    "        # t = df[\"time\"].iloc[i]\n",
    "        t = df[\"Timestamp\"].iloc[i]\n",
    "        \n",
    "        if df[\"rrcConnectionRelease\"].iloc[i] == 1:\n",
    "            D['Conn_Rel'].append(HO(start=t))\n",
    "\n",
    "        if df[\"rrcConnectionRequest\"].iloc[i] == 1:\n",
    "            a = find_1st_after('rrcConnectionReconfigurationComplete',look_after=2)[0]\n",
    "            b = find_1st_after('securityModeComplete',look_after=2)[0]\n",
    "            end = a if a > b else b\n",
    "            D['Conn_Req'].append(HO(start=t,end=end))\n",
    "        \n",
    "        if df[\"lte-rrc.t304\"].iloc[i] == 1:\n",
    "            others = ''\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            serv_cell, target_cell = df[\"PCI\"].iloc[i], df['lte_targetPhysCellId'].iloc[i]\n",
    "            serv_freq, target_freq = df[\"Freq\"].iloc[i], df['dl-CarrierFreq'].iloc[i]\n",
    "            if df[\"SCellToAddMod-r10\"].iloc[i] == 1:\n",
    "                n =len(str(df[\"SCellIndex-r10.1\"].iloc[i]).split('@'))\n",
    "                others=f'Set up {n} SCell.'\n",
    "            \n",
    "            if serv_freq != target_freq:\n",
    "                others += \" Inter freq. HO\"\n",
    "\n",
    "            if df[\"nr-rrc.t304\"].iloc[i] == 1 and df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    D['eNB_to_ENDC'].append(HO(start=t, end=end, others=others))\n",
    "                    # print(1, t, f\"Serving Cell: {serv_cell}->{target_cell}\")  \n",
    "                else:    \n",
    "                    D['MN_HO'].append(HO(start=t, end=end, others=others))\n",
    "            else:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    a, b = find_1st_before(\"scgFailureInformationNR-r15\")\n",
    "                    if a is not None:\n",
    "                        others += \" Caused by scg-failure.\"\n",
    "                    D['gNB_Rel'].append(HO(start=t, end=end, others=others))\n",
    "                else:\n",
    "                    D['LTE_HO'].append(HO(start=t, end=end, others=others))\n",
    "\n",
    "        if df[\"nr-rrc.t304\"].iloc[i] == 1 and not df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            D['gNB_HO'].append(HO(start=t,end=end))\n",
    "\n",
    "        if df[\"rrcConnectionReestablishmentRequest\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReestablishmentComplete', look_after=1)\n",
    "            b, _ = find_1st_after('rrcConnectionReestablishmentReject', look_after=1)\n",
    "            others = df[\"reestablishmentCause\"].iloc[i]\n",
    "            if end is not None: \n",
    "                # Type II\n",
    "                D['RLF'].append(HO(start=t,end=end,others=others))\n",
    "            else: \n",
    "                # Type III\n",
    "                D['RLF'].append(HO(start=t,end=b,others=others)) # End for Type III?\n",
    "            \n",
    "        if df[\"scgFailureInformationNR-r15\"].iloc[i] == 1:\n",
    "            others = df[\"failureType-r15\"].iloc[i]\n",
    "            D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_dict(band, d):\n",
    "    D = d.copy()\n",
    "    for key in list(d.keys()):\n",
    "        if not key.endswith(' '+band):\n",
    "            D.pop(key)\n",
    "    return D\n",
    "\n",
    "class ss_dict:\n",
    "    def __init__(self,pd_data=None,d=None): ## Input pd_df.iloc[index]\n",
    "        self.dict = {'PCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    def serv_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        serv_cell_id = pd_data[\"Serving Cell Index\"]\n",
    "        pci = pd_data[\"PCI\"]\n",
    "        rsrp = float(pd_data[\"RSRP(dBm)\"])\n",
    "        rsrq = float(pd_data[\"RSRQ(dB)\"])\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        if serv_cell_id == \"PCell\":\n",
    "            self.dict['PCell'][0].append(rsrp)\n",
    "            self.dict['PCell'][1].append(rsrq)\n",
    "            self.dict['PCell'][2].append(t)\n",
    "            # self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "        else:\n",
    "            self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "            # s = pci + ' ' + self.earfcn\n",
    "            # if s in \n",
    "    def nei_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        for i in range(9, len(pd_data), 3):\n",
    "            if pd_data[i] == '-':\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[str(pd_data[i])+' '+earfcn] = [[rsrp], [rsrq], [t]]              \n",
    "    \n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] = d1[key][0] + d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return ss_dict(d=d1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def sort_dict_by_time(self):\n",
    "        def sort_element(element):\n",
    "            d1 = [ [element[0][i], element[1][i], element[2][i]] for i in range(len(element[0]))]\n",
    "            d1.sort(key=lambda data:data[2])\n",
    "            RSRP = [i[0] for i in d1]\n",
    "            RSRQ = [i[1] for i in d1]\n",
    "            T = [i[2] for i in d1]\n",
    "            return [RSRP, RSRQ, T]\n",
    "        sorted_D = {}\n",
    "        for k in list(self.dict.keys()):\n",
    "            sorted_D[k] = sort_element(self.dict[k])\n",
    "        self.dict = sorted_D\n",
    "\n",
    "\n",
    "class nr_ss_dict:\n",
    "    def __init__(self, pd_data=None, d=None):\n",
    "        self.dict = {'PSCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    \n",
    "    def serv_cell(self, pd_data):\n",
    "        self.pscell = pd_data[\"Serving Cell PCI\"]\n",
    "        do = False\n",
    "        for cell in self.dict.keys():\n",
    "            if self.pscell == cell:\n",
    "                self.dict[\"PSCell\"][0] += self.dict[cell][0]\n",
    "                self.dict[\"PSCell\"][1] += self.dict[cell][1]\n",
    "                self.dict[\"PSCell\"][2] += self.dict[cell][2]\n",
    "                do,x = True, cell\n",
    "                break\n",
    "        if do:\n",
    "            self.dict.pop(x)\n",
    "            \n",
    "    def nei_cell(self, pd_data):\n",
    "        arfcn = pd_data[\"Raster ARFCN\"]\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        for i in range(6, len(pd_data), 3):\n",
    "            if pd_data[i] == '-':\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[pd_data[i]] = [[rsrp], [rsrq], [t]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] += d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return nr_ss_dict(d=d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data for a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_create(dir1, dir2, ci_file, outfile, ul_df, dl_df):\n",
    "    base_dir1 = dir1\n",
    "    base_dir2 = dir2\n",
    "    # out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "    out_file = outfile\n",
    "    f = open(out_file, 'w') \n",
    "\n",
    "    d1 = os.path.join(base_dir1,\"data\")\n",
    "    d2 = os.path.join(base_dir2,\"data\")\n",
    "\n",
    "    excessive_latency_value = 0.1\n",
    "    \n",
    "    # # Collect gps and gpsspeed from cellinfo\n",
    "    try:\n",
    "        ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "        ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "    except pd.errors.ParserError:\n",
    "        print(f'preprocess {ci_file}')\n",
    "        gps_dir = '/'.join(ci_file.split('/')[:-1])\n",
    "        os.system(f'python3 ./csv_processing.py {gps_dir}')\n",
    "        ci_df = pd.read_csv(ci_file[:-4]+'_new.csv', dtype=str)\n",
    "        ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "    GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "    \n",
    "    # Collect rsrp infomation\n",
    "    mi_ml1_dfs = []\n",
    "    nr_mi_ml1_dfs = []\n",
    "    HO_events_list = []\n",
    "    \n",
    "\n",
    "    for d in [d1, d2]:\n",
    "        matches = filter(lambda x: x.endswith('ml1.csv'), os.listdir(d))\n",
    "        ml1_filenames = sorted(list(matches))\n",
    "        mi_ml1_file = os.path.join(d, ml1_filenames[0])\n",
    "        mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "        mi_ml1_df[\"Timestamp\"] = mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        mi_ml1_dfs.append(mi_ml1_df)\n",
    "\n",
    "        nr_mi_ml1_file = os.path.join(d, ml1_filenames[1])\n",
    "        nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "        nr_mi_ml1_df[\"Timestamp\"] = nr_mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        nr_mi_ml1_dfs.append(nr_mi_ml1_df)\n",
    "\n",
    "        # Collect Ho information\n",
    "        matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(d))\n",
    "        mi_rrc_filename = list(matches)[0]\n",
    "        mi_rrc_file = os.path.join(d, mi_rrc_filename)\n",
    "        mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "        mi_rrc_df[\"Timestamp\"] = mi_rrc_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        HO_events = parse_mi_ho(mi_rrc_df)\n",
    "        HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "        HO_events_list.append(HO_events)\n",
    "\n",
    "    columns = [\"Timestamp\", \"lat\", \"long\", \"gpsspeed\"]+[\n",
    "        'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF','SCG_RLF',\n",
    "        \"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "        \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    ]*2 + [\"dl-lossrate\", \"ul-lossrate\", \"dl-exc-lat\", \"ul-exc-lat\",\"dl-latency\", \"ul-latency\"]\n",
    "\n",
    "\n",
    "    f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "    i_ci = 0\n",
    "    i_pcap = [0,0]\n",
    "    i_ = [[0,0], [0,0]] # For increase speed\n",
    "    data_buffers = [{'rsrp':0, 'rsrq':0}, {'rsrp':0, 'rsrq':0}]\n",
    "\n",
    "    \n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "        ss_relateds = []\n",
    "        HO_relateds = []\n",
    "        # Get GPS informations\n",
    "        # ========================================================================\n",
    "        gps_related = []\n",
    "\n",
    "        for i in range(i_ci, len(ci_df)):\n",
    "            t = ci_df['Date'].iloc[i]\n",
    "            lat = ci_df['GPSLat'].iloc[i]\n",
    "            long = ci_df['GPSLon'].iloc[i]\n",
    "            gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "            elif t > time_point:\n",
    "                i_ci = i\n",
    "                break\n",
    "        \n",
    "\n",
    "        gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "        gps_related = [str(feature) for feature in gps_related]\n",
    "        # print(f\"{time_point} {gps_info}\")\n",
    "\n",
    "        for j in range(2):\n",
    "            # ==========================================================================\n",
    "            # Get signal strength informations\n",
    "            ss_related = []\n",
    "\n",
    "            SS_DICT = ss_dict()\n",
    "            for i in range(i_[j][0], len(mi_ml1_df)):\n",
    "                t = mi_ml1_df['Timestamp'].iloc[i]\n",
    "                serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "                \n",
    "                if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "                    SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "                elif t > time_point:\n",
    "                    i_[j][0] = i\n",
    "                    break\n",
    "            \n",
    "            # Get primary serv cell rsrp, rsrq \n",
    "            if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "                pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "                pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "                data_buffers[j]['rsrp'], data_buffers[j]['rsrq'] = pcell_rsrp, pcell_rsrq\n",
    "            else:\n",
    "                pcell_rsrp, pcell_rsrq = data_buffers[j]['rsrp'], data_buffers[j]['rsrq'] # No sample value, use the previous one\n",
    "            SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "            # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "            if len(SS_DICT.dict) != 0:\n",
    "                cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "                cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "                cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "                SS_DICT.dict.pop(cell1)\n",
    "            else:\n",
    "                # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "                cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "            if len(SS_DICT.dict) != 0:\n",
    "                cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "                cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "                cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "                SS_DICT.dict.pop(cell2)\n",
    "            else:\n",
    "                # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "                cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "                # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "            ss_related += [pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "            NR_SS_DICT = nr_ss_dict()\n",
    "            for i in range(i_[j][1], len(nr_mi_ml1_df)):\n",
    "                t = nr_mi_ml1_df['Timestamp'].iloc[i]\n",
    "                serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "                \n",
    "                if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                    NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "                elif t > time_point:\n",
    "                    i_[j][1] = i\n",
    "                    break\n",
    "            \n",
    "            # Get primary secondary serv cell rsrp, rsrq \n",
    "            if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "                pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "                pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "            else:\n",
    "                # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "                pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "            NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "            # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "            if len(NR_SS_DICT.dict) != 0:\n",
    "                cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "                cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "                cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "                NR_SS_DICT.dict.pop(cell1)\n",
    "            else:\n",
    "                # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "                cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "            if len(NR_SS_DICT.dict) != 0:\n",
    "                cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "                cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "                cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "                NR_SS_DICT.dict.pop(cell2)\n",
    "            else:\n",
    "                # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "                cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "            \n",
    "            # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "            ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "            ss_related = [str(feature) for feature in ss_related]\n",
    "            ss_relateds.append(ss_related)\n",
    "            # ================================================================================\n",
    "            # Get HO informations\n",
    "            HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "            for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "                for ho in HO_events[ho_type]:\n",
    "                    t = ho.start\n",
    "                    if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                        HO_related[i] += 1\n",
    "                    elif t > time_point:\n",
    "                        break\n",
    "            \n",
    "            HO_related = [str(feature) for feature in HO_related]\n",
    "            HO_relateds.append(HO_related)\n",
    "        # ========================================================================\n",
    "        # Get DL/UL latency, loss...\n",
    "        performance_related = []\n",
    "\n",
    "        loss_col = f\"lost.{Setting[dev1]}.{Setting[dev2]}\"\n",
    "        latency_col = f\"latency.{Setting[dev1]}.{Setting[dev2]}\"\n",
    "\n",
    "        dl_lats, dl_excessive_lats, dl_losses = [], [], []\n",
    "        for i in range(i_pcap[0], len(dl_df)):\n",
    "            t = dl_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                dl_lat = float(dl_df[latency_col].iloc[i])\n",
    "                dl_loss = dl_df[loss_col].iloc[i]\n",
    "                dl_lats.append(dl_lat)\n",
    "                if dl_loss:\n",
    "                    dl_losses.append(t)\n",
    "                if dl_lat >  excessive_latency_value:\n",
    "                    dl_excessive_lats.append(t)\n",
    "            elif t > time_point:\n",
    "                i_pcap[0] = i\n",
    "                break\n",
    "\n",
    "        if len(dl_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "            dl_exc_rate = len(dl_excessive_lats)/len(dl_lats)\n",
    "\n",
    "        if (len(dl_losses)+len(dl_lats)) == 0:\n",
    "            pass # No package; will use previous value\n",
    "        else:\n",
    "            dl_loss_rate = len(dl_losses)/(len(dl_losses)+len(dl_lats))\n",
    "\n",
    "\n",
    "        ul_lats, ul_excessive_lats, ul_losses = [], [], []\n",
    "        for i in range(i_pcap[1], len(ul_df)):\n",
    "            t = ul_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                ul_lat = float(ul_df[latency_col].iloc[i])\n",
    "                ul_loss = ul_df[loss_col].iloc[i]\n",
    "                ul_lats.append(ul_lat)\n",
    "                if ul_loss:\n",
    "                    ul_losses.append(t)\n",
    "                if ul_lat >  excessive_latency_value:\n",
    "                    ul_excessive_lats.append(t)\n",
    "            elif t > time_point:\n",
    "                i_pcap[1] = i\n",
    "                break\n",
    "\n",
    "        if len(ul_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "            ul_exc_rate = len(ul_excessive_lats)/len(ul_lats)\n",
    "\n",
    "        if (len(ul_losses)+len(ul_lats)) == 0:\n",
    "            pass # No package; will use previous value\n",
    "        else:\n",
    "            ul_loss_rate = len(ul_losses)/(len(ul_losses)+len(ul_lats))\n",
    "            \n",
    "        performance_related += [dl_loss_rate, ul_loss_rate, dl_exc_rate, ul_exc_rate, dl_avg_lat, ul_avg_lat]\n",
    "        performance_related = [str(feature) for feature in performance_related]\n",
    "\n",
    "\n",
    "        f.write(\",\".join([str(time_point)]+gps_related+HO_relateds[0]+ss_relateds[0]+HO_relateds[1]+ss_relateds[1]+performance_related)+\"\\n\") \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace #01 from 2023-02-04 14:59:01 to 2023-02-04 15:06:23.\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc01/#01 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc02/#01\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#01_B3&B7.csv\n",
      "qc01 qc02\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc01/#01 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc03/#01\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#01_B3&B8.csv\n",
      "qc01 qc03\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc02/#01 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc03/#01\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#01_B7&B8.csv\n",
      "qc02 qc03\n",
      "Trace #02 from 2023-02-04 15:57:53 to 2023-02-04 16:02:26.\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc01/#02 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc02/#02\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#02_B3&B7.csv\n",
      "qc01 qc02\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc01/#02 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc03/#02\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#02_B3&B8.csv\n",
      "qc01 qc03\n",
      "/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc02/#02 /home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/qc03/#02\n",
      "/home/wmnlab/D/sheng-ru/ml_data/2023-02-04#02_B7&B8.csv\n",
      "qc02 qc03\n"
     ]
    }
   ],
   "source": [
    "Setting = {'qc01': 'B3', 'qc02': 'B7', 'qc03': 'B8'}\n",
    "base_dir = '/home/wmnlab/D/sheng-ru/test/test_data/'\n",
    "base_dir = '/home/wmnlab/D/database/2023-02-04/_Bandlock_Udp_B3_B7_B8_RM500Q/'\n",
    "date = [x for x in list(base_dir.split('/')) if len(x) != 0][-2]\n",
    "\n",
    "matches = filter(lambda x: x.startswith('qc') or x.startswith('sm'), os.listdir(base_dir))\n",
    "combo_dir = os.path.join(base_dir, 'combo')\n",
    "device_dir = [os.path.join(base_dir, x) for x in list(matches)]\n",
    "device_dir.sort()\n",
    "\n",
    "parent_dir = str(Path(base_dir).parent.absolute())\n",
    "gps_dir = os.path.join(parent_dir, 'gps')\n",
    "matches = list(filter(lambda x: 'ci' in x, os.listdir(gps_dir)))\n",
    "matches.sort()\n",
    "ci_file = os.path.join(gps_dir, matches[-1])\n",
    "\n",
    "for trace in sorted(os.listdir(combo_dir)):\n",
    "    ct_dir = os.path.join(combo_dir,trace) # combo+trace dir\n",
    "\n",
    "    ul_loss_lat = os.path.join(ct_dir, \"udp_uplk_combo_loss_latency.csv\")\n",
    "    ul_loss_lat_df = pd.read_csv(ul_loss_lat)\n",
    "    ul_loss_lat_df[\"Timestamp\"] = ul_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "    dl_loss_lat = os.path.join(ct_dir, \"udp_dnlk_combo_loss_latency.csv\")\n",
    "    dl_loss_lat_df = pd.read_csv(dl_loss_lat)\n",
    "    dl_loss_lat_df[\"Timestamp\"] = dl_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "    # Get timepoint from start to end\n",
    "    front_cut, back_cut = 5, 5\n",
    "    TS = 1\n",
    "    tp_range = 1\n",
    "    start = dl_loss_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut) # open the downlink file to decide start time and end time\n",
    "    end = dl_loss_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "    start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "    print(f'Trace {trace} from {start} to {end}.')\n",
    "    N = int((end - start).total_seconds()) # How many time_point\n",
    "\n",
    "\n",
    "    for i, device in enumerate(device_dir):\n",
    "        for j in range(i+1,len(device_dir)):\n",
    "            device2 = device_dir[j]\n",
    "            \n",
    "            dt_dir = os.path.join(device, trace) # device+trace dir\n",
    "            dt_dir2 = os.path.join(device2, trace)\n",
    "            print(dt_dir, dt_dir2)\n",
    "            dev1, dev2 = device[-4:], device2[-4:]\n",
    "            b1, b2 = Setting[dev1], Setting[dev2]\n",
    "            outfile = os.path.join('/home/wmnlab/D/sheng-ru/ml_data', date + f'{trace}_{b1}&{b2}.csv')\n",
    "            print(outfile)\n",
    "            print(dev1, dev2)\n",
    "            data_create(dt_dir, dt_dir2, ci_file, outfile, ul_loss_lat_df, dl_loss_lat_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lost.B3.B7', 'excl.B3.B7', 'latency.B3.B7']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [x for x in list(ul_loss_lat_df.columns) if Setting[dev1] in x and Setting[dev2] in x]\n",
    "matches = filter(lambda x: 'lost' in x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wmnlab/D/sheng-ru/test'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Path(base_dir)\n",
    "str(x.parent.absolute())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2022-12-22 15:04:43 to 2022-12-22 15:10:56.\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/sm05/#01\"\n",
    "out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "f = open(out_file, 'w') \n",
    "\n",
    "# Collecting the UDP Latency and Loss information first\n",
    "dir = os.path.join(base_dir,\"data\")\n",
    "\n",
    "dl_lat_file = os.path.join(dir, \"udp_dnlk_latency.csv\")\n",
    "dl_lat_df = pd.read_csv(dl_lat_file)\n",
    "dl_lat_df[\"Timestamp\"] = dl_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "dl_loss_file = os.path.join(dir, \"udp_dnlk_loss_timestamp.csv\")\n",
    "dl_loss_df = pd.read_csv(dl_loss_file)\n",
    "dl_loss_df[\"Timestamp\"] = dl_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_lat_file = os.path.join(dir, \"udp_uplk_latency.csv\")\n",
    "ul_lat_df = pd.read_csv(ul_lat_file)\n",
    "ul_lat_df[\"Timestamp\"] = ul_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_loss_file = os.path.join(dir, \"udp_uplk_loss_timestamp.csv\")\n",
    "ul_loss_df = pd.read_csv(ul_loss_file)\n",
    "ul_loss_df[\"Timestamp\"] = ul_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "# Get timepoint from start to end and get the latency and loss\n",
    "front_cut = 5\n",
    "back_cut = 5\n",
    "TS = 1\n",
    "tp_range = 1\n",
    "\n",
    "start = dl_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "end = dl_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "print(f'From {start} to {end}.')\n",
    "N = int((end - start).total_seconds()) # How many time_point\n",
    "\n",
    "# Collect rsrp infomation\n",
    "matches = filter(lambda x: x.endswith('ml1_new.csv'), os.listdir(dir))\n",
    "ml1_filenames = sorted(list(matches))\n",
    "mi_ml1_file = os.path.join(dir, ml1_filenames[0])\n",
    "mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "nr_mi_ml1_file = os.path.join(dir, ml1_filenames[1])\n",
    "nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "nr_mi_ml1_df[\"time\"] = nr_mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "\n",
    "# Collect gps and gpsspeed from cellinfo\n",
    "dir = os.path.join(base_dir,\"middle\")\n",
    "matches = filter(lambda x: x.startswith('cimon'), os.listdir(dir))\n",
    "ci_filename = list(matches)[0]\n",
    "ci_file = os.path.join(dir, ci_filename)\n",
    "ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "\n",
    "# Collect Ho information\n",
    "matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(dir))\n",
    "mi_rrc_filename = list(matches)[0]\n",
    "mi_rrc_file = os.path.join(dir, mi_rrc_filename)\n",
    "mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "mi_rrc_df[\"time\"] = mi_rrc_df[\"time\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "HO_events = parse_mi_ho(mi_rrc_df)\n",
    "HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "\n",
    "columns = [\n",
    "    \"Timestamp\",\n",
    "    \"lat\", \"long\", \"gpsspeed\",\n",
    "    'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF_II','RLF_III','SCG_RLF',\n",
    "    \"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "    \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    \"DL-lat\", \"DL-lossrate\", \"UL-lat\", \"UL-lossrate\"\n",
    "]\n",
    "f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "i_ = [0,0,0,0,0,0,0] # For increase speed\n",
    "for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "\n",
    "    # ========================================================================\n",
    "    # Get DL/UL latency, loss...\n",
    "    perfermance_related = []\n",
    "\n",
    "    dl_lats = []\n",
    "    for i in range(i_[0], len(dl_lat_df)):\n",
    "        t = dl_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_lat = float(dl_lat_df['latency'].iloc[i])\n",
    "            dl_lats.append(dl_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[0] = i\n",
    "            break\n",
    "\n",
    "    if len(dl_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        # perfermance_related.append('-')\n",
    "        perfermance_related.append(dl_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(dl_avg_lat)\n",
    "\n",
    "    dl_losses = []\n",
    "    for i in range(i_[1], len(dl_loss_df)):\n",
    "        t = dl_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[1] = i\n",
    "            break\n",
    "\n",
    "    if (len(dl_losses)+len(dl_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(dl_losses)/(len(dl_losses)+len(dl_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    ul_lats = []\n",
    "    for i in range(i_[2], len(ul_lat_df)):\n",
    "        t = ul_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_lat = float(ul_lat_df['latency'].iloc[i])\n",
    "            ul_lats.append(ul_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[2] = i\n",
    "            break\n",
    "\n",
    "    if len(ul_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "        perfermance_related.append(ul_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(ul_avg_lat)\n",
    "\n",
    "    ul_losses = []\n",
    "    for i in range(i_[3], len(ul_loss_df)):\n",
    "        t = ul_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[3] = i\n",
    "            break\n",
    "\n",
    "    if (len(ul_losses)+len(ul_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(ul_losses)/(len(ul_losses)+len(ul_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    perfermance_related = [str(feature) for feature in perfermance_related]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Get GPS informations\n",
    "    gps_related = []\n",
    "\n",
    "    for i in range(i_[4], len(ci_df)):\n",
    "        t = ci_df['Date'].iloc[i]\n",
    "        lat = ci_df['GPSLat'].iloc[i]\n",
    "        long = ci_df['GPSLon'].iloc[i]\n",
    "        gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "        elif t > time_point:\n",
    "            i_[4] = i\n",
    "            break\n",
    "    \n",
    "\n",
    "    gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "    gps_related = [str(feature) for feature in gps_related]\n",
    "    # print(f\"{time_point} {gps_info}\")\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Get signal strength informations\n",
    "    ss_related = []\n",
    "\n",
    "    SS_DICT = ss_dict()\n",
    "    for i in range(i_[5], len(mi_ml1_df)):\n",
    "        t = mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "        \n",
    "        if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "            SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "        elif t > time_point:\n",
    "            i_[5] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary serv cell rsrp, rsrq \n",
    "    if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "        pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "        pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "    else:\n",
    "        # pcell_rsrp, pcell_rsrq = '-', '-'\n",
    "        pcell_rsrp, pcell_rsrq = pcell_rsrp, pcell_rsrq # No sample value, use the previous one\n",
    "    SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "        SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "        SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "        # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    NR_SS_DICT = nr_ss_dict()\n",
    "    for i in range(i_[6], len(nr_mi_ml1_df)):\n",
    "        t = nr_mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "        \n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "        elif t > time_point:\n",
    "            i_[6] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary secondary serv cell rsrp, rsrq \n",
    "    if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "        pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "        pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "    else:\n",
    "        # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "        pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "    NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        NR_SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        NR_SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "    \n",
    "    # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    ss_related = [str(feature) for feature in ss_related]\n",
    "\n",
    "    # ================================================================================\n",
    "    # Get HO informations\n",
    "    HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "    for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "        for ho in HO_events[ho_type]:\n",
    "            t = ho.start\n",
    "            if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                HO_related[i] += 1\n",
    "            elif t > time_point:\n",
    "                break\n",
    "    \n",
    "    HO_related = [str(feature) for feature in HO_related]\n",
    "\n",
    "    f.write(\",\".join([str(time_point)]+gps_related+HO_related+ss_related+perfermance_related)+\"\\n\") \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'422': [[-90.109, -92.797, -93.578, -121.141, -103.07], [-15.492, -16.68, -23.008, -28.727, -25.156], [Timestamp('2022-12-22 15:10:55.193070'), Timestamp('2022-12-22 15:10:55.352650'), Timestamp('2022-12-22 15:10:55.672631'), Timestamp('2022-12-22 15:10:55.832622'), Timestamp('2022-12-22 15:10:55.992652')]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NR_SS_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input create from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trace #1\n",
      "Data loading for trace #1 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#1.csv\n",
      "Trace 1 done.\n",
      "Processing trace #2\n",
      "Data loading for trace #2 done.\n",
      "/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp/ml_data/input#2.csv\n",
      "Trace 2 done.\n"
     ]
    }
   ],
   "source": [
    "def ss_append(d, key):\n",
    "        ss_related.append(Average(d[key][0])) ## Avg RSRP of PCell\n",
    "        ss_related.append(Average(d[key][1])) ## Avg RSRQ of PCell\n",
    "        ss_related.append(d[key][0][0]) ## RSRP of first measure\n",
    "        ss_related.append(d[key][0][-1]) ## RSRP of of last measure\n",
    "        ss_related.append(d[key][1][0]) ## RSRQ of first measure\n",
    "        ss_related.append(d[key][1][-1]) ## RSRQ of last measure\n",
    "\n",
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_ml1_files = []\n",
    "CI_files = []\n",
    "\n",
    "redo=0\n",
    "CI_down = False\n",
    "for a in sorted(os.listdir(dir)):\n",
    "    if a == 'ml_data':\n",
    "        redo=1\n",
    "        continue\n",
    "    d0 = os.path.join(dir, a)\n",
    "    # CI_down = False\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        d00 = os.path.join(d0, b, 'data')\n",
    "        for c in sorted(os.listdir(d00)): \n",
    "            if c.endswith('txt_ml1_new.csv'):\n",
    "                d5 = os.path.join(d00, c)\n",
    "            if c.startswith('cimon') and c.endswith('new.csv') and CI_down == False:\n",
    "                CI_files.append(os.path.join(d00,c))\n",
    "                CI_down = True\n",
    "        All_ml1_files.append(d5)\n",
    "\n",
    "p_num = len(os.listdir(dir))-redo # How many device\n",
    "t_num = len(os.listdir(d0)) # How many trace\n",
    "\n",
    "ml1_files = []\n",
    "\n",
    "for i in range(t_num):\n",
    "    A = [All_ml1_files[(i)+t_num*j] for j in range(p_num)]\n",
    "    ml1_files.append(A)\n",
    "\n",
    "CI_dfs = []\n",
    "for i in range(1):\n",
    "    df = pd.read_csv(CI_files[i])\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x: pd.to_datetime(x))\n",
    "    CI_dfs.append(df)\n",
    "\n",
    "for tt in range(t_num):\n",
    "    print(f\"Processing trace #{tt+1}\")\n",
    "    \n",
    "    ml1_df_list = []\n",
    "    for file in ml1_files[tt]:\n",
    "        mi_ml1_df = pd.read_csv(file)\n",
    "        mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "        mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x)+dt.timedelta(hours=8))\n",
    "        ml1_df_list.append(mi_ml1_df)\n",
    "    \n",
    "    print(f\"Data loading for trace #{tt+1} done.\")\n",
    "    \n",
    "    start, end = start_end[tt]\n",
    "    TS = 30 # Time Slot (sec)\n",
    "    tp_range = 30 # Every time point look back range (sec)\n",
    "    num_cell = 3\n",
    "    #####################################################\n",
    "    try:\n",
    "        os.listdir(os.path.join(dir,'ml_data'))\n",
    "    except:\n",
    "        os.system(f\"mkdir {os.path.join(dir,'ml_data')}\")\n",
    "    f = open(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'), 'w') ## Out file !!!!!!!!\n",
    "    print(os.path.join(dir,'ml_data',f'input#{tt+1}.csv'))\n",
    "    #####################################################\n",
    "    n = int((end - start).total_seconds())\n",
    "    # HO = collect_ho_event(mi_rrc_df)\n",
    "    # A = list(HO.keys())\n",
    "    signal_strength = [\"avg_rsrp\", \"avg_rsrq\", \"rsrp_1st\", \"rsrp_last\", \"rsrq_1st\", \"rsrq_last\"]\n",
    "    gps = ['Latitude', 'Longtitude', 'GPSSpeed1','GPSSpeed2','GPSSpeed3','GPSSpeed4']\n",
    "    B = [\"B1 num\"] + signal_strength * num_cell + [\"B3 num\"] + signal_strength * num_cell + [\"B28 num\"] + signal_strength * num_cell\n",
    "    B[-1] += '\\n'\n",
    "    f.write(','.join(['time']+gps + B))\n",
    "\n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)]:\n",
    "        ######################################################################\n",
    "        # ho_time = list(np.zeros(len(A)))\n",
    "        # for i in range(len(A)):\n",
    "        #     for t in HO[A[i]]:\n",
    "        #         if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "        #             ho_time[i] += 1\n",
    "        # ho_time = [str(i) for i in ho_time]\n",
    "        #######################################################################\n",
    "        # GPS\n",
    "        CI_times = []\n",
    "        gps_lat = []\n",
    "        gps_lon = []\n",
    "        gps_speed = []\n",
    "\n",
    "        ci_df = CI_dfs[0]\n",
    "        for i in range(len(ci_df)):\n",
    "            t = ci_df[\"Date\"].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                CI_times.append(t)\n",
    "                gps_lat.append(ci_df[\"GPSLat\"].iloc[i])\n",
    "                gps_lon.append(ci_df[\"GPSLon\"].iloc[i])\n",
    "                gps_speed.append(ci_df[\"GPSSpeed\"].iloc[i])\n",
    "            elif t > time_point:\n",
    "                break\n",
    "\n",
    "        x = abs((CI_times[0] - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds()) \n",
    "        latitude = gps_lat[0]\n",
    "        lontitude = gps_lon[0]\n",
    "\n",
    "        for i, t in enumerate(CI_times):\n",
    "            x_ = abs((t - (time_point - dt.timedelta(seconds=tp_range/2))).total_seconds())\n",
    "            if x_ < x:\n",
    "                x = x_\n",
    "                latitude = gps_lat[i]\n",
    "                lontitude = gps_lon[i]\n",
    "        \n",
    "        \n",
    "        ind = list(np.linspace(0,len(gps_speed)-1,4))\n",
    "        ind = [round(i) for i in ind]\n",
    "        speed_related = [gps_speed[i] for i in ind]\n",
    "\n",
    "        GPS_related = [latitude, lontitude] + speed_related\n",
    "        GPS_related = [str(i) for i in GPS_related]\n",
    "        #######################################################################\n",
    "        # Signal Strength\n",
    "        d = ss_dict()\n",
    "        for df in ml1_df_list:\n",
    "            for i in range(len(df)):\n",
    "                t = df[\"time\"].iloc[i]\n",
    "                if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                    d += ss_dict(df.iloc[i])\n",
    "                elif t > time_point:\n",
    "                    break\n",
    "        d.sort_dict_by_time()\n",
    "        d = d.dict\n",
    "\n",
    "        ss_related = []\n",
    "        # ss_append(d, \"PCell\")\n",
    "        b1 = pop_dict('275',d)\n",
    "        b3 = pop_dict('1275',d)\n",
    "        b28 = pop_dict('9560',d)\n",
    "\n",
    "        for Band in [b1,b3,b28]:\n",
    "            ss_related.append(len(Band)) ## Num of detected serv + nei cell\n",
    "            N = num_cell\n",
    "            while N > 0:\n",
    "                if len(Band) == 0:\n",
    "                    for i in range(6):\n",
    "                        ss_related.append('-')\n",
    "                    N -= 1\n",
    "                else:\n",
    "                    a = max(Band, key=lambda x:Average(Band[x][0]))\n",
    "                    ss_append(Band, a)\n",
    "                    Band.pop(a)\n",
    "                    N -= 1\n",
    "\n",
    "        ss_related = [str(i) for i in ss_related]\n",
    "        if time_point != [start + dt.timedelta(seconds=i) for i in range(0, n+1, TS)][-1]:\n",
    "            ss_related[-1] += '\\n'\n",
    "        #######################################################################\n",
    "        \n",
    "        f.write(','.join([str(time_point)] + GPS_related + ss_related))\n",
    "    f.close()\n",
    "    print(f'Trace {tt+1} done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label input of all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace 1 done.\n",
      "Trace 2 done.\n",
      "Original unlabeled file deleted.\n"
     ]
    }
   ],
   "source": [
    "dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/test/_Bandlock_Udp\"\n",
    "All_input_files = []\n",
    "\n",
    "input_files = []\n",
    "label_files = []\n",
    "if 'ml_data' in os.listdir(dir):\n",
    "    d0 = os.path.join(dir, 'ml_data')\n",
    "    for b in sorted(os.listdir(d0)):\n",
    "        if 'input' in b and not 'label' in b and not '.csv#' in b:\n",
    "            input_files.append(os.path.join(d0, b))\n",
    "        elif 'label' in b and not 'input' in b:\n",
    "            label_files.append(os.path.join(d0, b))\n",
    "else:\n",
    "    print(\"Error, no dir ml_data\")\n",
    "\n",
    "input_files.sort()\n",
    "label_files.sort()\n",
    "\n",
    "for i in range(len(input_files)):\n",
    "    input_file = input_files[i]\n",
    "    f1 = open(input_file, 'r')\n",
    "\n",
    "    label_file = label_files[i]\n",
    "    f2 = open(label_file, 'r')\n",
    "    ##############################################\n",
    "    labeld_input = input_files[i][:-4]+'_labeled.csv'\n",
    "    f3 = open(labeld_input, 'w')\n",
    "    ##############################################\n",
    "\n",
    "    A = f1.readlines()\n",
    "    B = f2.readlines()\n",
    "\n",
    "    for j, (a,b) in enumerate(zip(A,B)):\n",
    "        if j != len(A):\n",
    "            f3.write(a[:-1] + ','+b.split(',')[-1])\n",
    "        else:   \n",
    "            f3.write(a[:] + ','+b.split(','))\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()\n",
    "    f3.close()\n",
    "    print(f'Trace {i+1} done.')\n",
    "\n",
    "for f in input_files:\n",
    "    os.system(f'rm {f}')\n",
    "print('Original unlabeled file deleted.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_data\n",
    "ml_data = '/home/wmnlab/ml_data'\n",
    "files = [os.path.join(ml_data, x) for x in os.listdir(ml_data)]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B3&B7\n",
      "300\n",
      "B3&B8\n",
      "300\n",
      "B7&B8\n",
      "300\n",
      "B3&B7\n",
      "274\n",
      "B3&B8\n",
      "274\n",
      "B7&B8\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "def find_text(x):\n",
    "    ind = x.find('#')\n",
    "    x = x[ind+4:-4].split('&')\n",
    "    x.sort()\n",
    "    x = '&'.join(x)\n",
    "    return x\n",
    "\n",
    "database = open('/home/wmnlab/ntu-experiments/sheng-ru/experiment/mobileinsight/database.csv', 'w')\n",
    "columns = ['B3&B7', 'B3&B8', 'B7&B8']\n",
    "database.write(','.join(columns)+'\\n')\n",
    "\n",
    "experiment_time = 300\n",
    "\n",
    "for file in files:\n",
    "    band_set = find_text(file)\n",
    "    f = open(file)\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    print(band_set)\n",
    "    \n",
    "    if len(df) > experiment_time:\n",
    "        print(len(df[-300:]))\n",
    "    else:\n",
    "        print(len(df[:]))\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "database.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
