{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import swifter\n",
    "from collections import namedtuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(L):\n",
    "    return sum(L)/len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mi_ho(df):\n",
    "    def NR_OTA():\n",
    "        if df[\"type_id\"].iloc[i] == \"5G_NR_RRC_OTA_Packet\":\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def find_1st_after(target, look_after=1):\n",
    "        for j in range(i, len(df)):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if (t_ - t).total_seconds() > look_after:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "        return None, None\n",
    "    \n",
    "    def find_1st_before(target, look_before=1):\n",
    "        for j in range(i, -1, -1):\n",
    "            t_ = df[\"Timestamp\"].iloc[j]\n",
    "            if (t - t_).total_seconds() > look_before:\n",
    "                return None, None\n",
    "            if df[target].iloc[j] not in [0,'0']:\n",
    "                return t_, j\n",
    "\n",
    "    HO = namedtuple('HO','start, end, others', defaults=(None,None))\n",
    "\n",
    "    D = {\n",
    "        'Conn_Rel':[], \n",
    "        'Conn_Req':[], # Setup\n",
    "        'LTE_HO': [], # LTE -> newLTE\n",
    "        'MN_HO': [], # LTE + NR -> newLTE + NR\n",
    "        'eNB_to_ENDC': [], # LTE -> LTE + NR => NR setup\n",
    "        'gNB_Rel': [], # LTE + NR -> LTE\n",
    "        'gNB_HO': [], # LTE + NR -> LTE + newNR\n",
    "        # 'HOF': [], # Didn't defined yet.\n",
    "        'RLF': [],\n",
    "        'SCG_RLF': [],\n",
    "        }\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if NR_OTA():\n",
    "            continue\n",
    "\n",
    "        # t = df[\"time\"].iloc[i]\n",
    "        t = df[\"Timestamp\"].iloc[i]\n",
    "        \n",
    "        if df[\"rrcConnectionRelease\"].iloc[i] == 1:\n",
    "            D['Conn_Rel'].append(HO(start=t))\n",
    "\n",
    "        if df[\"rrcConnectionRequest\"].iloc[i] == 1:\n",
    "            a = find_1st_after('rrcConnectionReconfigurationComplete',look_after=2)[0]\n",
    "            b = find_1st_after('securityModeComplete',look_after=2)[0]\n",
    "            try: end = a if a > b else b\n",
    "            except: end = None\n",
    "            D['Conn_Req'].append(HO(start=t,end=end))\n",
    "        \n",
    "        if df[\"lte-rrc.t304\"].iloc[i] == 1:\n",
    "            others = ''\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            serv_cell, target_cell = df[\"PCI\"].iloc[i], df['lte_targetPhysCellId'].iloc[i]\n",
    "            serv_freq, target_freq = df[\"Freq\"].iloc[i], df['dl-CarrierFreq'].iloc[i]\n",
    "            if df[\"SCellToAddMod-r10\"].iloc[i] == 1:\n",
    "                n =len(str(df[\"SCellIndex-r10.1\"].iloc[i]).split('@'))\n",
    "                others=f'Set up {n} SCell.'\n",
    "            \n",
    "            if serv_freq != target_freq:\n",
    "                others += \" Inter freq. HO\"\n",
    "\n",
    "            if df[\"nr-rrc.t304\"].iloc[i] == 1 and df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    D['eNB_to_ENDC'].append(HO(start=t, end=end, others=others))\n",
    "                    # print(1, t, f\"Serving Cell: {serv_cell}->{target_cell}\")  \n",
    "                else:    \n",
    "                    D['MN_HO'].append(HO(start=t, end=end, others=others))\n",
    "            else:\n",
    "                if serv_cell == target_cell and serv_freq == target_freq:\n",
    "                    a, b = find_1st_before(\"scgFailureInformationNR-r15\")\n",
    "                    if a is not None:\n",
    "                        others += \" Caused by scg-failure.\"\n",
    "                    D['gNB_Rel'].append(HO(start=t, end=end, others=others))\n",
    "                else:\n",
    "                    D['LTE_HO'].append(HO(start=t, end=end, others=others))\n",
    "\n",
    "        if df[\"nr-rrc.t304\"].iloc[i] == 1 and not df[\"dualConnectivityPHR: setup (1)\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReconfigurationComplete')\n",
    "            D['gNB_HO'].append(HO(start=t,end=end))\n",
    "\n",
    "        if df[\"rrcConnectionReestablishmentRequest\"].iloc[i] == 1:\n",
    "            end, _ = find_1st_after('rrcConnectionReestablishmentComplete', look_after=1)\n",
    "            b, _ = find_1st_after('rrcConnectionReestablishmentReject', look_after=1)\n",
    "            others = df[\"reestablishmentCause\"].iloc[i]\n",
    "            if end is not None: \n",
    "                # Type II\n",
    "                D['RLF'].append(HO(start=t,end=end,others=others))\n",
    "            else: \n",
    "                # Type III\n",
    "                D['RLF'].append(HO(start=t,end=b,others=others)) # End for Type III?\n",
    "            \n",
    "        if df[\"scgFailureInformationNR-r15\"].iloc[i] == 1:\n",
    "            others = df[\"failureType-r15\"].iloc[i]\n",
    "            D['SCG_RLF'].append(HO(start=t,others=others))\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_dict(band, d):\n",
    "    D = d.copy()\n",
    "    for key in list(d.keys()):\n",
    "        if not key.endswith(' '+band):\n",
    "            D.pop(key)\n",
    "    return D\n",
    "\n",
    "class ss_dict:\n",
    "    def __init__(self,pd_data=None,d=None): ## Input pd_df.iloc[index]\n",
    "        self.dict = {'PCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    def serv_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        serv_cell_id = pd_data[\"Serving Cell Index\"]\n",
    "        pci = pd_data[\"PCI\"]\n",
    "        rsrp = float(pd_data[\"RSRP(dBm)\"])\n",
    "        rsrq = float(pd_data[\"RSRQ(dB)\"])\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        if serv_cell_id == \"PCell\":\n",
    "            self.dict['PCell'][0].append(rsrp)\n",
    "            self.dict['PCell'][1].append(rsrq)\n",
    "            self.dict['PCell'][2].append(t)\n",
    "            # self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "        else:\n",
    "            self.dict[pci+' '+earfcn] = [[rsrp], [rsrq], [t]]\n",
    "            # s = pci + ' ' + self.earfcn\n",
    "            # if s in \n",
    "    def nei_cell(self, pd_data):\n",
    "        earfcn = pd_data[\"EARFCN\"]\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        for i in range(9, len(pd_data), 3):\n",
    "\n",
    "            if pd_data[i] == '-' or np.isnan(float(pd_data[i])):\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[str(pd_data[i])+' '+earfcn] = [[rsrp], [rsrq], [t]]              \n",
    "    \n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] = d1[key][0] + d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return ss_dict(d=d1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def sort_dict_by_time(self):\n",
    "        def sort_element(element):\n",
    "            d1 = [ [element[0][i], element[1][i], element[2][i]] for i in range(len(element[0]))]\n",
    "            d1.sort(key=lambda data:data[2])\n",
    "            RSRP = [i[0] for i in d1]\n",
    "            RSRQ = [i[1] for i in d1]\n",
    "            T = [i[2] for i in d1]\n",
    "            return [RSRP, RSRQ, T]\n",
    "        sorted_D = {}\n",
    "        for k in list(self.dict.keys()):\n",
    "            sorted_D[k] = sort_element(self.dict[k])\n",
    "        self.dict = sorted_D\n",
    "\n",
    "\n",
    "class nr_ss_dict:\n",
    "    def __init__(self, pd_data=None, d=None):\n",
    "        self.dict = {'PSCell':[[],[],[]]}\n",
    "        if pd_data is not None:\n",
    "            self.nei_cell(pd_data)\n",
    "            self.serv_cell(pd_data)\n",
    "        if d is not None:\n",
    "            self.dict = d\n",
    "    \n",
    "    def serv_cell(self, pd_data):\n",
    "        self.pscell = pd_data[\"Serving Cell PCI\"]\n",
    "        do = False\n",
    "        for cell in self.dict.keys():\n",
    "            if self.pscell == cell:\n",
    "                self.dict[\"PSCell\"][0] += self.dict[cell][0]\n",
    "                self.dict[\"PSCell\"][1] += self.dict[cell][1]\n",
    "                self.dict[\"PSCell\"][2] += self.dict[cell][2]\n",
    "                do,x = True, cell\n",
    "                break\n",
    "        if do:\n",
    "            self.dict.pop(x)\n",
    "            \n",
    "    def nei_cell(self, pd_data):\n",
    "        arfcn = pd_data[\"Raster ARFCN\"]\n",
    "        t = pd_data[\"Timestamp\"]\n",
    "        for i in range(6, len(pd_data), 3):\n",
    "            if pd_data[i] == '-' or np.isnan(float(pd_data[i])):\n",
    "                break\n",
    "            else:\n",
    "                rsrp = float(pd_data[i+1])\n",
    "                rsrq = float(pd_data[i+2])\n",
    "                self.dict[pd_data[i]] = [[rsrp], [rsrq], [t]]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.dict)\n",
    "\n",
    "    def __add__(self, sd2):\n",
    "        d1 = self.dict\n",
    "        d2 = sd2.dict\n",
    "        for key in list(d2.keys()):\n",
    "            if key in list(d1.keys()):\n",
    "                d1[key][0] += d2[key][0]\n",
    "                d1[key][1] += d2[key][1]\n",
    "                d1[key][2] += d2[key][2]\n",
    "            else:\n",
    "                d1[key] = d2[key]\n",
    "        return nr_ss_dict(d=d1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dual Radio csv Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_create_dual(dir1, dir2, ci_df, outfile, ul_df, dl_df):\n",
    "    base_dir1 = dir1\n",
    "    base_dir2 = dir2\n",
    "    # out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "    out_file = outfile\n",
    "    f = open(out_file, 'w') \n",
    "\n",
    "    d1 = os.path.join(base_dir1,\"data\")\n",
    "    d2 = os.path.join(base_dir2,\"data\")\n",
    "\n",
    "    excessive_latency_value = 0.1\n",
    "\n",
    "    GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "    \n",
    "    # Collect rsrp infomation\n",
    "    mi_ml1_dfs = []\n",
    "    nr_mi_ml1_dfs = []\n",
    "    HO_events_list = []\n",
    "    \n",
    "\n",
    "    for d in [d1, d2]:\n",
    "        matches = filter(lambda x: x.endswith('ml1.csv'), os.listdir(d))\n",
    "        ml1_filenames = sorted(list(matches))\n",
    "        mi_ml1_file = os.path.join(d, ml1_filenames[0])\n",
    "        mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "        mi_ml1_df[\"Timestamp\"] = mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        mi_ml1_dfs.append(mi_ml1_df)\n",
    "\n",
    "        nr_mi_ml1_file = os.path.join(d, ml1_filenames[1])\n",
    "        nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "        nr_mi_ml1_df[\"Timestamp\"] = nr_mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        nr_mi_ml1_dfs.append(nr_mi_ml1_df)\n",
    "\n",
    "        # Collect Ho information\n",
    "        matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(d))\n",
    "        mi_rrc_filename = list(matches)[0]\n",
    "        mi_rrc_file = os.path.join(d, mi_rrc_filename)\n",
    "        mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "        mi_rrc_df[\"Timestamp\"] = mi_rrc_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "        HO_events = parse_mi_ho(mi_rrc_df)\n",
    "        HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "        HO_events_list.append(HO_events)\n",
    "\n",
    "    columns = [\"Timestamp\", \"lat\", \"long\", \"gpsspeed\"]+[\n",
    "        'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF','SCG_RLF',\n",
    "        \"num_of_neis\",\"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "        \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    ]*2 + [\"dl-loss\", \"ul-loss\", \"dl-exc-lat\", \"ul-exc-lat\",\"dl-latency\", \"ul-latency\"]\n",
    "\n",
    "\n",
    "    f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "    i_ci = 0\n",
    "    i_pcap = [0,0]\n",
    "    i_ = [[0,0], [0,0]] # For increase speed\n",
    "    data_buffers = [{'rsrp':0, 'rsrq':0}, {'rsrp':0, 'rsrq':0}]\n",
    "\n",
    "    \n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "        ss_relateds = []\n",
    "        HO_relateds = []\n",
    "        # Get GPS informations\n",
    "        # ========================================================================\n",
    "        gps_related = []\n",
    "\n",
    "        for i in range(i_ci, len(ci_df)):\n",
    "            t = ci_df['Date'].iloc[i]\n",
    "            lat = ci_df['GPSLat'].iloc[i]\n",
    "            long = ci_df['GPSLon'].iloc[i]\n",
    "            gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "            elif t > time_point:\n",
    "                i_ci = i\n",
    "                break\n",
    "        \n",
    "        try : gps_info\n",
    "        except:\n",
    "            gps_info = GPS_info(lat='-',long='-',gpsspeed='-')\n",
    "            \n",
    "        gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "        gps_related = [str(feature) for feature in gps_related]\n",
    "        # print(f\"{time_point} {gps_info}\")\n",
    "\n",
    "        for j in range(2):\n",
    "            # ==========================================================================\n",
    "            # Get signal strength informations\n",
    "            ss_related = []\n",
    "\n",
    "            SS_DICT = ss_dict()\n",
    "            for i in range(i_[j][0], len(mi_ml1_df)):\n",
    "                t = mi_ml1_df['Timestamp'].iloc[i]\n",
    "                serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "                \n",
    "                if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "                    SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "                elif t > time_point:\n",
    "                    i_[j][0] = i\n",
    "                    break\n",
    "            \n",
    "            num_of_nei = len(SS_DICT.dict) - 1\n",
    "\n",
    "            # Get primary serv cell rsrp, rsrq \n",
    "            if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "                pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "                pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "                data_buffers[j]['rsrp'], data_buffers[j]['rsrq'] = pcell_rsrp, pcell_rsrq\n",
    "            else:\n",
    "                pcell_rsrp, pcell_rsrq = data_buffers[j]['rsrp'], data_buffers[j]['rsrq'] # No sample value, use the previous one\n",
    "            SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "            # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "            if len(SS_DICT.dict) != 0:\n",
    "                cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "                cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "                cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "                SS_DICT.dict.pop(cell1)\n",
    "            else:\n",
    "                # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "                cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "            if len(SS_DICT.dict) != 0:\n",
    "                cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "                cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "                cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "                SS_DICT.dict.pop(cell2)\n",
    "            else:\n",
    "                # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "                cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "                # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "            ss_related += [num_of_nei, pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "            NR_SS_DICT = nr_ss_dict()\n",
    "            for i in range(i_[j][1], len(nr_mi_ml1_df)):\n",
    "                t = nr_mi_ml1_df['Timestamp'].iloc[i]\n",
    "                serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "                \n",
    "                if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                    NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "                elif t > time_point:\n",
    "                    i_[j][1] = i\n",
    "                    break\n",
    "            \n",
    "            # Get primary secondary serv cell rsrp, rsrq \n",
    "            if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "                pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "                pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "            else:\n",
    "                # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "                pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "            NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "            # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "            if len(NR_SS_DICT.dict) != 0:\n",
    "                cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "                cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "                cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "                NR_SS_DICT.dict.pop(cell1)\n",
    "            else:\n",
    "                # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "                cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "            if len(NR_SS_DICT.dict) != 0:\n",
    "                cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "                cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "                cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "                NR_SS_DICT.dict.pop(cell2)\n",
    "            else:\n",
    "                # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "                cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "            \n",
    "            # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "            ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "            ss_related = [str(feature) for feature in ss_related]\n",
    "            ss_relateds.append(ss_related)\n",
    "            # ================================================================================\n",
    "            # Get HO informations\n",
    "            HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "            for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "                for ho in HO_events[ho_type]:\n",
    "                    t = ho.start\n",
    "                    if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                        HO_related[i] += 1\n",
    "                    elif t > time_point:\n",
    "                        break\n",
    "            \n",
    "            HO_related = [str(feature) for feature in HO_related]\n",
    "            HO_relateds.append(HO_related)\n",
    "        # ========================================================================\n",
    "        # Get DL/UL latency, loss...\n",
    "        performance_related = []\n",
    "\n",
    "        loss_col = f\"lost_{Setting[dev1]}+{Setting[dev2]}\"\n",
    "        latency_col = f\"latency_{Setting[dev1]}+{Setting[dev2]}\"\n",
    "        \n",
    "        dl_lats, dl_excessive_lats, dl_losses = [], [], []\n",
    "        for i in range(i_pcap[0], len(dl_df)):\n",
    "            t = dl_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "\n",
    "                dl_loss = dl_df[loss_col].iloc[i]             \n",
    "                \n",
    "                if dl_loss:\n",
    "                    dl_losses.append(t)\n",
    "                else:\n",
    "                    dl_lat = float(dl_df[latency_col].iloc[i])                \n",
    "                    dl_lats.append(dl_lat)\n",
    "                    if dl_lat >  excessive_latency_value:\n",
    "                        dl_excessive_lats.append(t)\n",
    "\n",
    "            elif t > time_point:\n",
    "                i_pcap[0] = i\n",
    "                break\n",
    "\n",
    "        if len(dl_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "        \n",
    "        dl_exc_num = len(dl_excessive_lats)\n",
    "        dl_loss_num = len(dl_losses)\n",
    "\n",
    "        ul_lats, ul_excessive_lats, ul_losses = [], [], []\n",
    "        for i in range(i_pcap[1], len(ul_df)):\n",
    "            t = ul_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                \n",
    "                ul_loss = ul_df[loss_col].iloc[i]\n",
    "\n",
    "                if ul_loss:\n",
    "                    ul_losses.append(t)\n",
    "                else:\n",
    "                    ul_lat = float(ul_df[latency_col].iloc[i])\n",
    "                    ul_lats.append(ul_lat)\n",
    "                    if ul_lat >  excessive_latency_value:\n",
    "                        ul_excessive_lats.append(t)\n",
    "\n",
    "            elif t > time_point:\n",
    "                i_pcap[1] = i\n",
    "                break\n",
    "\n",
    "        if len(ul_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "        \n",
    "        ul_exc_num = len(ul_excessive_lats)\n",
    "        ul_loss_num = len(ul_losses)\n",
    "\n",
    "        performance_related += [dl_loss_num, ul_loss_num, dl_exc_num, ul_exc_num, dl_avg_lat, ul_avg_lat]\n",
    "        performance_related = [str(feature) for feature in performance_related]\n",
    "\n",
    "\n",
    "        f.write(\",\".join([str(time_point)]+gps_related+HO_relateds[0]+ss_relateds[0]+HO_relateds[1]+ss_relateds[1]+performance_related)+\"\\n\") \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Setting = {'qc00': 'All', 'qc01': 'LTE', 'qc02': 'B7', 'qc03': 'B8'}\n",
    "# base_dir = '/home/wmnlab/D/sheng-ru/test/test_data/'\n",
    "base_dir = '/home/wmnlab/D/database/2023-04-17/_Bandlock_Udp_All_LTE_B7_B8_RM500Q/'\n",
    "weird_trace = []\n",
    "date = [x for x in list(base_dir.split('/')) if len(x) != 0][-2]\n",
    "\n",
    "matches = filter(lambda x: x.startswith('qc') or x.startswith('sm'), os.listdir(base_dir))\n",
    "combo_dir = os.path.join(base_dir, 'combo')\n",
    "device_dir = [os.path.join(base_dir, x) for x in list(matches)]\n",
    "device_dir.sort()\n",
    "\n",
    "parent_dir = str(Path(base_dir).parent.absolute())\n",
    "matches = list(filter(lambda x: 'ci' in x, os.listdir(parent_dir)))\n",
    "matches.sort()\n",
    "ci_file = os.path.join(parent_dir, matches[-1])\n",
    "\n",
    "try:\n",
    "    ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "    ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "except pd.errors.ParserError:\n",
    "    print(f'preprocess {ci_file}')\n",
    "    gps_dir = '/'.join(ci_file.split('/')[:-1])\n",
    "    os.system(f'python3 ./csv_processing.py {gps_dir}')\n",
    "    ci_df = pd.read_csv(ci_file[:-4]+'_new.csv', dtype=str)\n",
    "    ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "\n",
    "\n",
    "for trace in sorted(os.listdir(combo_dir)):\n",
    "    \n",
    "    if trace in weird_trace:\n",
    "        continue\n",
    "\n",
    "    ct_dir = os.path.join(combo_dir,trace) # combo+trace dir\n",
    "\n",
    "    ul_loss_lat = os.path.join(ct_dir, \"udp_uplk_combo_loss_latency.csv\")\n",
    "    ul_loss_lat_df = pd.read_csv(ul_loss_lat)\n",
    "    ul_loss_lat_df[\"Timestamp\"] = ul_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "    dl_loss_lat = os.path.join(ct_dir, \"udp_dnlk_combo_loss_latency.csv\")\n",
    "    dl_loss_lat_df = pd.read_csv(dl_loss_lat)\n",
    "    dl_loss_lat_df[\"Timestamp\"] = dl_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "    # Get timepoint from start to end\n",
    "    front_cut, back_cut = 1, 0\n",
    "    TS = 1\n",
    "    tp_range = 1\n",
    "    start = dl_loss_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut) # open the downlink file to decide start time and end time\n",
    "    end = dl_loss_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "    start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "    print(f'Trace {trace} from {start} to {end}.')\n",
    "    N = int((end - start).total_seconds()) # How many time_point\n",
    "\n",
    "\n",
    "    for i, device in enumerate(device_dir):\n",
    "        for j in range(i+1,len(device_dir)):\n",
    "            device2 = device_dir[j]\n",
    "            \n",
    "            dt_dir = os.path.join(device, trace) # device+trace dir\n",
    "            dt_dir2 = os.path.join(device2, trace)\n",
    "            print(dt_dir, dt_dir2)\n",
    "            dev1, dev2 = device[-4:], device2[-4:]\n",
    "            b1, b2 = Setting[dev1], Setting[dev2]\n",
    "            outfile = os.path.join('/home/wmnlab/D/sheng-ru/ml_data/dual/', date + f'{trace}_{b1}&{b2}.csv')\n",
    "            print(outfile)\n",
    "            print(dev1, dev2)\n",
    "            data_create_dual(dt_dir, dt_dir2, ci_df, outfile, ul_loss_lat_df, dl_loss_lat_df) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Radio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Radio\n",
    "def data_create(dir, ci_df, outfile, ul_df, dl_df):\n",
    "    base_dir = dir\n",
    "    # out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "    out_file = outfile\n",
    "    f = open(out_file, 'w') \n",
    "\n",
    "    d = os.path.join(base_dir,\"data\")\n",
    "\n",
    "    excessive_latency_value = 0.1\n",
    "\n",
    "    GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "    \n",
    "    # Collect rsrp infomation\n",
    "    mi_ml1_dfs = []\n",
    "    nr_mi_ml1_dfs = []\n",
    "    HO_events_list = []\n",
    "    \n",
    "    matches = filter(lambda x: x.endswith('ml1.csv'), os.listdir(d))\n",
    "    ml1_filenames = sorted(list(matches))\n",
    "    mi_ml1_file = os.path.join(d, ml1_filenames[0])\n",
    "    mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "    mi_ml1_df[\"Timestamp\"] = mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "    mi_ml1_dfs.append(mi_ml1_df)\n",
    "\n",
    "    nr_mi_ml1_file = os.path.join(d, ml1_filenames[1])\n",
    "    nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "    nr_mi_ml1_df[\"Timestamp\"] = nr_mi_ml1_df[\"Timestamp\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "    nr_mi_ml1_dfs.append(nr_mi_ml1_df)\n",
    "\n",
    "    # Collect Ho information\n",
    "    matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(d))\n",
    "    mi_rrc_filename = list(matches)[0]\n",
    "    mi_rrc_file = os.path.join(d, mi_rrc_filename)\n",
    "    mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "    mi_rrc_df[\"Timestamp\"] = mi_rrc_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "    HO_events = parse_mi_ho(mi_rrc_df)\n",
    "    HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "    HO_events_list.append(HO_events)\n",
    "\n",
    "    columns = [\"Timestamp\", \"lat\", \"long\", \"gpsspeed\"]+[\n",
    "        'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF','SCG_RLF',\n",
    "        \"num_of_neis\",\"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "        \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    ] + [\"dl-loss\", \"ul-loss\", \"dl-exc-lat\", \"ul-exc-lat\",\"dl-latency\", \"ul-latency\"]\n",
    "\n",
    "\n",
    "    f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "    i_ci = 0\n",
    "    i_pcap = [0, 0]\n",
    "    i_ = [0, 0] # For increase speed\n",
    "    data_buffers = {'rsrp':0, 'rsrq':0}\n",
    "\n",
    "    \n",
    "    for time_point in [start + dt.timedelta(seconds=i) for i in np.arange(0, N+TS, TS, dtype='float64')]:\n",
    "    # for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "        # Get GPS informations\n",
    "        # ========================================================================\n",
    "        gps_related = []\n",
    "\n",
    "        for i in range(i_ci, len(ci_df)):\n",
    "            t = ci_df['Date'].iloc[i]\n",
    "            lat = ci_df['GPSLat'].iloc[i]\n",
    "            long = ci_df['GPSLon'].iloc[i]\n",
    "            gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "            elif t > time_point:\n",
    "                i_ci = i\n",
    "                break\n",
    "        \n",
    "        try : gps_info\n",
    "        except:\n",
    "            gps_info = GPS_info(lat='-',long='-',gpsspeed='-')\n",
    "\n",
    "        gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "        gps_related = [str(feature) for feature in gps_related]\n",
    "        # print(f\"{time_point} {gps_info}\")\n",
    "\n",
    "        # ==========================================================================\n",
    "        # Get signal strength informations\n",
    "        ss_related = []\n",
    "\n",
    "        SS_DICT = ss_dict()\n",
    "        for i in range(i_[0], len(mi_ml1_df)):\n",
    "            t = mi_ml1_df['Timestamp'].iloc[i]\n",
    "            serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "            \n",
    "            if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "                SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "            elif t > time_point:\n",
    "                i_[0] = i\n",
    "                break\n",
    "        \n",
    "        num_of_nei = len(SS_DICT.dict) - 1\n",
    "\n",
    "        # Get primary serv cell rsrp, rsrq \n",
    "        if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "            pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "            pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "            data_buffers['rsrp'], data_buffers['rsrq'] = pcell_rsrp, pcell_rsrq\n",
    "        else:\n",
    "            pcell_rsrp, pcell_rsrq = data_buffers['rsrp'], data_buffers['rsrq'] # No sample value, use the previous one\n",
    "        SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "        # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "        if len(SS_DICT.dict) != 0:\n",
    "            cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "            cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "            cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "            SS_DICT.dict.pop(cell1)\n",
    "        else:\n",
    "            # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "            cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "        if len(SS_DICT.dict) != 0:\n",
    "            cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "            cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "            cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "            SS_DICT.dict.pop(cell2)\n",
    "        else:\n",
    "            # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "            cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "            # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "        ss_related += [num_of_nei, pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "        NR_SS_DICT = nr_ss_dict()\n",
    "        for i in range(i_[1], len(nr_mi_ml1_df)):\n",
    "            t = nr_mi_ml1_df['Timestamp'].iloc[i]\n",
    "            serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "            \n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "            elif t > time_point:\n",
    "                i_[1] = i\n",
    "                break\n",
    "        \n",
    "        # Get primary secondary serv cell rsrp, rsrq \n",
    "        if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "            pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "            pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "        else:\n",
    "            # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "            pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "        NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "        # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "        if len(NR_SS_DICT.dict) != 0:\n",
    "            cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "            cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "            cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "            NR_SS_DICT.dict.pop(cell1)\n",
    "        else:\n",
    "            # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "            cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "        if len(NR_SS_DICT.dict) != 0:\n",
    "            cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "            cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "            cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "            NR_SS_DICT.dict.pop(cell2)\n",
    "        else:\n",
    "            # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "            cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "        \n",
    "        # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "        ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "        ss_related = [str(feature) for feature in ss_related]\n",
    "        # ================================================================================\n",
    "        # Get HO informations\n",
    "        HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "        for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "            for ho in HO_events[ho_type]:\n",
    "                t = ho.start\n",
    "                if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                    \n",
    "                    HO_related[i] += 1\n",
    "\n",
    "                    # if HO_related[i] == 0:\n",
    "                    #     HO_related[i] = (t - (time_point - dt.timedelta(seconds=tp_range))).total_seconds()\n",
    "                    # else:\n",
    "                    #     x = t - (time_point - dt.timedelta(seconds=tp_range))\n",
    "                    #     HO_related[i] = str(HO_related[i]) + '@' + str(x.total_seconds())\n",
    "\n",
    "                elif t > time_point:\n",
    "                    break\n",
    "        \n",
    "        HO_related = [str(feature) for feature in HO_related]\n",
    "\n",
    "        # ========================================================================\n",
    "        # Get DL/UL latency, loss...\n",
    "        performance_related = []\n",
    "\n",
    "        loss_col = f\"lost\"\n",
    "        latency_col = f\"latency\"\n",
    "        \n",
    "        dl_lats, dl_excessive_lats, dl_losses = [], [], []\n",
    "        for i in range(i_pcap[0], len(dl_df)):\n",
    "            t = dl_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "\n",
    "                dl_loss = dl_df[loss_col].iloc[i]             \n",
    "                \n",
    "                if dl_loss:\n",
    "                    dl_losses.append(t)\n",
    "                else:\n",
    "                    dl_lat = float(dl_df[latency_col].iloc[i])                \n",
    "                    dl_lats.append(dl_lat)\n",
    "                    if dl_lat >  excessive_latency_value:\n",
    "                        dl_excessive_lats.append(t)\n",
    "\n",
    "            elif t > time_point:\n",
    "                i_pcap[0] = i\n",
    "                break\n",
    "\n",
    "        if len(dl_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "\n",
    "        dl_exc = len(dl_excessive_lats)\n",
    "        dl_loss = len(dl_losses)\n",
    "\n",
    "        ul_lats, ul_excessive_lats, ul_losses = [], [], []\n",
    "        for i in range(i_pcap[1], len(ul_df)):\n",
    "            t = ul_df['Timestamp'].iloc[i]\n",
    "            if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "                \n",
    "                ul_loss = ul_df[loss_col].iloc[i]\n",
    "\n",
    "                if ul_loss:\n",
    "                    ul_losses.append(t)\n",
    "                else:\n",
    "                    ul_lat = float(ul_df[latency_col].iloc[i])\n",
    "                    ul_lats.append(ul_lat)\n",
    "                    if ul_lat >  excessive_latency_value:\n",
    "                        ul_excessive_lats.append(t)\n",
    "\n",
    "            elif t > time_point:\n",
    "                i_pcap[1] = i\n",
    "                break\n",
    "\n",
    "        if len(ul_lats) == 0:\n",
    "            pass # No package arrive; will use previous value\n",
    "        else:\n",
    "            ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "\n",
    "        ul_exc = len(ul_excessive_lats)\n",
    "        ul_loss = len(ul_losses)\n",
    "\n",
    "        performance_related += [dl_loss, ul_loss, dl_exc, ul_exc, dl_avg_lat, ul_avg_lat]\n",
    "        performance_related = [str(feature) for feature in performance_related]\n",
    "\n",
    "\n",
    "        f.write(\",\".join([str(time_point)]+gps_related+HO_related+ss_related+performance_related)+\"\\n\") \n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir is /home/wmnlab/D/database/2023-09-12_2/Bandlock_9_Schemes_Phone_UDP\n",
      "Out Files at /home/wmnlab/sheng-ru/ml_data/single\n",
      "Experiment name: _udp; weird trace []; cell_info: False\n",
      "TS, tp_range = 1, 1\n",
      "Setting is {'sm00': 'All', 'sm01': 'All', 'sm02': 'B3', 'sm03': 'B7', 'sm04': 'B8', 'sm05': 'B3B7', 'sm06': 'B3B8', 'sm07': 'B7B8', 'sm08': 'LTE'}\n",
      "\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm00 All\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm00_udp_#01_All.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329535/1536972165.py:35: DtypeWarning: Columns (26,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mi_rrc_df = pd.read_csv(mi_rrc_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:46.\n",
      "sm00 All\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm00_udp_#02_All.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_329535/1536972165.py:35: DtypeWarning: Columns (26,75) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mi_rrc_df = pd.read_csv(mi_rrc_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm01 All\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm01_udp_#01_All.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm01 All\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm01_udp_#02_All.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm02 B3\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm02_udp_#01_B3.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm02 B3\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm02_udp_#02_B3.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm03 B7\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm03_udp_#01_B7.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm03 B7\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm03_udp_#02_B7.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm04 B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm04_udp_#01_B8.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm04 B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm04_udp_#02_B8.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm05 B3B7\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm05_udp_#01_B3B7.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:46.\n",
      "sm05 B3B7\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm05_udp_#02_B3B7.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm06 B3B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm06_udp_#01_B3B8.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm06 B3B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm06_udp_#02_B3B8.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm07 B7B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm07_udp_#01_B7B8.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm07 B7B8\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm07_udp_#02_B7B8.csv\n",
      "Trace #01 from 2023-09-12 13:34:17 to 2023-09-12 14:21:30.\n",
      "sm08 LTE\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm08_udp_#01_LTE.csv\n",
      "Trace #02 from 2023-09-12 14:52:30 to 2023-09-12 15:40:45.\n",
      "sm08 LTE\n",
      "/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm08_udp_#02_LTE.csv\n"
     ]
    }
   ],
   "source": [
    "## Data type\n",
    "front_cut, back_cut = 1, 1\n",
    "TS, tp_range = 1, 1\n",
    "\n",
    "## outpath\n",
    "out_path = '/home/wmnlab/sheng-ru/ml_data/single'\n",
    "# out_path = '/home/wmnlab/D/sheng-ru/ml_data/single0.5/'\n",
    "\n",
    "# Setting\n",
    "# Setting = {'qc00': 'All', 'qc03': 'All'}\n",
    "# Setting = {'sm00': 'All', 'sm01': 'LTE'}\n",
    "# Setting = {'sm00': 'All(wo_B1)', 'sm01': 'B3', 'sm02': 'B7', 'sm03': 'B8', 'sm04': 'B3B7', 'sm05': 'B3B8', 'sm06': 'B7B8', 'sm07': 'LTE'}\n",
    "Setting = { \"sm00\": \"All\", \"sm01\": \"All\", \"sm02\": \"B3\", \"sm03\": \"B7\", \"sm04\": \"B8\", \"sm05\": \"B3B7\", \"sm06\": \"B3B8\", \"sm07\": \"B7B8\", \"sm08\": \"LTE\" }\n",
    "\n",
    "## Which data dir to create \n",
    "# base_dir = '/home/wmnlab/D/sheng-ru/test/test_data/'\n",
    "base_dir = '/home/wmnlab/D/database/2023-09-12_2/Bandlock_9_Schemes_Phone_UDP'\n",
    "exp_name = '_udp'\n",
    "weird_devs = []; weird_trace = []\n",
    "cell_info = False\n",
    "\n",
    "date = [x for x in list(base_dir.split('/')) if len(x) != 0][-2]\n",
    "\n",
    "print(f\"basedir is {base_dir}\\n\" + f\"Out Files at {out_path}\\n\" +\n",
    "      f\"Experiment name: {exp_name}; weird trace {weird_trace}; cell_info: {cell_info}\\n\" + \n",
    "      f\"TS, tp_range = {TS}, {tp_range}\\n\" + f\"Setting is {Setting}\\n\"\n",
    "      )\n",
    "\n",
    "time.sleep(.2)\n",
    "warning = input('Correct Setting? (y/n)')\n",
    "if warning == 'y': pass\n",
    "else: raise\n",
    "\n",
    "# Collect gps and gpsspeed from cellinfo\n",
    "if not cell_info:\n",
    "    ci_df = pd.DataFrame({'Date' : []})\n",
    "else:\n",
    "    # For modem only one cell info\n",
    "    parent_dir = str(Path(base_dir).parent.absolute())\n",
    "    matches = list(filter(lambda x: 'ci' in x and not os.path.isdir(x), os.listdir(parent_dir)))\n",
    "    matches.sort()\n",
    "    ci_file = os.path.join(parent_dir, matches[-1])\n",
    "\n",
    "    # For phones multiple cellinfo\n",
    "    # parent_dir = str(Path(base_dir).parent.absolute())\n",
    "    # ci_dir = os.path.join(parent_dir, 'cimon')\n",
    "    # ci_file = os.path.join(ci_dir, os.listdir(ci_dir)[0])\n",
    "\n",
    "    try:\n",
    "        ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "        ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "    except pd.errors.ParserError:\n",
    "        print(f'preprocess {ci_file}')\n",
    "        # gps_dir = '/'.join(ci_file.split('/')[:-1])\n",
    "        os.system(f'python3 ./csv_processing.py {ci_file}')\n",
    "        ci_df = pd.read_csv(ci_file[:-4]+'_new.csv', dtype=str)\n",
    "        ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "devs = [x for x in os.listdir(base_dir) if ('sm' in x) or ('qc' in x)]\n",
    "\n",
    "for dev in sorted(devs):\n",
    "    \n",
    "    if dev in weird_devs:\n",
    "        continue\n",
    "    \n",
    "    device_dir = os.path.join(base_dir, dev)\n",
    "\n",
    "    for trace in sorted(os.listdir(device_dir)):\n",
    "        if trace in weird_trace:\n",
    "            continue\n",
    "    \n",
    "        dt_dir = os.path.join(device_dir, trace) # device trace directory\n",
    "\n",
    "        ul_loss_lat = os.path.join(dt_dir, 'data', \"udp_uplk_loss_latency.csv\")\n",
    "        ul_loss_lat_df = pd.read_csv(ul_loss_lat)\n",
    "        ul_loss_lat_df[\"Timestamp\"] = ul_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "        dl_loss_lat = os.path.join(dt_dir, 'data', \"udp_dnlk_loss_latency.csv\")\n",
    "        dl_loss_lat_df = pd.read_csv(dl_loss_lat)\n",
    "        dl_loss_lat_df[\"Timestamp\"] = dl_loss_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "        # Get timepoint from start to end\n",
    "        start = dl_loss_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut) # open the downlink file to decide start time and end time\n",
    "        end = dl_loss_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "        start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "        print(f'Trace {trace} from {start} to {end}.')\n",
    "        N = int((end - start).total_seconds()) # How many time_point in second\n",
    "        \n",
    "        b = Setting[dev]\n",
    "        print(dev, b)\n",
    "\n",
    "        # Out file here\n",
    "        # outfile = os.path.join(out_path, date + f'_{dev}_{trace}_{b}.csv')\n",
    "        outfile = os.path.join(out_path, date + f'_{dev}{exp_name}_{trace}_{b}.csv')\n",
    "\n",
    "        print(outfile)\n",
    "\n",
    "        data_create(dt_dir, ci_df, outfile, ul_loss_lat_df, dl_loss_lat_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm06_udp_#01_B3B8.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm02_udp_#02_B3.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm07_udp_#01_B7B8.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm04_udp_#02_B8.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm03_udp_#01_B7.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm08_udp_#01_LTE.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm06_udp_#02_B3B8.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm05_udp_#01_B3B7.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm00_udp_#02_All.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm08_udp_#02_LTE.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm01_udp_#01_All.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm04_udp_#01_B8.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm00_udp_#01_All.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm03_udp_#02_B7.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm02_udp_#01_B3.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm01_udp_#02_All.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm05_udp_#02_B3B7.csv',\n",
      " '/home/wmnlab/sheng-ru/ml_data/single/2023-09-12_2_sm07_udp_#02_B7B8.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# \n",
    "folder_path = \"/home/wmnlab/sheng-ru/ml_data/single\"\n",
    "\n",
    "# CSV\n",
    "file_list = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".csv\") and ('09-12' in file)]\n",
    "\n",
    "pprint.pprint(file_list)\n",
    "time.sleep(.5)\n",
    "warning = input('Sure to process the listed data? (y/n)')\n",
    "if warning == 'y': pass\n",
    "else: raise\n",
    "\n",
    "# CSV\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 0\n",
    "    start_index = df.index[df['RSRP'].iloc[:] != 0][0]\n",
    "\n",
    "    # 0\n",
    "    df = df.iloc[start_index:]\n",
    "\n",
    "    # CSV\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/wmnlab/Code_Test_Space/sheng-ru/2022-12-22/_Bandlock_Udp_B1_B3/sm05/#01\"\n",
    "out_file = \"/home/wmnlab/test1.csv\" ## Out file !!!!!!!!\n",
    "f = open(out_file, 'w') \n",
    "\n",
    "# Collecting the UDP Latency and Loss information first\n",
    "dir = os.path.join(base_dir,\"data\")\n",
    "\n",
    "dl_lat_file = os.path.join(dir, \"udp_dnlk_latency.csv\")\n",
    "dl_lat_df = pd.read_csv(dl_lat_file)\n",
    "dl_lat_df[\"Timestamp\"] = dl_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "dl_loss_file = os.path.join(dir, \"udp_dnlk_loss_timestamp.csv\")\n",
    "dl_loss_df = pd.read_csv(dl_loss_file)\n",
    "dl_loss_df[\"Timestamp\"] = dl_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_lat_file = os.path.join(dir, \"udp_uplk_latency.csv\")\n",
    "ul_lat_df = pd.read_csv(ul_lat_file)\n",
    "ul_lat_df[\"Timestamp\"] = ul_lat_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "ul_loss_file = os.path.join(dir, \"udp_uplk_loss_timestamp.csv\")\n",
    "ul_loss_df = pd.read_csv(ul_loss_file)\n",
    "ul_loss_df[\"Timestamp\"] = ul_loss_df[\"Timestamp\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "# Get timepoint from start to end and get the latency and loss\n",
    "front_cut = 5\n",
    "back_cut = 5\n",
    "TS = 1\n",
    "tp_range = 1\n",
    "\n",
    "start = dl_lat_df[\"Timestamp\"].iloc[0] + dt.timedelta(seconds=front_cut)\n",
    "end = dl_lat_df[\"Timestamp\"].iloc[-1] - dt.timedelta(seconds=back_cut)\n",
    "start, end = start.replace(microsecond=0), end.replace(microsecond=0)\n",
    "print(f'From {start} to {end}.')\n",
    "N = int((end - start).total_seconds()) # How many time_point\n",
    "\n",
    "# Collect rsrp infomation\n",
    "matches = filter(lambda x: x.endswith('ml1_new.csv'), os.listdir(dir))\n",
    "ml1_filenames = sorted(list(matches))\n",
    "mi_ml1_file = os.path.join(dir, ml1_filenames[0])\n",
    "mi_ml1_df = pd.read_csv(mi_ml1_file, dtype=str)\n",
    "mi_ml1_df = mi_ml1_df[mi_ml1_df.type_id == 'LTE_PHY_Connected_Mode_Intra_Freq_Meas']\n",
    "mi_ml1_df[\"time\"] = mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "nr_mi_ml1_file = os.path.join(dir, ml1_filenames[1])\n",
    "nr_mi_ml1_df = pd.read_csv(nr_mi_ml1_file, dtype=str)\n",
    "nr_mi_ml1_df[\"time\"] = nr_mi_ml1_df[\"time\"].apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "\n",
    "\n",
    "# Collect gps and gpsspeed from cellinfo\n",
    "dir = os.path.join(base_dir,\"middle\")\n",
    "matches = filter(lambda x: x.startswith('cimon'), os.listdir(dir))\n",
    "ci_filename = list(matches)[0]\n",
    "ci_file = os.path.join(dir, ci_filename)\n",
    "ci_df = pd.read_csv(ci_file, dtype=str)\n",
    "ci_df[\"Date\"] = ci_df[\"Date\"].swifter.apply(lambda x: pd.to_datetime(x))\n",
    "\n",
    "GPS_info = namedtuple('gps_info','lat, long, gpsspeed')\n",
    "\n",
    "# Collect Ho information\n",
    "matches = filter(lambda x: x.endswith('rrc.csv'), os.listdir(dir))\n",
    "mi_rrc_filename = list(matches)[0]\n",
    "mi_rrc_file = os.path.join(dir, mi_rrc_filename)\n",
    "mi_rrc_df = pd.read_csv(mi_rrc_file)\n",
    "mi_rrc_df[\"time\"] = mi_rrc_df[\"time\"].swifter.apply(lambda x: pd.to_datetime(x) + dt.timedelta(hours=8))\n",
    "HO_events = parse_mi_ho(mi_rrc_df)\n",
    "HO_events.pop('Conn_Rel'), HO_events.pop('Conn_Req')\n",
    "\n",
    "columns = [\n",
    "    \"Timestamp\",\n",
    "    \"lat\", \"long\", \"gpsspeed\",\n",
    "    'LTE_HO','MN_HO','eNB_to_ENDC','gNB_Rel','gNB_HO','RLF_II','RLF_III','SCG_RLF',\n",
    "    \"RSRP\",\"RSRQ\",\"RSRP1\",\"RSRQ1\",\"RSRP2\",\"RSRQ2\",\n",
    "    \"nr-RSRP\",\"nr-RSRQ\",\"nr-RSRP1\",\"nr-RSRQ1\",\"nr-RSRP2\",\"nr-RSRQ2\",\n",
    "    \"DL-lat\", \"DL-lossrate\", \"UL-lat\", \"UL-lossrate\"\n",
    "]\n",
    "f.write(\",\".join(columns)+\"\\n\")\n",
    "\n",
    "i_ = [0,0,0,0,0,0,0] # For increase speed\n",
    "for time_point in [start + dt.timedelta(seconds=i) for i in range(0, N+1, TS)]:\n",
    "\n",
    "    # ========================================================================\n",
    "    # Get DL/UL latency, loss...\n",
    "    perfermance_related = []\n",
    "\n",
    "    dl_lats = []\n",
    "    for i in range(i_[0], len(dl_lat_df)):\n",
    "        t = dl_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_lat = float(dl_lat_df['latency'].iloc[i])\n",
    "            dl_lats.append(dl_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[0] = i\n",
    "            break\n",
    "\n",
    "    if len(dl_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        # perfermance_related.append('-')\n",
    "        perfermance_related.append(dl_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        dl_avg_lat = sum(dl_lats)/len(dl_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(dl_avg_lat)\n",
    "\n",
    "    dl_losses = []\n",
    "    for i in range(i_[1], len(dl_loss_df)):\n",
    "        t = dl_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            dl_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[1] = i\n",
    "            break\n",
    "\n",
    "    if (len(dl_losses)+len(dl_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(dl_losses)/(len(dl_losses)+len(dl_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    ul_lats = []\n",
    "    for i in range(i_[2], len(ul_lat_df)):\n",
    "        t = ul_lat_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_lat = float(ul_lat_df['latency'].iloc[i])\n",
    "            ul_lats.append(ul_lat)\n",
    "            # if lat >  excessive_latency_value:\n",
    "            #     excessive_latency.append(lat)\n",
    "        elif t > time_point:\n",
    "            i_[2] = i\n",
    "            break\n",
    "\n",
    "    if len(ul_lats) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "        perfermance_related.append(ul_avg_lat) # Apeend previous value\n",
    "    else:\n",
    "        ul_avg_lat = sum(ul_lats)/len(ul_lats)\n",
    "        # print(f\"{time_point} average latency: {avg_lat}\")\n",
    "        perfermance_related.append(ul_avg_lat)\n",
    "\n",
    "    ul_losses = []\n",
    "    for i in range(i_[3], len(ul_loss_df)):\n",
    "        t = ul_loss_df['Timestamp'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            ul_losses.append(t)\n",
    "        elif t > time_point:\n",
    "            i_[3] = i\n",
    "            break\n",
    "\n",
    "    if (len(ul_losses)+len(ul_lats)) == 0:\n",
    "        # print(f\"{time_point} No package arrive\")\n",
    "        perfermance_related.append('-')\n",
    "    else:\n",
    "        loss_rate = len(ul_losses)/(len(ul_losses)+len(ul_lats))\n",
    "        # print(f\"{time_point} loss rate: {loss_rate}\")\n",
    "        perfermance_related.append(loss_rate)\n",
    "\n",
    "    perfermance_related = [str(feature) for feature in perfermance_related]\n",
    "\n",
    "    # ==========================================================================\n",
    "    # Get GPS informations\n",
    "    gps_related = []\n",
    "\n",
    "    for i in range(i_[4], len(ci_df)):\n",
    "        t = ci_df['Date'].iloc[i]\n",
    "        lat = ci_df['GPSLat'].iloc[i]\n",
    "        long = ci_df['GPSLon'].iloc[i]\n",
    "        gpsspeed = ci_df['GPSSpeed'].iloc[i]\n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            gps_info = GPS_info(lat=lat,long=long,gpsspeed=gpsspeed)\n",
    "        elif t > time_point:\n",
    "            i_[4] = i\n",
    "            break\n",
    "    \n",
    "\n",
    "    gps_related += [gps_info.lat, gps_info.long, gps_info.gpsspeed]\n",
    "    gps_related = [str(feature) for feature in gps_related]\n",
    "    # print(f\"{time_point} {gps_info}\")\n",
    "    \n",
    "    # ===========================================================================\n",
    "    # Get signal strength informations\n",
    "    ss_related = []\n",
    "\n",
    "    SS_DICT = ss_dict()\n",
    "    for i in range(i_[5], len(mi_ml1_df)):\n",
    "        t = mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = mi_ml1_df['Serving Cell Index'].iloc[i]\n",
    "        \n",
    "        if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point) and serv_cell_idx=='PCell':\n",
    "            SS_DICT += ss_dict(mi_ml1_df.iloc[i])\n",
    "        elif t > time_point:\n",
    "            i_[5] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary serv cell rsrp, rsrq \n",
    "    if len(SS_DICT.dict[\"PCell\"][0]) != 0:\n",
    "        pcell_rsrp = sum(SS_DICT.dict[\"PCell\"][0])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "        pcell_rsrq = sum(SS_DICT.dict[\"PCell\"][1])/len(SS_DICT.dict[\"PCell\"][0])\n",
    "    else:\n",
    "        # pcell_rsrp, pcell_rsrq = '-', '-'\n",
    "        pcell_rsrp, pcell_rsrq = pcell_rsrp, pcell_rsrq # No sample value, use the previous one\n",
    "    SS_DICT.dict.pop(\"PCell\") \n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell1 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(SS_DICT.dict[cell1][0])/len(SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(SS_DICT.dict[cell1][1])/len(SS_DICT.dict[cell1][0])\n",
    "        SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(SS_DICT.dict) != 0:\n",
    "        cell2 = max(SS_DICT.dict, key=lambda x:sum(SS_DICT.dict[x][0])/len(SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(SS_DICT.dict[cell2][0])/len(SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(SS_DICT.dict[cell2][1])/len(SS_DICT.dict[cell2][0])\n",
    "        SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "        # print(f\"{time_point} {pcell_rsrp}, {pcell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pcell_rsrp, pcell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    NR_SS_DICT = nr_ss_dict()\n",
    "    for i in range(i_[6], len(nr_mi_ml1_df)):\n",
    "        t = nr_mi_ml1_df['time'].iloc[i]\n",
    "        serv_cell_idx = nr_mi_ml1_df['Serving Cell PCI'].iloc[i]\n",
    "        \n",
    "        if time_point - dt.timedelta(seconds=tp_range) < t <= time_point:\n",
    "            NR_SS_DICT += nr_ss_dict(nr_mi_ml1_df.iloc[i])\n",
    "\n",
    "        elif t > time_point:\n",
    "            i_[6] = i\n",
    "            break\n",
    "    \n",
    "    # Get primary secondary serv cell rsrp, rsrq \n",
    "    if len(NR_SS_DICT.dict[\"PSCell\"][0]) != 0:\n",
    "        pscell_rsrp = sum(NR_SS_DICT.dict[\"PSCell\"][0])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "        pscell_rsrq = sum(NR_SS_DICT.dict[\"PSCell\"][1])/len(NR_SS_DICT.dict[\"PSCell\"][0])\n",
    "    else:\n",
    "        # pscell_rsrp, pscell_rsrq = '-', '-'\n",
    "        pscell_rsrp, pscell_rsrq = 0,0 # No nr serving or no sample value assign 0\n",
    "    NR_SS_DICT.dict.pop(\"PSCell\")\n",
    "\n",
    "    # Get 1st, 2nd neighbor cell rsrp, rsrq\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell1 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell1_rsrp = sum(NR_SS_DICT.dict[cell1][0])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        cell1_rsrq = sum(NR_SS_DICT.dict[cell1][1])/len(NR_SS_DICT.dict[cell1][0])\n",
    "        NR_SS_DICT.dict.pop(cell1)\n",
    "    else:\n",
    "        # cell1_rsrp, cell1_rsrq = '-', '-'\n",
    "        cell1_rsrp, cell1_rsrq = 0,0 # No sample value, assign 0\n",
    "\n",
    "    if len(NR_SS_DICT.dict) != 0:\n",
    "        cell2 = max(NR_SS_DICT.dict, key=lambda x:sum(NR_SS_DICT.dict[x][0])/len(NR_SS_DICT.dict[x][0]))\n",
    "        cell2_rsrp = sum(NR_SS_DICT.dict[cell2][0])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        cell2_rsrq = sum(NR_SS_DICT.dict[cell2][1])/len(NR_SS_DICT.dict[cell2][0])\n",
    "        NR_SS_DICT.dict.pop(cell2)\n",
    "    else:\n",
    "        # cell2_rsrp, cell2_rsrq = '-', '-'\n",
    "        cell2_rsrp, cell2_rsrq = 0,0 # No sample value, assign 0\n",
    "    \n",
    "    # print(f\"{time_point} {pscell_rsrp}, {pscell_rsrq} {cell1_rsrp}, {cell1_rsrq} {cell2_rsrp}, {cell2_rsrq}\")\n",
    "    ss_related += [pscell_rsrp, pscell_rsrq, cell1_rsrp, cell1_rsrq, cell2_rsrp, cell2_rsrq]\n",
    "\n",
    "    ss_related = [str(feature) for feature in ss_related]\n",
    "\n",
    "    # ================================================================================\n",
    "    # Get HO informations\n",
    "    HO_related = [0] * len(HO_events.keys())\n",
    "\n",
    "    for i, ho_type in  enumerate(list(HO_events.keys())):\n",
    "        for ho in HO_events[ho_type]:\n",
    "            t = ho.start\n",
    "            if (time_point - dt.timedelta(seconds=tp_range) < t <= time_point):\n",
    "                HO_related[i] += 1\n",
    "            elif t > time_point:\n",
    "                break\n",
    "    \n",
    "    HO_related = [str(feature) for feature in HO_related]\n",
    "\n",
    "    f.write(\",\".join([str(time_point)]+gps_related+HO_related+ss_related+perfermance_related)+\"\\n\") \n",
    "\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_data\n",
    "ml_data = '/home/wmnlab/ml_data'\n",
    "files = [os.path.join(ml_data, x) for x in os.listdir(ml_data)]\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_text(x):\n",
    "    ind = x.find('#')\n",
    "    x = x[ind+4:-4].split('&')\n",
    "    x.sort()\n",
    "    x = '&'.join(x)\n",
    "    return x\n",
    "\n",
    "def get_points(num):\n",
    "    L = []\n",
    "    a, b = divmod(num, 30)\n",
    "    if b != 0:\n",
    "        L.append(b)\n",
    "    L += [b+i*30 for i in range(1,a+1)]\n",
    "    return L\n",
    "\n",
    "experiment_time = 300\n",
    "time_slot = 30\n",
    "database = open('/home/wmnlab/ntu-experiments/sheng-ru/experiment/mobileinsight/database.csv', 'w')\n",
    "smoothing_lambda = 0.5\n",
    "columns = ['B3&B7', 'B3&B8', 'B7&B8']\n",
    "rows = [[0]*len(columns) for i in range(int(experiment_time/time_slot))]\n",
    "database.write(','.join(columns)+'\\n')\n",
    "\n",
    "for file in files:\n",
    "    band_set = find_text(file)\n",
    "    set_ind = columns.index(band_set)\n",
    "    f = open(file)\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    print(band_set, columns.index(band_set))\n",
    "    \n",
    "    if len(df) > experiment_time:\n",
    "        df = df[-300:]\n",
    "        time_points = get_points(300)\n",
    "        i = 0\n",
    "        for j, l in enumerate(time_points):\n",
    "            count, val = 0, 0\n",
    "            while i < l:\n",
    "                count += 1\n",
    "                x = df.iloc[i]\n",
    "                val += x['dl-lossrate'] + x['ul-lossrate'] + x['dl-exc-lat'] + x['ul-exc-lat']\n",
    "                i += 1\n",
    "            if rows[j][set_ind] == 0:\n",
    "                rows[j][set_ind] = val/count\n",
    "            else:\n",
    "                rows[j][set_ind] += val/count\n",
    "                rows[j][set_ind] /= 2\n",
    "\n",
    "    else:\n",
    "        time_points = get_points(len(df))\n",
    "        i = 0\n",
    "        for j, l in enumerate(time_points):\n",
    "            count, val = 0, 0\n",
    "            while i < l:\n",
    "                count += 1\n",
    "                x = df.iloc[i]\n",
    "                val += x['dl-lossrate'] + x['ul-lossrate'] + x['dl-exc-lat'] + x['ul-exc-lat']\n",
    "                i += 1\n",
    "            if rows[j][set_ind] == 0:\n",
    "                rows[j][set_ind] = val/count\n",
    "            else:\n",
    "                rows[j][set_ind] += val/count\n",
    "                rows[j][set_ind] /= 2\n",
    "                \n",
    "    f.close()\n",
    "\n",
    "# Write database\n",
    "for row in rows:\n",
    "    row = [str(x) for x in row]\n",
    "    database.write(','.join(row)+'\\n')\n",
    "\n",
    "database.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
